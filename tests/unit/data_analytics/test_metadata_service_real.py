#!/usr/bin/env python3
"""
Real Metadata Service Test
ä½¿ç”¨çœŸå®æµ·å…³è´¸æ˜“æ•°æ®æµ‹è¯•å…ƒæ•°æ®å‘ç°æœåŠ¡
"""

import sys
import json
import tempfile
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from tools.services.data_analytics_service.services.metadata_service import MetadataDiscoveryService
from tools.services.data_analytics_service.utils.error_handler import DataAnalyticsError

# çœŸå®æ•°æ®åº“é…ç½®
REAL_DB_CONFIG = {
    "type": "postgresql",
    "host": "localhost",
    "port": 5432,
    "database": "customs_trade_db",
    "username": "xenodennis",
    "password": "",
    "include_data_analysis": True,
    "sample_size": 1000
}

def test_discover_database_metadata():
    """æµ‹è¯•æ•°æ®åº“å…ƒæ•°æ®å‘ç°"""
    print("ğŸ” æµ‹è¯•æ•°æ®åº“å…ƒæ•°æ®å‘ç°...")
    
    service = MetadataDiscoveryService()
    
    try:
        metadata = service.discover_database_metadata(REAL_DB_CONFIG)
        
        # éªŒè¯åŸºæœ¬ç»“æ„
        if not isinstance(metadata, dict):
            print("âŒ å…ƒæ•°æ®æ ¼å¼é”™è¯¯")
            return False
        
        required_keys = ['source_info', 'tables', 'columns', 'relationships', 'indexes', 'discovery_info']
        for key in required_keys:
            if key not in metadata:
                print(f"âŒ ç¼ºå°‘å¿…è¦é”®: {key}")
                return False
        
        print("âœ… å…ƒæ•°æ®ç»“æ„éªŒè¯é€šè¿‡")
        
        # éªŒè¯å‘ç°ä¿¡æ¯
        discovery_info = metadata['discovery_info']
        if discovery_info['service'] != 'MetadataDiscoveryService':
            print(f"âŒ æœåŠ¡åç§°é”™è¯¯: {discovery_info['service']}")
            return False
        
        if discovery_info['source_type'] != 'database':
            print(f"âŒ æºç±»å‹é”™è¯¯: {discovery_info['source_type']}")
            return False
        
        if discovery_info['source_subtype'] != 'postgresql':
            print(f"âŒ æºå­ç±»å‹é”™è¯¯: {discovery_info['source_subtype']}")
            return False
        
        print("âœ… å‘ç°ä¿¡æ¯éªŒè¯é€šè¿‡")
        
        # éªŒè¯è¡¨ä¿¡æ¯
        tables = metadata['tables']
        expected_tables = ['companies', 'declarations', 'goods_details', 'hs_codes', 'ports']
        table_names = [t['table_name'] for t in tables]
        
        print(f"ğŸ“‹ å‘ç° {len(tables)} ä¸ªè¡¨:")
        for expected_table in expected_tables:
            if expected_table in table_names:
                print(f"   âœ… {expected_table}")
            else:
                print(f"   âŒ ç¼ºå°‘è¡¨: {expected_table}")
                return False
        
        # éªŒè¯å­—æ®µä¿¡æ¯
        columns = metadata['columns']
        if len(columns) == 0:
            print("âŒ æœªå‘ç°ä»»ä½•å­—æ®µ")
            return False
        
        print(f"ğŸ·ï¸ å‘ç° {len(columns)} ä¸ªå­—æ®µ")
        
        # éªŒè¯companiesè¡¨çš„å…³é”®å­—æ®µ
        companies_columns = [c for c in columns if c['table_name'] == 'companies']
        companies_column_names = [c['column_name'] for c in companies_columns]
        expected_columns = ['company_code', 'company_name', 'company_type', 'credit_level']
        
        for expected_col in expected_columns:
            if expected_col in companies_column_names:
                print(f"   âœ… companies.{expected_col}")
            else:
                print(f"   âŒ ç¼ºå°‘å­—æ®µ: companies.{expected_col}")
                return False
        
        # éªŒè¯å…³ç³»ä¿¡æ¯
        relationships = metadata['relationships']
        print(f"ğŸ”— å‘ç° {len(relationships)} ä¸ªå…³ç³»")
        
        if relationships:
            for rel in relationships[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå…³ç³»
                print(f"   - {rel['from_table']}.{rel['from_column']} -> {rel['to_table']}.{rel['to_column']}")
                
                required_rel_keys = ['from_table', 'to_table', 'constraint_type']
                for key in required_rel_keys:
                    if key not in rel:
                        print(f"âŒ å…³ç³»ç¼ºå°‘é”®: {key}")
                        return False
        
        print("âœ… æ•°æ®åº“å…ƒæ•°æ®å‘ç°æµ‹è¯•é€šè¿‡")
        return True, metadata
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“å…ƒæ•°æ®å‘ç°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def test_get_summary_statistics(metadata):
    """æµ‹è¯•è·å–æ±‡æ€»ç»Ÿè®¡"""
    print("\nğŸ“Š æµ‹è¯•è·å–æ±‡æ€»ç»Ÿè®¡...")
    
    service = MetadataDiscoveryService()
    
    try:
        summary = service.get_summary_statistics(metadata)
        
        # éªŒè¯æ±‡æ€»ç»“æ„
        if not isinstance(summary, dict):
            print("âŒ æ±‡æ€»ç»Ÿè®¡æ ¼å¼é”™è¯¯")
            return False
        
        required_keys = ['totals', 'table_statistics', 'column_statistics', 'relationship_statistics', 'data_quality_overview']
        for key in required_keys:
            if key not in summary:
                print(f"âŒ ç¼ºå°‘æ±‡æ€»é”®: {key}")
                return False
        
        print("âœ… æ±‡æ€»ç»Ÿè®¡ç»“æ„éªŒè¯é€šè¿‡")
        
        # éªŒè¯totals
        totals = summary['totals']
        print(f"ğŸ“ˆ æ•°æ®æ€»è®¡:")
        print(f"   - è¡¨æ•°é‡: {totals.get('tables', 0)}")
        print(f"   - å­—æ®µæ•°é‡: {totals.get('columns', 0)}")
        print(f"   - å…³ç³»æ•°é‡: {totals.get('relationships', 0)}")
        
        if not all(isinstance(totals.get(key, 0), int) for key in ['tables', 'columns', 'relationships']):
            print("âŒ totalsæ•°æ®ç±»å‹é”™è¯¯")
            return False
        
        # éªŒè¯è¡¨ç»Ÿè®¡
        table_stats = summary['table_statistics']
        if table_stats:
            print(f"ğŸ“‹ è¡¨ç»Ÿè®¡ä¿¡æ¯:")
            if 'total_records' in table_stats:
                print(f"   - æ€»è®°å½•æ•°: {table_stats['total_records']}")
            if 'avg_records_per_table' in table_stats:
                print(f"   - å¹³å‡æ¯è¡¨è®°å½•æ•°: {table_stats['avg_records_per_table']:.1f}")
        
        # éªŒè¯å­—æ®µç»Ÿè®¡
        column_stats = summary['column_statistics']
        if column_stats:
            print(f"ğŸ·ï¸ å­—æ®µç»Ÿè®¡ä¿¡æ¯:")
            if 'total_columns' in column_stats:
                print(f"   - æ€»å­—æ®µæ•°: {column_stats['total_columns']}")
            if 'data_type_distribution' in column_stats:
                print(f"   - æ•°æ®ç±»å‹åˆ†å¸ƒ: {len(column_stats['data_type_distribution'])} ç§ç±»å‹")
        
        print("âœ… æ±‡æ€»ç»Ÿè®¡æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ±‡æ€»ç»Ÿè®¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_export_metadata_json(metadata):
    """æµ‹è¯•å¯¼å‡ºå…ƒæ•°æ®åˆ°JSON"""
    print("\nğŸ’¾ æµ‹è¯•å¯¼å‡ºå…ƒæ•°æ®åˆ°JSON...")
    
    service = MetadataDiscoveryService()
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp_file:
        output_path = tmp_file.name
    
    try:
        # å¯¼å‡ºå…ƒæ•°æ®
        success = service.export_metadata(metadata, output_path, 'json')
        
        if not success:
            print("âŒ å¯¼å‡ºæ“ä½œå¤±è´¥")
            return False
        
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        output_file = Path(output_path)
        if not output_file.exists():
            print("âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        print(f"âœ… æ–‡ä»¶å·²å¯¼å‡º: {output_file.name}")
        print(f"   æ–‡ä»¶å¤§å°: {output_file.stat().st_size} å­—èŠ‚")
        
        # éªŒè¯JSONå†…å®¹
        with open(output_path, 'r', encoding='utf-8') as f:
            loaded_metadata = json.load(f)
        
        # éªŒè¯åŠ è½½çš„å…ƒæ•°æ®ç»“æ„
        required_keys = ['source_info', 'tables', 'columns']
        for key in required_keys:
            if key not in loaded_metadata:
                print(f"âŒ å¯¼å‡ºæ–‡ä»¶ç¼ºå°‘é”®: {key}")
                return False
        
        print("âœ… JSONå¯¼å‡ºå†…å®¹éªŒè¯é€šè¿‡")
        
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        if len(loaded_metadata['tables']) != len(metadata['tables']):
            print("âŒ å¯¼å‡ºçš„è¡¨æ•°é‡ä¸ä¸€è‡´")
            return False
        
        if len(loaded_metadata['columns']) != len(metadata['columns']):
            print("âŒ å¯¼å‡ºçš„å­—æ®µæ•°é‡ä¸ä¸€è‡´")
            return False
        
        print("âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
        print("âœ… JSONå¯¼å‡ºæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ JSONå¯¼å‡ºæµ‹è¯•å¤±è´¥: {e}")
        return False
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            Path(output_path).unlink()
        except:
            pass

def test_export_metadata_csv(metadata):
    """æµ‹è¯•å¯¼å‡ºå…ƒæ•°æ®åˆ°CSV"""
    print("\nğŸ“„ æµ‹è¯•å¯¼å‡ºå…ƒæ•°æ®åˆ°CSV...")
    
    service = MetadataDiscoveryService()
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp_file:
        output_path = tmp_file.name
    
    try:
        # å¯¼å‡ºå…ƒæ•°æ®
        success = service.export_metadata(metadata, output_path, 'csv')
        
        if not success:
            print("âŒ CSVå¯¼å‡ºæ“ä½œå¤±è´¥")
            return False
        
        # éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
        tables_path = output_path.replace('.csv', '_tables.csv')
        columns_path = output_path.replace('.csv', '_columns.csv')
        
        files_to_check = [
            (tables_path, "è¡¨ä¿¡æ¯"),
            (columns_path, "å­—æ®µä¿¡æ¯")
        ]
        
        for file_path, description in files_to_check:
            file_obj = Path(file_path)
            if not file_obj.exists():
                print(f"âŒ {description}æ–‡ä»¶ä¸å­˜åœ¨: {file_obj.name}")
                return False
            
            file_size = file_obj.stat().st_size
            if file_size == 0:
                print(f"âŒ {description}æ–‡ä»¶ä¸ºç©º")
                return False
            
            print(f"âœ… {description}æ–‡ä»¶: {file_obj.name} ({file_size} å­—èŠ‚)")
        
        print("âœ… CSVå¯¼å‡ºæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ CSVå¯¼å‡ºæµ‹è¯•å¤±è´¥: {e}")
        return False
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        cleanup_paths = [
            output_path,
            output_path.replace('.csv', '_tables.csv'),
            output_path.replace('.csv', '_columns.csv'),
            output_path.replace('.csv', '_relationships.csv')
        ]
        
        for path in cleanup_paths:
            try:
                Path(path).unlink()
            except:
                pass

def test_compare_schemas():
    """æµ‹è¯•æ¶æ„æ¯”è¾ƒ"""
    print("\nğŸ”„ æµ‹è¯•æ¶æ„æ¯”è¾ƒ...")
    
    service = MetadataDiscoveryService()
    
    try:
        # è·å–åŒä¸€ä¸ªæ•°æ®åº“çš„å…ƒæ•°æ®è¿›è¡Œæ¯”è¾ƒ
        metadata1 = service.discover_database_metadata(REAL_DB_CONFIG)
        metadata2 = service.discover_database_metadata(REAL_DB_CONFIG)
        
        # æ¯”è¾ƒæ¶æ„
        comparison = service.compare_schemas(metadata1, metadata2)
        
        # éªŒè¯æ¯”è¾ƒç»“æ„
        if not isinstance(comparison, dict):
            print("âŒ æ¯”è¾ƒç»“æœæ ¼å¼é”™è¯¯")
            return False
        
        required_keys = ['comparison_time', 'source1_info', 'source2_info', 'table_comparison', 'column_comparison', 'relationship_comparison']
        for key in required_keys:
            if key not in comparison:
                print(f"âŒ ç¼ºå°‘æ¯”è¾ƒé”®: {key}")
                return False
        
        print("âœ… æ¯”è¾ƒç»“æ„éªŒè¯é€šè¿‡")
        
        # éªŒè¯è¡¨æ¯”è¾ƒ
        table_comp = comparison['table_comparison']
        print(f"ğŸ“‹ è¡¨æ¯”è¾ƒç»“æœ:")
        print(f"   - æº1è¡¨æ•°: {table_comp.get('total_source1', 0)}")
        print(f"   - æº2è¡¨æ•°: {table_comp.get('total_source2', 0)}")
        print(f"   - ç›¸ä¼¼åº¦: {table_comp.get('similarity_score', 0):.2f}")
        
        if not isinstance(table_comp.get('similarity_score'), (int, float)):
            print("âŒ è¡¨ç›¸ä¼¼åº¦åˆ†æ•°ç±»å‹é”™è¯¯")
            return False
        
        # åŒä¸€æ•°æ®æºåº”è¯¥æœ‰100%ç›¸ä¼¼åº¦
        if table_comp.get('similarity_score') != 1.0:
            print("âŒ åŒä¸€æ•°æ®æºæ¯”è¾ƒç›¸ä¼¼åº¦åº”è¯¥æ˜¯1.0")
            return False
        
        # éªŒè¯å…³ç³»æ¯”è¾ƒ
        rel_comp = comparison['relationship_comparison']
        print(f"ğŸ”— å…³ç³»æ¯”è¾ƒç»“æœ:")
        print(f"   - æº1å…³ç³»æ•°: {rel_comp.get('total_source1', 0)}")
        print(f"   - æº2å…³ç³»æ•°: {rel_comp.get('total_source2', 0)}")
        print(f"   - ç›¸ä¼¼åº¦: {rel_comp.get('similarity_score', 0):.2f}")
        
        if rel_comp.get('similarity_score') != 1.0:
            print("âŒ åŒä¸€æ•°æ®æºå…³ç³»æ¯”è¾ƒç›¸ä¼¼åº¦åº”è¯¥æ˜¯1.0")
            return False
        
        print("âœ… æ¶æ„æ¯”è¾ƒæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ¶æ„æ¯”è¾ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_validation_errors():
    """æµ‹è¯•éªŒè¯å’Œé”™è¯¯å¤„ç†"""
    print("\nâš ï¸ æµ‹è¯•éªŒè¯å’Œé”™è¯¯å¤„ç†...")
    
    service = MetadataDiscoveryService()
    
    # æµ‹è¯•æ— æ•ˆçš„æ•°æ®åº“é…ç½®
    invalid_db_config = {
        "type": "postgresql",
        # ç¼ºå°‘å¿…è¦å­—æ®µ
    }
    
    try:
        service.discover_database_metadata(invalid_db_config)
        print("âŒ åº”è¯¥æŠ›å‡ºé”™è¯¯ä½†æ²¡æœ‰")
        return False
    except DataAnalyticsError:
        print("âœ… æ— æ•ˆæ•°æ®åº“é…ç½®æ­£ç¡®æŠ›å‡ºDataAnalyticsError")
    except Exception as e:
        print(f"âŒ æŠ›å‡ºäº†é”™è¯¯çš„å¼‚å¸¸ç±»å‹: {type(e)}")
        return False
    
    # æµ‹è¯•ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹
    unsupported_db_config = {
        "type": "unsupported_db",
        "host": "localhost",
        "database": "test",
        "username": "user",
        "password": "pass"
    }
    
    try:
        service.discover_database_metadata(unsupported_db_config)
        print("âŒ åº”è¯¥æŠ›å‡ºé”™è¯¯ä½†æ²¡æœ‰")
        return False
    except DataAnalyticsError:
        print("âœ… ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹æ­£ç¡®æŠ›å‡ºDataAnalyticsError")
    except Exception as e:
        print(f"âŒ æŠ›å‡ºäº†é”™è¯¯çš„å¼‚å¸¸ç±»å‹: {type(e)}")
        return False
    
    # æµ‹è¯•æ— æ•ˆçš„æ–‡ä»¶é…ç½®
    invalid_file_config = {
        "file_path": "/nonexistent/file.csv"
    }
    
    try:
        service.discover_file_metadata(invalid_file_config)
        print("âŒ åº”è¯¥æŠ›å‡ºé”™è¯¯ä½†æ²¡æœ‰")
        return False
    except DataAnalyticsError:
        print("âœ… æ— æ•ˆæ–‡ä»¶é…ç½®æ­£ç¡®æŠ›å‡ºDataAnalyticsError")
    except Exception as e:
        print(f"âŒ æŠ›å‡ºäº†é”™è¯¯çš„å¼‚å¸¸ç±»å‹: {type(e)}")
        return False
    
    print("âœ… éªŒè¯å’Œé”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")
    return True

def test_metadata_service_configuration():
    """æµ‹è¯•å…ƒæ•°æ®æœåŠ¡é…ç½®"""
    print("\nâš™ï¸ æµ‹è¯•å…ƒæ•°æ®æœåŠ¡é…ç½®...")
    
    service = MetadataDiscoveryService()
    
    # æµ‹è¯•æ”¯æŒçš„æ•°æ®åº“
    expected_databases = ['postgresql', 'mysql', 'sqlserver']
    for db_type in expected_databases:
        if db_type not in service.supported_databases:
            print(f"âŒ ç¼ºå°‘æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}")
            return False
        print(f"âœ… æ”¯æŒæ•°æ®åº“: {db_type}")
    
    # æµ‹è¯•æ”¯æŒçš„æ–‡ä»¶ç±»å‹
    expected_files = ['excel', 'csv']
    for file_type in expected_files:
        if file_type not in service.supported_files:
            print(f"âŒ ç¼ºå°‘æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
            return False
        print(f"âœ… æ”¯æŒæ–‡ä»¶: {file_type}")
    
    # æµ‹è¯•æœåŠ¡ç»„ä»¶åˆå§‹åŒ–
    if service.config_manager is None:
        print("âŒ é…ç½®ç®¡ç†å™¨æœªåˆå§‹åŒ–")
        return False
    print("âœ… é…ç½®ç®¡ç†å™¨å·²åˆå§‹åŒ–")
    
    if service.data_validator is None:
        print("âŒ æ•°æ®éªŒè¯å™¨æœªåˆå§‹åŒ–")
        return False
    print("âœ… æ•°æ®éªŒè¯å™¨å·²åˆå§‹åŒ–")
    
    print("âœ… å…ƒæ•°æ®æœåŠ¡é…ç½®æµ‹è¯•é€šè¿‡")
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å…ƒæ•°æ®å‘ç°æœåŠ¡çœŸå®æ•°æ®æµ‹è¯•")
    print("=" * 60)
    print(f"æµ‹è¯•æ•°æ®åº“: {REAL_DB_CONFIG['database']}")
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # å…ˆæµ‹è¯•æ•°æ®åº“å…ƒæ•°æ®å‘ç°ï¼Œè·å–å…ƒæ•°æ®ç”¨äºåç»­æµ‹è¯•
    print("ğŸ” æ‰§è¡Œä¸»è¦æµ‹è¯•ï¼šæ•°æ®åº“å…ƒæ•°æ®å‘ç°")
    success, metadata = test_discover_database_metadata()
    
    if not success:
        print("âŒ ä¸»è¦æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å…¶ä»–æµ‹è¯•")
        return False
    
    # å®šä¹‰å…¶ä»–æµ‹è¯•
    other_tests = [
        ("è·å–æ±‡æ€»ç»Ÿè®¡", lambda: test_get_summary_statistics(metadata)),
        ("å¯¼å‡ºJSONæ ¼å¼", lambda: test_export_metadata_json(metadata)),
        ("å¯¼å‡ºCSVæ ¼å¼", lambda: test_export_metadata_csv(metadata)),
        ("æ¶æ„æ¯”è¾ƒ", test_compare_schemas),
        ("éªŒè¯å’Œé”™è¯¯å¤„ç†", test_validation_errors),
        ("æœåŠ¡é…ç½®", test_metadata_service_configuration),
    ]
    
    passed_tests = 1  # ä¸»æµ‹è¯•å·²é€šè¿‡
    failed_tests = 0
    
    for test_name, test_func in other_tests:
        try:
            print(f"\n{'='*20} {test_name} {'='*20}")
            if test_func():
                passed_tests += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                failed_tests += 1
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            failed_tests += 1
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n{'='*60}")
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"{'='*60}")
    print(f"âœ… é€šè¿‡: {passed_tests}")
    print(f"âŒ å¤±è´¥: {failed_tests}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {passed_tests/(passed_tests+failed_tests)*100:.1f}%")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    if metadata:
        result_file = f"metadata_service_test_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
        print(f"ğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    if failed_tests == 0:
        print("\nğŸ‰ æ‰€æœ‰å…ƒæ•°æ®æœåŠ¡æµ‹è¯•é€šè¿‡!")
        return True
    else:
        print(f"\nâš ï¸ æœ‰ {failed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)