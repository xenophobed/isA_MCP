#!/usr/bin/env python
"""
æµ‹è¯• crawl_and_extract å·¥å…·åŠŸèƒ½
æµ‹è¯•æˆ‘ä»¬åœ¨ tools/web_tools.py ä¸­å®ç°çš„ MCP å·¥å…·
"""
import asyncio
import json
import sys
import os

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))

# å¯¼å…¥æˆ‘ä»¬çš„MCPå·¥å…·æ³¨å†Œå™¨
from tools.web_tools import register_web_tools
from core.logging import get_logger
from core.security import initialize_security

logger = get_logger(__name__)


class MockMCP:
    """æ¨¡æ‹ŸMCPæœåŠ¡å™¨ç”¨äºæµ‹è¯•"""
    def __init__(self):
        self.tools = {}
    
    def tool(self):
        """è£…é¥°å™¨ç”¨äºæ³¨å†Œå·¥å…·"""
        def decorator(func):
            self.tools[func.__name__] = func
            return func
        return decorator
    
    async def call_tool(self, tool_name, **kwargs):
        """è°ƒç”¨æ³¨å†Œçš„å·¥å…·"""
        if tool_name in self.tools:
            return await self.tools[tool_name](**kwargs)
        else:
            raise ValueError(f"Tool {tool_name} not found")


async def test_crawl_extract_tool():
    """æµ‹è¯•æˆ‘ä»¬çš„crawl_and_extract MCPå·¥å…·"""
    print("ğŸš€ æµ‹è¯•crawl_and_extract MCPå·¥å…·")
    print("=" * 60)
    
    # åˆå§‹åŒ–å®‰å…¨ç®¡ç†å™¨
    initialize_security()
    
    # åˆ›å»ºæ¨¡æ‹ŸMCPæœåŠ¡å™¨
    mock_mcp = MockMCP()
    
    # æ³¨å†Œæˆ‘ä»¬çš„webå·¥å…·
    register_web_tools(mock_mcp)
    
    print("âœ… Webå·¥å…·å·²æ³¨å†Œåˆ°MCPæœåŠ¡å™¨")
    print(f"ğŸ“‹ å¯ç”¨å·¥å…·: {list(mock_mcp.tools.keys())}")
    
    try:
        # æµ‹è¯•1: Wikipediaæ–‡ç« æå–
        print("\nğŸ“– æµ‹è¯•1: Wikipediaæ–‡ç« æå–")
        wikipedia_urls = [
            "https://en.wikipedia.org/wiki/Artificial_intelligence"
        ]
        
        result1 = await mock_mcp.call_tool(
            "crawl_and_extract",
            urls=json.dumps(wikipedia_urls),
            extraction_schema="article",
            filter_query="",
            max_urls=1,
            user_id="test_user"
        )
        
        print("âœ… Wikipediaæµ‹è¯•ç»“æœ:")
        result1_data = json.loads(result1)
        if result1_data["status"] == "success":
            items = result1_data["data"]["extracted_items"]
            print(f"   æå–äº† {len(items)} ä¸ªæ•°æ®é¡¹")
            if items:
                item = items[0]
                print(f"   æ ‡é¢˜: {item.get('title', 'N/A')}")
                print(f"   å†…å®¹: {str(item.get('content', 'N/A'))[:150]}...")
        else:
            print(f"   âŒ å¤±è´¥: {result1_data.get('error', 'Unknown error')}")
        
        # æµ‹è¯•2: Amazonäº§å“æå–
        print("\nğŸ›ï¸ æµ‹è¯•2: Amazon JBLè€³æœºäº§å“æå–")
        amazon_url = "https://www.amazon.com/JBL-Endurance-Race-Waterproof-Cancelling/dp/B0DVTGBCBL/?_encoding=UTF8&pd_rd_w=t5IWN&content-id=amzn1.sym.27713fe8-b8c8-4a62-8381-8d094618df19&pf_rd_p=27713fe8-b8c8-4a62-8381-8d094618df19&pf_rd_r=6KJGBZZ66FH7B63HYX20&pd_rd_wg=c00mx&pd_rd_r=f638579d-46a7-44c8-ab46-9097fe59a690&ref_=pd_hp_d_atf_dealz_cs&th=1"
        
        result2 = await mock_mcp.call_tool(
            "crawl_and_extract",
            urls=json.dumps([amazon_url]),
            extraction_schema="product",
            filter_query="price review rating",
            max_urls=1,
            user_id="test_user"
        )
        
        print("âœ… Amazonäº§å“æµ‹è¯•ç»“æœ:")
        result2_data = json.loads(result2)
        if result2_data["status"] == "success":
            items = result2_data["data"]["extracted_items"]
            print(f"   æå–äº† {len(items)} ä¸ªäº§å“é¡¹")
            if items:
                item = items[0]
                print(f"   äº§å“å: {item.get('name', 'N/A')}")
                print(f"   ä»·æ ¼: {item.get('price', 'N/A')}")
                print(f"   åŸä»·: {item.get('original_price', 'N/A')}")
                print(f"   è¯„åˆ†: {item.get('rating', 'N/A')}")
                print(f"   è¯„è®ºæ•°: {item.get('reviews_count', 'N/A')}")
                print(f"   åº“å­˜: {item.get('availability', 'N/A')}")
                print(f"   æè¿°: {str(item.get('description', 'N/A'))[:200]}...")
        else:
            print(f"   âŒ å¤±è´¥: {result2_data.get('error', 'Unknown error')}")
        
        # æµ‹è¯•3: å¤šURLæå–
        print("\nğŸ”— æµ‹è¯•3: å¤šURLæå–")
        multi_urls = [
            "https://en.wikipedia.org/wiki/Machine_learning",
            "https://en.wikipedia.org/wiki/Deep_learning"
        ]
        
        result3 = await mock_mcp.call_tool(
            "crawl_and_extract",
            urls=json.dumps(multi_urls),
            extraction_schema="research",
            filter_query="machine learning AI neural network",
            max_urls=2,
            user_id="test_user"
        )
        
        print("âœ… å¤šURLæµ‹è¯•ç»“æœ:")
        result3_data = json.loads(result3)
        if result3_data["status"] == "success":
            items = result3_data["data"]["extracted_items"]
            stats = result3_data["data"]["processing_stats"]
            print(f"   æå–äº† {len(items)} ä¸ªæ•°æ®é¡¹")
            print(f"   å¤„ç†ç»Ÿè®¡: {stats['successful_extractions']}/{stats['total_urls']} æˆåŠŸ")
            print(f"   æˆåŠŸç‡: {stats['success_rate']}%")
            print(f"   å¹³å‡å¤„ç†æ—¶é—´: {stats['average_processing_time_seconds']}ç§’")
        else:
            print(f"   âŒ å¤±è´¥: {result3_data.get('error', 'Unknown error')}")
        
        # æµ‹è¯•4: è‡ªå®šä¹‰schema
        print("\nâš™ï¸ æµ‹è¯•4: è‡ªå®šä¹‰schemaæå–")
        custom_schema = {
            "name": "Product Review Extractor",
            "content_type": "product",
            "fields": [
                {"name": "product_name", "description": "Product name"},
                {"name": "current_price", "description": "Current selling price"},
                {"name": "customer_rating", "description": "Customer rating score"},
                {"name": "review_summary", "description": "Summary of customer reviews"}
            ]
        }
        
        result4 = await mock_mcp.call_tool(
            "crawl_and_extract",
            urls=json.dumps([amazon_url]),
            extraction_schema=json.dumps(custom_schema),
            filter_query="",
            max_urls=1,
            user_id="test_user"
        )
        
        print("âœ… è‡ªå®šä¹‰schemaæµ‹è¯•ç»“æœ:")
        result4_data = json.loads(result4)
        if result4_data["status"] == "success":
            items = result4_data["data"]["extracted_items"]
            print(f"   æå–äº† {len(items)} ä¸ªæ•°æ®é¡¹")
            if items:
                item = items[0]
                print(f"   äº§å“å: {item.get('product_name', 'N/A')}")
                print(f"   å½“å‰ä»·æ ¼: {item.get('current_price', 'N/A')}")
                print(f"   å®¢æˆ·è¯„åˆ†: {item.get('customer_rating', 'N/A')}")
                print(f"   è¯„è®ºæ‘˜è¦: {str(item.get('review_summary', 'N/A'))[:150]}...")
        else:
            print(f"   âŒ å¤±è´¥: {result4_data.get('error', 'Unknown error')}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸ”¥ å¯åŠ¨crawl_and_extract MCPå·¥å…·æµ‹è¯•...")
    asyncio.run(test_crawl_extract_tool())