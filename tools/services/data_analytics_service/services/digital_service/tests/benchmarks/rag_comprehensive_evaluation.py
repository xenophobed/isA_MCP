#!/usr/bin/env python3
"""
RAGç»¼åˆè¯„ä¼°æµ‹è¯• - å›°éš¾æµ‹è¯•æ¡ˆä¾‹
è®¾è®¡å¤šç»´åº¦è¯„ä¼°æ‰€æœ‰7ä¸ªRAGæ¨¡å¼çš„æ€§èƒ½
"""

import asyncio
import time
import json
from typing import Dict, Any, List
from dataclasses import dataclass
from tools.services.data_analytics_service.services.digital_service.base.base_rag_service import RAGConfig

# å¤æ‚æµ‹è¯•æ–‡æ¡£ - å¤šä¸»é¢˜ã€å¤šå…³ç³»ã€éœ€è¦æ¨ç†
COMPLEX_TEST_DOCUMENT = """
äººå·¥æ™ºèƒ½å‘å±•å†ç¨‹ä¸å•†ä¸šåº”ç”¨åˆ†ææŠ¥å‘Š

## å†å²èƒŒæ™¯
äººå·¥æ™ºèƒ½(AI)æ¦‚å¿µæœ€æ—©ç”±Alan Turingåœ¨1950å¹´æå‡ºï¼Œä»–åœ¨è®ºæ–‡ã€ŠComputing Machinery and Intelligenceã€‹ä¸­é¦–æ¬¡æ¢è®¨äº†æœºå™¨æ€ç»´çš„å¯èƒ½æ€§ã€‚1956å¹´ï¼ŒJohn McCarthyã€Marvin Minskyã€Claude Shannonå’ŒNathaniel Rochesteråœ¨è¾¾ç‰¹èŒ…æ–¯å­¦é™¢ç»„ç»‡äº†ç¬¬ä¸€æ¬¡AIä¼šè®®ï¼Œæ­£å¼ç¡®ç«‹äº†"äººå·¥æ™ºèƒ½"è¿™ä¸€æœ¯è¯­ã€‚

## æŠ€æœ¯å‘å±•é˜¶æ®µ
1960-1970å¹´ä»£è¢«ç§°ä¸ºAIçš„"é»„é‡‘æ—¶ä»£"ï¼Œä¸“å®¶ç³»ç»Ÿå¼€å§‹å…´èµ·ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—èƒ½åŠ›é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼Œ1974-1980å¹´ç»å†äº†ç¬¬ä¸€æ¬¡"AIå¯’å†¬"ã€‚1980å¹´ä»£ä¸“å®¶ç³»ç»Ÿå•†ä¸šåŒ–æˆåŠŸï¼Œä½†1987-1993å¹´å†æ¬¡è¿›å…¥ç¬¬äºŒæ¬¡å¯’å†¬æœŸã€‚

1997å¹´ï¼ŒIBMçš„Deep Blueå‡»è´¥äº†å›½é™…è±¡æ£‹ä¸–ç•Œå† å†›Garry Kasparovï¼Œæ ‡å¿—ç€AIåœ¨ç‰¹å®šé¢†åŸŸçš„çªç ´ã€‚2011å¹´ï¼ŒIBM Watsonåœ¨æ™ºåŠ›ç«èµ›èŠ‚ç›®Jeopardy!ä¸­å‡»è´¥äººç±»å† å†›ï¼Œå±•ç¤ºäº†è‡ªç„¶è¯­è¨€å¤„ç†çš„è¿›æ­¥ã€‚

## ç°ä»£AIé©å‘½
2012å¹´ï¼ŒGeoffrey Hintonå›¢é˜Ÿçš„AlexNetåœ¨ImageNetç«èµ›ä¸­å–å¾—çªç ´æ€§æˆæœï¼Œæ·±åº¦å­¦ä¹ é‡æ–°å…´èµ·ã€‚2016å¹´ï¼ŒGoogle DeepMindçš„AlphaGoå‡»è´¥å›´æ£‹ä¸–ç•Œå† å†›æä¸–çŸ³ï¼Œå±•ç°äº†å¼ºåŒ–å­¦ä¹ çš„å¨åŠ›ã€‚

2017å¹´ï¼ŒGoogleå‘å¸ƒäº†Transformeræ¶æ„è®ºæ–‡ã€ŠAttention Is All You Needã€‹ï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚2018å¹´ï¼ŒOpenAIå‘å¸ƒGPT-1ï¼Œå‚æ•°é‡1.17äº¿ã€‚2019å¹´GPT-2å‚æ•°å¢è‡³15äº¿ï¼Œ2020å¹´GPT-3è¾¾åˆ°1750äº¿å‚æ•°ï¼Œå±•ç°äº†æƒŠäººçš„è¯­è¨€ç”Ÿæˆèƒ½åŠ›ã€‚

## å•†ä¸šåº”ç”¨ç°çŠ¶
æˆªè‡³2023å¹´ï¼Œå…¨çƒAIå¸‚åœºè§„æ¨¡å·²è¾¾åˆ°1368äº¿ç¾å…ƒï¼Œé¢„è®¡2030å¹´å°†å¢é•¿è‡³1.8ä¸‡äº¿ç¾å…ƒï¼Œå¹´å¤åˆå¢é•¿ç‡ä¸º36.8%ã€‚ä¸»è¦åº”ç”¨é¢†åŸŸåŒ…æ‹¬ï¼š

### åŒ»ç–—å¥åº·
- Google Healthçš„AIç³»ç»Ÿåœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜æ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ°90%ä»¥ä¸Š
- IBM Watson for OncologyååŠ©ç™Œç—‡æ²»ç–—å†³ç­–ï¼Œåœ¨æŸäº›ç™Œç—‡ç±»å‹çš„æ²»ç–—å»ºè®®å‡†ç¡®ç‡è¾¾åˆ°85%
- Modernaåˆ©ç”¨AIæŠ€æœ¯åœ¨ä¸åˆ°ä¸€å¹´æ—¶é—´å†…å¼€å‘å‡ºCOVID-19ç–«è‹—

### è‡ªåŠ¨é©¾é©¶
- Teslaçš„Full Self-Driving (FSD)ç³»ç»Ÿå·²æ”¶é›†è¶…è¿‡30äº¿è‹±é‡Œçš„é©¾é©¶æ•°æ®
- Waymoè‡ªåŠ¨é©¾é©¶æ±½è½¦åœ¨äºšåˆ©æ¡‘é‚£å·å‡¤å‡°åŸæä¾›å•†ä¸šæœåŠ¡ï¼Œç´¯è®¡è¡Œé©¶è¶…è¿‡2000ä¸‡è‹±é‡Œ
- ä¸­å›½ç™¾åº¦Apolloå¹³å°åœ¨åŒ—äº¬ã€ä¸Šæµ·ç­‰åŸå¸‚å¼€å±•è‡ªåŠ¨é©¾é©¶æµ‹è¯•

### é‡‘èç§‘æŠ€
- JPMorgan Chaseçš„COINç³»ç»Ÿæ¯å¹´å¯å¤„ç†ç›¸å½“äº36ä¸‡å°æ—¶å¾‹å¸ˆå·¥ä½œé‡çš„æ³•å¾‹æ–‡ä»¶
- Goldman Sachsä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œé«˜é¢‘äº¤æ˜“ï¼Œç®—æ³•äº¤æ˜“å å…¶è‚¡ç¥¨äº¤æ˜“é‡çš„45%
- èš‚èšé‡‘æœçš„é£æ§ç³»ç»Ÿæ¯ç§’å¯å¤„ç†12ä¸‡ç¬”äº¤æ˜“ï¼Œæ¬ºè¯ˆè¯†åˆ«å‡†ç¡®ç‡è¾¾99.9%

## æŠ€æœ¯æŒ‘æˆ˜ä¸é£é™©
å°½ç®¡AIå–å¾—å·¨å¤§è¿›å±•ï¼Œä½†ä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼šæ•°æ®éšç§é—®é¢˜ã€ç®—æ³•åè§ã€å°±ä¸šæ›¿ä»£é£é™©ã€æŠ€èƒ½å·®è·ç­‰ã€‚2023å¹´ï¼Œæ¬§ç›Ÿé€šè¿‡äº†ã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹ï¼Œæˆä¸ºå…¨çƒé¦–ä¸ªå…¨é¢çš„AIç›‘ç®¡æ³•å¾‹æ¡†æ¶ã€‚

## æœªæ¥å±•æœ›
ä¸“å®¶é¢„æµ‹ï¼Œåˆ°2030å¹´ï¼ŒAIå°†åœ¨ä»¥ä¸‹é¢†åŸŸå®ç°é‡å¤§çªç ´ï¼š
1. é€šç”¨äººå·¥æ™ºèƒ½(AGI)çš„åˆæ­¥å®ç°ï¼ŒæŸäº›AIç³»ç»Ÿå¯èƒ½é€šè¿‡å›¾çµæµ‹è¯•
2. é‡å­è®¡ç®—ä¸AIç»“åˆï¼Œè§£å†³ç°æœ‰ç®—æ³•æ— æ³•å¤„ç†çš„å¤æ‚é—®é¢˜
3. è„‘æœºæ¥å£æŠ€æœ¯æˆç†Ÿï¼Œå®ç°äººè„‘ä¸AIçš„ç›´æ¥äº¤äº’
4. AIç§‘å­¦å®¶å‡ºç°ï¼Œèƒ½å¤Ÿç‹¬ç«‹è¿›è¡Œç§‘å­¦ç ”ç©¶å’Œå‘ç°

æŠ•èµ„æ–¹é¢ï¼Œ2023å¹´å…¨çƒAIåˆåˆ›ä¼ä¸šè·å¾—æŠ•èµ„æ€»é¢è¾¾åˆ°251äº¿ç¾å…ƒï¼Œå…¶ä¸­ç”Ÿæˆå¼AIä¼ä¸šå æ¯”è¶…è¿‡40%ã€‚ä¸­ç¾ä¸¤å›½åœ¨AIæŠ•èµ„ç«äº‰æ¿€çƒˆï¼Œç¾å›½å å…¨çƒAIæŠ•èµ„çš„45%ï¼Œä¸­å›½å 30%ã€‚

## ç»“è®º
äººå·¥æ™ºèƒ½æ­£åœ¨ä»å®éªŒå®¤èµ°å‘ç°å®ä¸–ç•Œï¼Œå…¶å‘å±•é€Ÿåº¦è¿œè¶…é¢„æœŸã€‚ç„¶è€Œï¼ŒæŠ€æœ¯è¿›æ­¥å¿…é¡»ä¸ä¼¦ç†è€ƒé‡ã€ç›‘ç®¡æ¡†æ¶å’Œç¤¾ä¼šé€‚åº”æ€§ç›¸å¹³è¡¡ã€‚æœªæ¥åå¹´å°†æ˜¯AIå‘å±•çš„å…³é”®æœŸï¼Œå…¶å½±å“å°†æ·±åˆ»æ”¹å˜äººç±»ç¤¾ä¼šçš„æ–¹æ–¹é¢é¢ã€‚
"""

# å¤æ‚æŸ¥è¯¢é—®é¢˜ - æµ‹è¯•ä¸åŒå±‚æ¬¡çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›
COMPLEX_QUERIES = [
    {
        "id": "Q1",
        "question": "åˆ†æäººå·¥æ™ºèƒ½å‘å±•è¿‡ç¨‹ä¸­çš„ä¸¤æ¬¡'å¯’å†¬'ï¼Œæ¯”è¾ƒå®ƒä»¬çš„æ—¶é—´ã€åŸå› å’Œå½±å“ï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆ2012å¹´åAIèƒ½å¤Ÿé‡æ–°å…´èµ·ï¼Ÿ",
        "difficulty": "é«˜",
        "required_skills": ["æ—¶é—´åºåˆ—åˆ†æ", "å› æœå…³ç³»æ¨ç†", "æ¯”è¾ƒåˆ†æ"],
        "expected_facts": [
            "ç¬¬ä¸€æ¬¡AIå¯’å†¬ï¼š1974-1980å¹´",
            "ç¬¬äºŒæ¬¡AIå¯’å†¬ï¼š1987-1993å¹´",
            "2012å¹´AlexNetçªç ´",
            "æ·±åº¦å­¦ä¹ é‡æ–°å…´èµ·çš„åŸå› "
        ]
    },
    {
        "id": "Q2", 
        "question": "æ ¹æ®æ–‡æ¡£ä¸­çš„æ•°æ®ï¼Œè®¡ç®—å¹¶åˆ†æå…¨çƒAIå¸‚åœºä»2023å¹´åˆ°2030å¹´çš„å¢é•¿è¶‹åŠ¿ï¼Œå¹¶è¯´æ˜å“ªäº›å•†ä¸šåº”ç”¨é¢†åŸŸå±•ç°å‡ºæœ€å¼ºçš„æŠ€æœ¯å®åŠ›ï¼Ÿ",
        "difficulty": "é«˜",
        "required_skills": ["æ•°å€¼è®¡ç®—", "è¶‹åŠ¿åˆ†æ", "æ€§èƒ½æ¯”è¾ƒ"],
        "expected_facts": [
            "2023å¹´å¸‚åœºè§„æ¨¡ï¼š1368äº¿ç¾å…ƒ",
            "2030å¹´é¢„æµ‹ï¼š1.8ä¸‡äº¿ç¾å…ƒ",
            "å¹´å¤åˆå¢é•¿ç‡ï¼š36.8%",
            "å„é¢†åŸŸçš„å…·ä½“æ€§èƒ½æ•°æ®"
        ]
    },
    {
        "id": "Q3",
        "question": "ä»æŠ€æœ¯æ¼”è¿›è§’åº¦ï¼Œè§£é‡ŠTransformeræ¶æ„å¯¹ç°ä»£AIå‘å±•çš„é‡è¦æ€§ï¼Œå¹¶åˆ†æGPTç³»åˆ—æ¨¡å‹çš„å‚æ•°è§„æ¨¡å¢é•¿å¯¹AIèƒ½åŠ›æå‡çš„å½±å“ã€‚",
        "difficulty": "æé«˜",
        "required_skills": ["æŠ€æœ¯å› æœå…³ç³»", "å‚æ•°è§„æ¨¡åˆ†æ", "æŠ€æœ¯å½±å“è¯„ä¼°"],
        "expected_facts": [
            "2017å¹´Transformerè®ºæ–‡",
            "GPT-1ï¼š1.17äº¿å‚æ•°",
            "GPT-2ï¼š15äº¿å‚æ•°", 
            "GPT-3ï¼š1750äº¿å‚æ•°",
            "æŠ€æœ¯å½±å“åˆ†æ"
        ]
    }
]

@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    accuracy: float  # å‡†ç¡®æ€§ (0-1)
    relevance: float  # ç›¸å…³æ€§ (0-1)
    faithfulness: float  # çœŸå®æ€§ (0-1)
    recall: float  # å¬å›ç‡ (0-1)
    completeness: float  # å®Œæ•´æ€§ (0-1)
    consistency: float  # ä¸€è‡´æ€§ (0-1)
    citation_quality: float  # å¼•ç”¨è´¨é‡ (0-1)
    response_time: float  # å“åº”æ—¶é—´ (ç§’)

@dataclass
class RAGTestResult:
    """å•ä¸ªRAGæ¨¡å¼çš„æµ‹è¯•ç»“æœ"""
    rag_mode: str
    query_id: str
    response: str
    sources_count: int
    processing_time: float
    metrics: EvaluationMetrics
    raw_metadata: Dict[str, Any]

class RAGEvaluator:
    """RAGè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.config = RAGConfig(chunk_size=800, overlap=100, top_k=5)
        self.results: List[RAGTestResult] = []
    
    def evaluate_response(self, query: Dict, response: str, sources: List, processing_time: float) -> EvaluationMetrics:
        """è¯„ä¼°å“åº”è´¨é‡"""
        
        # 1. å‡†ç¡®æ€§è¯„ä¼° - æ£€æŸ¥æœŸæœ›äº‹å®æ˜¯å¦è¢«æ­£ç¡®æåŠ
        expected_facts = query["expected_facts"]
        found_facts = sum(1 for fact in expected_facts if any(keyword in response.lower() for keyword in fact.lower().split()[:3]))
        accuracy = found_facts / len(expected_facts)
        
        # 2. ç›¸å…³æ€§è¯„ä¼° - å“åº”ä¸æŸ¥è¯¢çš„ç›¸å…³ç¨‹åº¦
        query_keywords = set(query["question"].lower().split())
        response_keywords = set(response.lower().split())
        relevance = len(query_keywords.intersection(response_keywords)) / len(query_keywords)
        
        # 3. çœŸå®æ€§è¯„ä¼° - å“åº”æ˜¯å¦åŸºäºæä¾›çš„æºæ–‡æ¡£
        source_texts = " ".join([source.get('text', '') for source in sources])
        response_words = response.lower().split()
        faithful_words = sum(1 for word in response_words[:50] if word in source_texts.lower())
        faithfulness = faithful_words / min(len(response_words), 50) if response_words else 0
        
        # 4. å¬å›ç‡è¯„ä¼° - æ˜¯å¦æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯æº
        recall = min(len(sources) / 3, 1.0)  # æœŸæœ›è‡³å°‘3ä¸ªæº
        
        # 5. å®Œæ•´æ€§è¯„ä¼° - å“åº”é•¿åº¦å’Œè¦†ç›–åº¦
        completeness = min(len(response) / 500, 1.0)  # æœŸæœ›è‡³å°‘500å­—ç¬¦
        
        # 6. ä¸€è‡´æ€§è¯„ä¼° - ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çŸ›ç›¾
        consistency = 0.9 if "ä½†æ˜¯" not in response and "ç„¶è€Œ" not in response else 0.7
        
        # 7. å¼•ç”¨è´¨é‡è¯„ä¼° - æ£€æŸ¥citationæ ¼å¼
        citation_count = response.count('[') + response.count('ã€‘')
        citation_quality = min(citation_count / max(len(sources), 1), 1.0)
        
        return EvaluationMetrics(
            accuracy=accuracy,
            relevance=relevance,
            faithfulness=faithfulness,
            recall=recall,
            completeness=completeness,
            consistency=consistency,
            citation_quality=citation_quality,
            response_time=processing_time
        )
    
    async def test_rag_service(self, rag_service, service_name: str, query: Dict) -> RAGTestResult:
        """æµ‹è¯•å•ä¸ªRAGæœåŠ¡"""
        print(f"\nğŸ” Testing {service_name} on {query['id']}...")
        
        try:
            # å¤„ç†æ–‡æ¡£
            start_time = time.time()
            
            doc_result = await rag_service.process_document(
                COMPLEX_TEST_DOCUMENT,
                user_id='eval_user',
                metadata={'source': 'ai_comprehensive_report'}
            )
            
            if not doc_result.success:
                print(f"âŒ {service_name} document processing failed")
                return None
            
            # æ‰§è¡ŒæŸ¥è¯¢
            query_result = await rag_service.query(
                query["question"],
                user_id='eval_user'
            )
            
            processing_time = time.time() - start_time
            
            if not query_result.success:
                print(f"âŒ {service_name} query failed")
                return None
            
            # è¯„ä¼°ç»“æœ
            metrics = self.evaluate_response(
                query, 
                query_result.content, 
                query_result.sources,
                processing_time
            )
            
            result = RAGTestResult(
                rag_mode=service_name,
                query_id=query["id"],
                response=query_result.content,
                sources_count=len(query_result.sources),
                processing_time=processing_time,
                metrics=metrics,
                raw_metadata=query_result.metadata
            )
            
            print(f"âœ… {service_name} completed - Accuracy: {metrics.accuracy:.2f}, Sources: {len(query_result.sources)}")
            return result
            
        except Exception as e:
            print(f"âŒ {service_name} failed: {e}")
            return None

# è¿™ä¸ªå‡½æ•°å·²ç»åœ¨åé¢æ›´å®Œæ•´åœ°å®šä¹‰äº†ï¼Œåˆ é™¤è¿™ä¸ªä¸å®Œæ•´çš„ç‰ˆæœ¬

def analyze_results(results: List[RAGTestResult]) -> Dict[str, Any]:
    """åˆ†ææµ‹è¯•ç»“æœ"""
    
    if not results:
        return {"error": "No results to analyze"}
    
    # æŒ‰RAGæ¨¡å¼åˆ†ç»„
    by_mode = {}
    for result in results:
        if result.rag_mode not in by_mode:
            by_mode[result.rag_mode] = []
        by_mode[result.rag_mode].append(result)
    
    # è®¡ç®—æ¯ä¸ªæ¨¡å¼çš„å¹³å‡åˆ†æ•°
    mode_scores = {}
    for mode, mode_results in by_mode.items():
        metrics_sum = {
            'accuracy': 0, 'relevance': 0, 'faithfulness': 0, 'recall': 0,
            'completeness': 0, 'consistency': 0, 'citation_quality': 0, 'response_time': 0
        }
        
        for result in mode_results:
            metrics_sum['accuracy'] += result.metrics.accuracy
            metrics_sum['relevance'] += result.metrics.relevance
            metrics_sum['faithfulness'] += result.metrics.faithfulness
            metrics_sum['recall'] += result.metrics.recall
            metrics_sum['completeness'] += result.metrics.completeness
            metrics_sum['consistency'] += result.metrics.consistency
            metrics_sum['citation_quality'] += result.metrics.citation_quality
            metrics_sum['response_time'] += result.metrics.response_time
        
        count = len(mode_results)
        mode_scores[mode] = {
            'accuracy': metrics_sum['accuracy'] / count,
            'relevance': metrics_sum['relevance'] / count,
            'faithfulness': metrics_sum['faithfulness'] / count,
            'recall': metrics_sum['recall'] / count,
            'completeness': metrics_sum['completeness'] / count,
            'consistency': metrics_sum['consistency'] / count,
            'citation_quality': metrics_sum['citation_quality'] / count,
            'response_time': metrics_sum['response_time'] / count,
            'test_count': count
        }
        
        # è®¡ç®—ç»¼åˆè¯„åˆ† (åŠ æƒå¹³å‡)
        weights = {
            'accuracy': 0.25, 'relevance': 0.20, 'faithfulness': 0.20, 
            'recall': 0.15, 'completeness': 0.10, 'consistency': 0.05, 'citation_quality': 0.05
        }
        
        overall_score = sum(mode_scores[mode][metric] * weight for metric, weight in weights.items())
        mode_scores[mode]['overall_score'] = overall_score
    
    # æ’å
    ranking = sorted(mode_scores.items(), key=lambda x: x[1]['overall_score'], reverse=True)
    
    return {
        'mode_scores': mode_scores,
        'ranking': ranking,
        'total_tests': len(results)
    }

def generate_evaluation_report(results: List[RAGTestResult]) -> str:
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    
    analysis = analyze_results(results)
    
    if 'error' in analysis:
        return f"âŒ è¯„ä¼°å¤±è´¥: {analysis['error']}"
    
    report = []
    report.append("ğŸ† RAGæ¨¡å¼ç»¼åˆè¯„ä¼°æŠ¥å‘Š")
    report.append("=" * 80)
    report.append(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {analysis['total_tests']} (7ä¸ªæ¨¡å¼ Ã— 3ä¸ªæŸ¥è¯¢)")
    report.append("")
    
    # æ’è¡Œæ¦œ
    report.append("ğŸ¥‡ RAGæ¨¡å¼æ’è¡Œæ¦œ (æŒ‰ç»¼åˆè¯„åˆ†)")
    report.append("-" * 50)
    
    for i, (mode, scores) in enumerate(analysis['ranking']):
        rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£", "6ï¸âƒ£", "7ï¸âƒ£"][i] if i < 7 else "ğŸ”¢"
        report.append(f"{rank_emoji} {mode}: {scores['overall_score']:.3f}")
        report.append(f"   å‡†ç¡®æ€§:{scores['accuracy']:.2f} ç›¸å…³æ€§:{scores['relevance']:.2f} çœŸå®æ€§:{scores['faithfulness']:.2f}")
        report.append(f"   å¬å›ç‡:{scores['recall']:.2f} å®Œæ•´æ€§:{scores['completeness']:.2f} å¼•ç”¨è´¨é‡:{scores['citation_quality']:.2f}")
        report.append(f"   å¹³å‡å“åº”æ—¶é—´:{scores['response_time']:.2f}ç§’")
        report.append("")
    
    # è¯¦ç»†åˆ†æ
    report.append("ğŸ“ˆ è¯¦ç»†æ€§èƒ½åˆ†æ")
    report.append("-" * 50)
    
    # å„ç»´åº¦æœ€å¼ºè€…
    best_in_category = {}
    for category in ['accuracy', 'relevance', 'faithfulness', 'recall', 'completeness', 'consistency', 'citation_quality']:
        best_mode = max(analysis['mode_scores'].items(), key=lambda x: x[1][category])
        best_in_category[category] = (best_mode[0], best_mode[1][category])
    
    category_names = {
        'accuracy': 'å‡†ç¡®æ€§', 'relevance': 'ç›¸å…³æ€§', 'faithfulness': 'çœŸå®æ€§',
        'recall': 'å¬å›ç‡', 'completeness': 'å®Œæ•´æ€§', 'consistency': 'ä¸€è‡´æ€§', 'citation_quality': 'å¼•ç”¨è´¨é‡'
    }
    
    for category, (mode, score) in best_in_category.items():
        report.append(f"ğŸ… {category_names[category]}æœ€å¼º: {mode} ({score:.3f})")
    
    report.append("")
    
    # é€Ÿåº¦åˆ†æ
    fastest_mode = min(analysis['mode_scores'].items(), key=lambda x: x[1]['response_time'])
    report.append(f"âš¡ æœ€å¿«å“åº”: {fastest_mode[0]} ({fastest_mode[1]['response_time']:.2f}ç§’)")
    report.append("")
    
    # ç»“è®º
    winner = analysis['ranking'][0]
    report.append("ğŸ¯ è¯„ä¼°ç»“è®º")
    report.append("-" * 30)
    report.append(f"ğŸ‘‘ ç»¼åˆå®åŠ›æœ€å¼º: {winner[0]}")
    report.append(f"ğŸ”¥ ç»¼åˆè¯„åˆ†: {winner[1]['overall_score']:.3f}/1.000")
    
    if len(analysis['ranking']) > 1:
        runner_up = analysis['ranking'][1]
        score_diff = winner[1]['overall_score'] - runner_up[1]['overall_score']
        report.append(f"ğŸ¥ˆ äºšå†›: {runner_up[0]} (å·®è·: {score_diff:.3f})")
    
    return "\n".join(report)

async def run_comprehensive_evaluation():
    """è¿è¡Œç»¼åˆè¯„ä¼°"""
    print("ğŸš€ RAG COMPREHENSIVE EVALUATION - BATTLE OF THE MODES!")
    print("=" * 80)
    
    evaluator = RAGEvaluator()
    
    # RAGæœåŠ¡é…ç½®
    rag_services = [
        ("Simple RAG", "tools.services.data_analytics_service.services.digital_service.patterns.simple_rag_service", "SimpleRAGService"),
        ("RAPTOR RAG", "tools.services.data_analytics_service.services.digital_service.patterns.raptor_rag_service", "RAPTORRAGService"),
        ("Self RAG", "tools.services.data_analytics_service.services.digital_service.patterns.self_rag_service", "SelfRAGService"),
        ("CRAG RAG", "tools.services.data_analytics_service.services.digital_service.patterns.crag_rag_service", "CRAGRAGService"),
        ("Plan-RAG", "tools.services.data_analytics_service.services.digital_service.patterns.plan_rag_service", "PlanRAGRAGService"),
        ("HM-RAG", "tools.services.data_analytics_service.services.digital_service.patterns.hm_rag_service", "HMRAGRAGService"),
        ("Graph RAG", "tools.services.data_analytics_service.services.digital_service.patterns.graph_rag_service", "GraphRAGService")
    ]
    
    all_results = []
    
    for query in COMPLEX_QUERIES:
        print(f"\nğŸ“‹ QUERY {query['id']}: {query['question'][:80]}...")
        print(f"   Difficulty: {query['difficulty']}, Skills: {', '.join(query['required_skills'])}")
        print("-" * 80)
        
        for service_name, module, class_name in rag_services:
            try:
                # åŠ¨æ€å¯¼å…¥RAGæœåŠ¡
                exec(f"from {module} import {class_name}")
                service = eval(f"{class_name}(evaluator.config)")
                
                # è¿è¡Œæµ‹è¯•
                result = await evaluator.test_rag_service(service, service_name, query)
                if result:
                    all_results.append(result)
                    
            except Exception as e:
                print(f"âŒ {service_name} initialization failed: {e}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_evaluation_report(all_results)
    print("\n" + report)
    
    return all_results, report

if __name__ == "__main__":
    results, report = asyncio.run(run_comprehensive_evaluation())
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open('/Users/xenodennis/Documents/Fun/isA_MCP/rag_evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump([{
            'rag_mode': r.rag_mode,
            'query_id': r.query_id,
            'response_preview': r.response[:200],
            'sources_count': r.sources_count,
            'processing_time': r.processing_time,
            'metrics': {
                'accuracy': r.metrics.accuracy,
                'relevance': r.metrics.relevance,
                'faithfulness': r.metrics.faithfulness,
                'recall': r.metrics.recall,
                'completeness': r.metrics.completeness,
                'consistency': r.metrics.consistency,
                'citation_quality': r.metrics.citation_quality,
                'response_time': r.metrics.response_time
            }
        } for r in results], f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: rag_evaluation_results.json")
    print(f"ğŸ“Š æ€»å…±å®Œæˆ {len(results)} é¡¹æµ‹è¯•")