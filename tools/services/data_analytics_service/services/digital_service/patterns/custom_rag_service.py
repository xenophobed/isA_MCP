#!/usr/bin/env python3
"""
Custom RAG Service for Multimodal PDF Processing

ÂÆåÊï¥ÁöÑ PDF Â§öÊ®°ÊÄÅ RAG ÊúçÂä°ÔºåÂåÖÂê´Ôºö
1. Ingestion (Â≠òÂÇ®): PDF ÊèêÂèñ ‚Üí ÂõæÁâá VLM ÊèèËø∞ ‚Üí MinIO Â≠òÂÇ® ‚Üí Embedding ‚Üí Supabase pgvector  
2. Retrieval (Ê£ÄÁ¥¢): Êü•ËØ¢ ‚Üí Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÊú¨ÂíåÂõæÁâá
3. Generation (ÁîüÊàê): LLM ÁîüÊàêÁ≠îÊ°àÔºàÂåÖÂê´ÂõæÁâáÈìæÊé•Ôºâ

‰ΩøÁî®Áé∞ÊúâÁªÑ‰ª∂Ôºö
- PDFExtractService: PDF Â§ÑÁêÜÂíåÂõæÂÉèÊèêÂèñ
- ImageAnalyzer: VLM ÂõæÂÉèÊèèËø∞ÁîüÊàê
- MinIOIntegration: ÂõæÁâáÂ≠òÂÇ®Âíå URL ÁîüÊàê
- EmbeddingIntegration: ÊñáÊú¨ embedding ÁîüÊàê
- VectorDBIntegration: Supabase pgvector Â≠òÂÇ®
"""

import asyncio
import logging
import time
import base64
import uuid
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from datetime import datetime

# ÂØºÂÖ•Áé∞ÊúâÁªÑ‰ª∂
from ..pdf_extract_service import PDFExtractService
from tools.services.intelligence_service.vision.image_analyzer import analyze as vlm_analyze
from ..integrations.minio_integration import get_minio_integration
from ..integrations.embedding_integration import EmbeddingIntegration
from ..integrations.vector_db_integration import VectorDBIntegration
from ..config.analytics_config import VectorDBPolicy

logger = logging.getLogger(__name__)


class CustomRAGService:
    """
    Custom Multimodal RAG Service for PDF Processing
    
    Ê†∏ÂøÉÂäüËÉΩÔºö
    1. ingest_pdf() - ÂÆåÊï¥ÁöÑ PDF ÊëÑÂèñÊµÅÁ®ãÔºàÊñáÊú¨+ÂõæÁâáÔºâ
    2. retrieve() - Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÂÜÖÂÆπÔºàÊñáÊú¨+ÂõæÁâáÔºâ  
    3. generate() - ÁîüÊàêÁ≠îÊ°àÔºàÂ∏¶ÂõæÁâáÂºïÁî®Ôºâ
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.service_name = "custom_rag_service"
        self.version = "1.0.0"
        
        # ÂàùÂßãÂåñÁªÑ‰ª∂
        self.pdf_extract_service = PDFExtractService(self.config)
        self.minio_integration = get_minio_integration(self.config)
        self.embedding_integration = EmbeddingIntegration()
        
        # ÂêëÈáèÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ
        vector_db_policy = VectorDBPolicy.STORAGE  # ‰ΩøÁî® Supabase
        self.vector_db_integration = VectorDBIntegration(
            policy=vector_db_policy,
            config=self.config
        )
        
        # ÈÖçÁΩÆÂèÇÊï∞
        self.chunk_size = self.config.get('chunk_size', 1000)
        self.chunk_overlap = self.config.get('chunk_overlap', 100)
        self.top_k = self.config.get('top_k_results', 5)
        
        logger.info(f"CustomRAGService initialized with vector DB: {vector_db_policy.value}")
    
    async def ingest_pdf(
        self,
        pdf_path: str,
        user_id: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ÂÆåÊï¥ÁöÑ PDF ÊëÑÂèñÊµÅÁ®ã
        
        ÊµÅÁ®ãÔºö
        1. ‰ΩøÁî® PDFExtractService ÊèêÂèñ PDFÔºàÊñáÊú¨ + ÂõæÁâáÔºâ
        2. ÂØπÊØè‰∏™ÂõæÁâá‰ΩøÁî® VLM ÁîüÊàêÊèèËø∞
        3. ‰∏ä‰º†ÂõæÁâáÂà∞ MinIOÔºåËé∑Âèñ URL
        4. ÊñáÊú¨ÂàÜÂùó + Embedding
        5. Â≠òÂÇ®Âà∞ Supabase pgvectorÔºàÊñáÊú¨ chunk + ÂõæÁâáÊèèËø∞ + ÂõæÁâá URLÔºâ
        
        Args:
            pdf_path: PDF Êñá‰ª∂Ë∑ØÂæÑ
            user_id: Áî®Êà∑ ID
            metadata: È¢ùÂ§ñÂÖÉÊï∞ÊçÆ
            
        Returns:
            Â≠òÂÇ®ÁªìÊûúÁªüËÆ°
        """
        start_time = time.time()
        
        try:
            metadata = metadata or {}
            pdf_name = Path(pdf_path).name
            
            logger.info(f"üì• ÂºÄÂßã PDF ÊëÑÂèñ: {pdf_name} (user: {user_id})")
            
            # ========== Èò∂ÊÆµ 1: È°µÈù¢Á∫ßÂ§öÊ®°ÊÄÅÂ§ÑÁêÜ ==========
            logger.info("üîß Èò∂ÊÆµ 1: È°µÈù¢Á∫ßÂ§öÊ®°ÊÄÅÂàÜÊûê...")
            
            # Â§ÑÁêÜÊØè‰∏ÄÈ°µÔºàÂõæ+Êñá‰∏ÄËµ∑Ôºâ
            page_records = await self._process_pages_multimodal(
                pdf_path, user_id, pdf_name, metadata
            )
            
            logger.info(f"‚úÖ Â§ÑÁêÜÂÆåÊàê: {len(page_records)} ‰∏™È°µÈù¢")
            
            # ========== Èò∂ÊÆµ 2: Â≠òÂÇ®Âà∞ Supabase ==========
            logger.info("üíæ Èò∂ÊÆµ 2: Â≠òÂÇ®Âà∞ Supabase pgvector...")
            storage_result = await self._store_pages_to_vector_db(
                user_id, page_records, pdf_name
            )
            
            processing_time = time.time() - start_time
            
            # ÁªüËÆ°‰ø°ÊÅØ
            total_images = sum(len(p.get('photo_urls', [])) for p in page_records)
            
            return {
                'success': True,
                'pdf_name': pdf_name,
                'user_id': user_id,
                'statistics': {
                    'pages_stored': len(page_records),
                    'images_stored': total_images,
                    'total_records': len(page_records),
                    'processing_time': processing_time
                },
                'storage_result': storage_result,
                'processing_time': processing_time
            }
            
        except Exception as e:
            logger.error(f"‚ùå PDF ÊëÑÂèñÂ§±Ë¥•: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    async def _process_pages_multimodal(
        self,
        pdf_path: str,
        user_id: str,
        pdf_name: str,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        È°µÈù¢Á∫ßÂ§öÊ®°ÊÄÅÂ§ÑÁêÜÔºöÊØèÈ°µ‰Ωú‰∏∫‰∏Ä‰∏™ chunk
        
        ÊµÅÁ®ãÔºö
        1. VLM ÂàÜÊûêÊï¥È°µÔºàpage as photoÔºâ ‚Üí summary + photo descriptions
        2. PDF Processor ÊèêÂèñÊñáÂ≠ó
        3. PDF Processor ÊèêÂèñÂõæÁâá ‚Üí MinIO
        4. ÂêàÂπ∂: VLM_output + page_text ‚Üí embedding
        
        Returns:
            È°µÈù¢ËÆ∞ÂΩïÂàóË°®ÔºåÊØè‰∏™È°µÈù¢‰∏ÄÊù°ËÆ∞ÂΩï
        """
        from tools.services.data_analytics_service.processors.file_processors.pdf_processor import PDFProcessor
        
        pdf_processor = PDFProcessor(self.config)
        page_records = []
        
        # 1. ÊèêÂèñ PDF ÂÆåÊï¥‰ø°ÊÅØÔºàÊñáÂ≠ó + ÂõæÁâáÔºâ
        logger.info("üìÑ ÊèêÂèñ PDF ÂÜÖÂÆπ...")
        result = await pdf_processor.process_pdf_unified(pdf_path, {
            'extract_text': True,
            'extract_images': True,
            'extract_tables': False
        })
        
        if not result.get('success'):
            logger.error(f"PDF ÊèêÂèñÂ§±Ë¥•: {result.get('error')}")
            return []
        
        # ÊèêÂèñÊñáÂ≠óÔºàÊåâÈ°µÔºâ - ‰øÆÂ§çÔºö‰ΩøÁî® text_extraction ËÄå‰∏çÊòØ text_content
        text_extraction = result.get('text_extraction', {})
        pages_text_list = text_extraction.get('pages', [])  # Â≠óÁ¨¶‰∏≤ÂàóË°®ÔºåÊØè‰∏™Â≠óÁ¨¶‰∏≤ÊòØ‰∏ÄÈ°µÊñáÂ≠ó
        logger.info(f"ÊèêÂèñÂà∞ {len(pages_text_list)} ‰∏™È°µÈù¢ÁöÑÊñáÂ≠ó")
        
        # ÊèêÂèñÂõæÁâáÔºàÊåâÈ°µÂàÜÁªÑÔºâ
        image_analysis = result.get('image_analysis', {})
        all_images = image_analysis.get('extracted_images', [])
        logger.info(f"ÊèêÂèñÂà∞ {len(all_images)} Âº†ÂõæÁâá")
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÈ°µÈù¢
        if not pages_text_list:
            logger.warning("‚ùå Ê≤°ÊúâÊèêÂèñÂà∞‰ªª‰ΩïÈ°µÈù¢ÔºÅ")
            return []
        
        # ÈôêÂà∂Â§ÑÁêÜÁöÑÈ°µÈù¢Êï∞Èáè
        max_pages = self.config.get('max_pages')
        if max_pages and len(pages_text_list) > max_pages:
            logger.info(f"‚ö†Ô∏è ÈôêÂà∂È°µÈù¢Â§ÑÁêÜÊï∞Èáè: {len(pages_text_list)} -> {max_pages}")
            pages_text_list = pages_text_list[:max_pages]
        
        # 2. Âπ∂ÂèëÂ§ÑÁêÜÊØè‰∏ÄÈ°µ
        logger.info(f"üîÑ ÂºÄÂßãÂ§ÑÁêÜ {len(pages_text_list)} ‰∏™È°µÈù¢ÔºàÈ°µÈù¢Á∫ßÂ§öÊ®°ÊÄÅÂàÜÊûêÔºâ...")
        
        # ÂàõÂª∫ semaphore ÈôêÂà∂Âπ∂ÂèëÊï∞
        max_concurrent = self.config.get('max_concurrent_pages', 3)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        tasks = []
        for page_idx, page_text in enumerate(pages_text_list, start=1):
            page_num = page_idx  # È°µÁ†Å‰ªé1ÂºÄÂßã
            
            # Ëé∑ÂèñËØ•È°µÁöÑÂõæÁâá
            page_images = [
                img for img in all_images 
                if img.get('page_number') == page_num
            ]
            
            task = self._process_single_page_multimodal(
                pdf_path=pdf_path,
                page_number=page_num,
                page_text=page_text,
                page_images=page_images,
                user_id=user_id,
                pdf_name=pdf_name,
                metadata=metadata,
                semaphore=semaphore
            )
            tasks.append(task)
        
        # Âπ∂ÂèëÊâßË°å
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Ê£ÄÊü•ÂºÇÂ∏∏
        for idx, r in enumerate(results):
            if isinstance(r, Exception):
                logger.error(f"È°µÈù¢{idx}Â§ÑÁêÜÂ§±Ë¥•: {r}")
        
        # ËøáÊª§ÊàêÂäüÁöÑÁªìÊûú
        page_records = [
            r for r in results 
            if r is not None and not isinstance(r, Exception)
        ]
        
        logger.info(f"‚úÖ È°µÈù¢Â§ÑÁêÜÂÆåÊàê: {len(page_records)}/{len(pages_text_list)} ÊàêÂäü")
        
        return page_records
    
    async def _process_single_page_multimodal(
        self,
        pdf_path: str,
        page_number: int,
        page_text: str,
        page_images: List[Dict[str, Any]],
        user_id: str,
        pdf_name: str,
        metadata: Dict[str, Any],
        semaphore: asyncio.Semaphore
    ) -> Optional[Dict[str, Any]]:
        """
        Â§ÑÁêÜÂçï‰∏™È°µÈù¢ÔºàÂ§öÊ®°ÊÄÅÔºâ
        
        Returns:
            È°µÈù¢ËÆ∞ÂΩï: {
                'page_number': int,
                'text': str,  # VLM summary + page text + photo descriptions
                'embedding': List[float],
                'photo_urls': List[str],  # MinIO URLs
                'metadata': {...}
            }
        """
        async with semaphore:
            try:
                logger.info(f"üìÑ Â§ÑÁêÜÈ°µÈù¢ {page_number}...")
                
                # 1. VLM ÂàÜÊûêÊï¥È°µÔºàÂèØÈÖçÁΩÆÊòØÂê¶ÂêØÁî®Ôºâ
                enable_vlm = self.config.get('enable_vlm_analysis', True)
                if enable_vlm:
                    page_summary, photo_descriptions = await self._analyze_page_with_vlm(
                        pdf_path, page_number, page_text, len(page_images)
                    )
                else:
                    # ÁÆÄÂåñÊ®°ÂºèÔºö‰∏ç‰ΩøÁî® VLM
                    page_summary = f"Á¨¨{page_number}È°µ"
                    photo_descriptions = [f"ÂõæÁâá{i+1}" for i in range(len(page_images))]
                    logger.info(f"‚ö†Ô∏è VLM ÂàÜÊûêÂ∑≤Á¶ÅÁî®ÔºàÁÆÄÂåñÊ®°ÂºèÔºâ")
                
                # 2. ‰∏ä‰º†È°µÈù¢ÂõæÁâáÂà∞ MinIOÔºàÂèØÈÖçÁΩÆÊòØÂê¶ÂêØÁî®Ôºâ
                enable_minio = self.config.get('enable_minio_upload', True)
                photo_urls = []
                if enable_minio:
                    for idx, img_data in enumerate(page_images):
                        try:
                            photo_url = await self._upload_image_to_minio(
                                img_data, user_id, pdf_name, page_number, idx
                            )
                            if photo_url:
                                photo_urls.append(photo_url)
                        except Exception as e:
                            logger.warning(f"ÂõæÁâá‰∏ä‰º†Â§±Ë¥• (page={page_number}, img={idx}): {e}")
                            continue
                else:
                    # ÁÆÄÂåñÊ®°ÂºèÔºö‰∏ç‰∏ä‰º† MinIOÔºåÂè™ËÆ∞ÂΩïÊï∞Èáè
                    photo_urls = [f"placeholder_url_{i}" for i in range(len(page_images))]
                    logger.info(f"‚ö†Ô∏è MinIO ‰∏ä‰º†Â∑≤Á¶ÅÁî®ÔºàÁÆÄÂåñÊ®°ÂºèÔºâ")
                
                # 3. ÂêàÂπ∂ÊñáÊú¨ÔºöVLM summary + page text + photo descriptions
                combined_text = self._combine_page_content(
                    page_summary, page_text, photo_descriptions, photo_urls
                )
                
                # 4. ÁîüÊàê embedding
                embedding = await self.embedding_integration.embed_text(combined_text)
                
                # 5. ÂàõÂª∫È°µÈù¢ËÆ∞ÂΩï
                page_record = {
                    'page_number': page_number,
                    'text': combined_text,
                    'embedding': embedding,
                    'photo_urls': photo_urls,
                    'metadata': {
                        'pdf_name': pdf_name,
                        'page_number': page_number,
                        'page_summary': page_summary,
                        'num_photos': len(photo_urls),
                        'photo_descriptions': photo_descriptions,
                        'content_type': 'page',
                        **metadata
                    }
                }
                
                logger.info(f"‚úÖ È°µÈù¢ {page_number} Â§ÑÁêÜÂÆåÊàê ({len(photo_urls)} Âº†ÂõæÁâá)")
                return page_record
                
            except Exception as e:
                logger.error(f"‚ùå È°µÈù¢ {page_number} Â§ÑÁêÜÂ§±Ë¥•: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return None
    
    async def _analyze_page_with_vlm(
        self,
        pdf_path: str,
        page_number: int,
        page_text: str,
        num_photos: int
    ) -> Tuple[str, List[str]]:
        """
        VLM ÂàÜÊûêÊï¥‰∏™È°µÈù¢
        
        Returns:
            (page_summary, [photo_1_description, photo_2_description, ...])
        """
        try:
            # Ê∏≤ÊüìÈ°µÈù¢‰∏∫ÂõæÁâá
            page_image_bytes = await self._render_pdf_page_as_image(pdf_path, page_number)
            
            if not page_image_bytes:
                # Â¶ÇÊûúÊ∏≤ÊüìÂ§±Ë¥•Ôºå‰ΩøÁî®ÊñáÂ≠óÁîüÊàêÁÆÄÂçï summary
                return f"È°µÈù¢ {page_number}", []
            
            # VLM ÂàÜÊûê
            prompt = f"""
ÂàÜÊûêËøô‰∏™PDFÈ°µÈù¢ÔºàÁ¨¨{page_number}È°µÔºâ„ÄÇÈ°µÈù¢ÂåÖÂê´ {num_photos} Âº†ÂõæÁâá„ÄÇ

È°µÈù¢ÊñáÂ≠óÂÜÖÂÆπÔºö
{page_text[:800] if page_text else '(Êó†ÊñáÂ≠ó)'}...

ËØ∑Êèê‰æõÔºö
1. page_summary: Áî®1-2Âè•ËØùÊ¶ÇÊã¨È°µÈù¢‰∏ªÈ¢ò
2. photo_1: Á¨¨1Âº†ÂõæÁâáÁöÑÂÜÖÂÆπÊèèËø∞ÔºàÂ¶ÇÊûúÊúâÔºâ
3. photo_2: Á¨¨2Âº†ÂõæÁâáÁöÑÂÜÖÂÆπÊèèËø∞ÔºàÂ¶ÇÊûúÊúâÔºâ
...

Ê†ºÂºèÔºö
page_summary: xxx
photo_1: xxx  
photo_2: xxx
"""
            
            result = await vlm_analyze(
                image=page_image_bytes,
                prompt=prompt,
                model="gpt-4o-mini",
                provider="yyds"
            )
            
            # Ëß£Êûê VLM ËæìÂá∫
            if result.success:
                response_text = result.response.strip()
                page_summary, photo_descriptions = self._parse_vlm_response(response_text, num_photos)
                return page_summary, photo_descriptions
            else:
                logger.warning(f"VLM ÂàÜÊûêÂ§±Ë¥•: {result.error}")
                return f"È°µÈù¢ {page_number}", []
                
        except Exception as e:
            logger.error(f"VLM È°µÈù¢ÂàÜÊûêÂ§±Ë¥•: {e}")
            return f"È°µÈù¢ {page_number}", []
    
    async def _render_pdf_page_as_image(
        self,
        pdf_path: str,
        page_number: int
    ) -> Optional[bytes]:
        """Â∞Ü PDF È°µÈù¢Ê∏≤Êüì‰∏∫ÂõæÁâáÔºàÁî®‰∫é VLM ÂàÜÊûêÔºâ"""
        try:
            import fitz  # PyMuPDF
            
            doc = fitz.open(pdf_path)
            page = doc[page_number - 1]  # PyMuPDF uses 0-based indexing
            
            # Ê∏≤Êüì‰∏∫ÂõæÁâáÔºàÈ´òË¥®ÈáèÔºâ
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom for quality
            img_bytes = pix.tobytes("png")
            
            doc.close()
            return img_bytes
            
        except Exception as e:
            logger.error(f"È°µÈù¢Ê∏≤ÊüìÂ§±Ë¥• (page={page_number}): {e}")
            return None
    
    def _parse_vlm_response(self, response: str, num_photos: int) -> Tuple[str, List[str]]:
        """Ëß£Êûê VLM ËæìÂá∫"""
        page_summary = ""
        photo_descriptions = []
        
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('page_summary:'):
                page_summary = line.split(':', 1)[1].strip()
            elif line.startswith('photo_'):
                desc = line.split(':', 1)[1].strip() if ':' in line else ""
                photo_descriptions.append(desc)
        
        # Â¶ÇÊûúÊ≤°ÊúâËß£ÊûêÂà∞ summaryÔºå‰ΩøÁî®Á¨¨‰∏ÄË°å
        if not page_summary and lines:
            page_summary = lines[0][:200]
        
        return page_summary, photo_descriptions
    
    def _combine_page_content(
        self,
        page_summary: str,
        page_text: str,
        photo_descriptions: List[str],
        photo_urls: List[str]
    ) -> str:
        """ÂêàÂπ∂È°µÈù¢ÂÜÖÂÆπ‰∏∫ÊúÄÁªàÁöÑÊñáÊú¨ÔºàÁî®‰∫é embeddingÔºâ"""
        parts = []
        
        # 1. È°µÈù¢ÊëòË¶Å
        if page_summary:
            parts.append(f"„ÄêÈ°µÈù¢Ê¶ÇË¶Å„Äë{page_summary}")
        
        # 2. È°µÈù¢ÊñáÂ≠ó
        if page_text.strip():
            parts.append(f"\n„ÄêÈ°µÈù¢ÊñáÂ≠ó„Äë\n{page_text}")
        
        # 3. ÂõæÁâáÊèèËø∞ + URL
        if photo_descriptions:
            parts.append("\n„ÄêÈ°µÈù¢ÂõæÁâá„Äë")
            for idx, (desc, url) in enumerate(zip(photo_descriptions, photo_urls), 1):
                parts.append(f"ÂõæÁâá{idx}: {desc}")
                if url:
                    parts.append(f"  ÈìæÊé•: {url}")
        
        return "\n".join(parts)
    
    async def _upload_image_to_minio(
        self,
        img_data: Dict[str, Any],
        user_id: str,
        pdf_name: str,
        page_number: int,
        image_index: int
    ) -> Optional[str]:
        """‰∏ä‰º†ÂõæÁâáÂà∞ MinIO"""
        try:
            # Ëß£Á†ÅÂõæÁâáÊï∞ÊçÆ
            image_data_str = img_data.get('image_data', '')
            if not image_data_str:
                return None
            
            # ÁßªÈô§ Data URL ÂâçÁºÄ
            if image_data_str.startswith('data:'):
                if ';base64,' in image_data_str:
                    image_data_base64 = image_data_str.split(';base64,', 1)[1]
                else:
                    return None
            else:
                image_data_base64 = image_data_str
            
            # Ëß£Á†Å base64
            image_bytes = base64.b64decode(image_data_base64)
            
            # ‰∏ä‰º†Âà∞ MinIO
            image_format = img_data.get('format', 'png')
            image_name = f"{Path(pdf_name).stem}_page{page_number}_img{image_index}.{image_format}"
            
            minio_result = await self.minio_integration.upload_image(
                image_data=image_bytes,
                user_id=user_id,
                image_name=image_name
            )
            
            if minio_result.get('success'):
                return minio_result.get('presigned_url')
            else:
                return None
                
        except Exception as e:
            logger.warning(f"ÂõæÁâá‰∏ä‰º†Â§±Ë¥•: {e}")
            return None
    
    async def _extract_pdf_images(self, pdf_path: str) -> Dict[str, Any]:
        """ÊèêÂèñ PDF ‰∏≠ÁöÑÊâÄÊúâÂõæÁâáÔºà‰ΩøÁî® PDFProcessorÔºâ"""
        try:
            from tools.services.data_analytics_service.processors.file_processors.pdf_processor import PDFProcessor
            
            pdf_processor = PDFProcessor(self.config)
            result = await pdf_processor.process_pdf_unified(pdf_path, {
                'extract_text': False,
                'extract_images': True,
                'extract_tables': False
            })
            
            if not result.get('success'):
                return {'success': False, 'images': []}
            
            # ÊèêÂèñÊâÄÊúâÂõæÁâáÊï∞ÊçÆ
            image_analysis = result.get('image_analysis', {})
            extracted_images = image_analysis.get('extracted_images', [])
            
            images_data = []
            for img in extracted_images:
                image_data_str = img.get('image_data', '')
                if image_data_str:
                    # Ëß£Á†Å base64 ‰∏∫Â≠óËäÇ
                    try:
                        # ÁßªÈô§ Data URL ÂâçÁºÄ (data:image/xxx;base64,)
                        if image_data_str.startswith('data:'):
                            # ÊâæÂà∞ base64 Êï∞ÊçÆÈÉ®ÂàÜ
                            if ';base64,' in image_data_str:
                                image_data_base64 = image_data_str.split(';base64,', 1)[1]
                            else:
                                logger.warning(f"ÂõæÁâáÊï∞ÊçÆÊ†ºÂºè‰∏çÊ≠£Á°Æ: {image_data_str[:50]}")
                                continue
                        else:
                            image_data_base64 = image_data_str
                        
                        # Ëß£Á†Å base64
                        image_bytes = base64.b64decode(image_data_base64)
                        images_data.append({
                            'page_number': img.get('page_number', 0),
                            'image_index': img.get('image_index', 0),
                            'image_bytes': image_bytes,
                            'format': img.get('format', 'png'),
                            'size': f"{img.get('width', 0)}x{img.get('height', 0)}"
                        })
                    except Exception as e:
                        logger.warning(f"ÂõæÁâáËß£Á†ÅÂ§±Ë¥• (page={img.get('page_number')}, idx={img.get('image_index')}): {e}")
                        continue
            
            return {
                'success': True,
                'images': images_data
            }
            
        except Exception as e:
            logger.error(f"PDF ÂõæÁâáÊèêÂèñÂ§±Ë¥•: {e}")
            return {'success': False, 'images': []}
    
    async def _process_single_image(
        self,
        img_data: Dict[str, Any],
        idx: int,
        total: int,
        user_id: str,
        pdf_name: str,
        semaphore: asyncio.Semaphore
    ) -> Optional[Dict[str, Any]]:
        """Â§ÑÁêÜÂçïÂº†ÂõæÁâáÔºàÂπ∂ÂèëÂÆâÂÖ®Ôºâ"""
        async with semaphore:
            try:
                page_num = img_data.get('page_number', 0)
                image_bytes = img_data.get('image_bytes')
                image_format = img_data.get('format', 'png')
                
                logger.info(f"Â§ÑÁêÜÂõæÁâá {idx+1}/{total}: È°µ {page_num}")
                
                # 1. ‰ΩøÁî® VLM ÁîüÊàêÂõæÁâáÊèèËø∞
                description = await self._generate_image_description(image_bytes)
                
                # 2. ‰∏ä‰º†Âà∞ MinIO
                image_name = f"{Path(pdf_name).stem}_page{page_num}_img{idx}.{image_format}"
                minio_result = await self.minio_integration.upload_image(
                    image_data=image_bytes,
                    user_id=user_id,
                    image_name=image_name
                )
                
                if not minio_result.get('success'):
                    logger.warning(f"ÂõæÁâá‰∏ä‰º† MinIO Â§±Ë¥•: {image_name}")
                    return None
                
                # 3. ÁîüÊàêÊèèËø∞ÁöÑ embedding
                description_embedding = await self.embedding_integration.embed_text(description)
                
                # 4. ÂàõÂª∫ÂõæÁâáËÆ∞ÂΩï
                image_record = {
                    'type': 'image',
                    'page_number': page_num,
                    'image_index': idx,
                    'description': description,
                    'embedding': description_embedding,
                    'minio_url': minio_result.get('presigned_url'),
                    'minio_path': minio_result.get('object_path'),
                    'metadata': {
                        'pdf_name': pdf_name,
                        'image_name': image_name,
                        'size': img_data.get('size', 'unknown'),
                        'format': image_format,
                        'content_type': 'image'
                    }
                }
                
                logger.info(f"‚úÖ ÂõæÁâá {idx+1} Â§ÑÁêÜÂÆåÊàê")
                return image_record
                
            except Exception as e:
                logger.error(f"ÂõæÁâá {idx+1} Â§ÑÁêÜÂ§±Ë¥•: {e}")
                return None
    
    async def _process_images(
        self,
        images_data: List[Dict[str, Any]],
        user_id: str,
        pdf_name: str
    ) -> List[Dict[str, Any]]:
        """
        Âπ∂ÂèëÂ§ÑÁêÜÂõæÁâáÔºöVLM ÊèèËø∞ÁîüÊàê + MinIO Â≠òÂÇ®
        
        ‰ΩøÁî® asyncio.gather + semaphore ÂÆûÁé∞Âπ∂ÂèëÊéßÂà∂
        
        Returns:
            ÂõæÁâáËÆ∞ÂΩïÂàóË°®ÔºåÂåÖÂê´ÊèèËø∞„ÄÅURL„ÄÅembedding
        """
        # ÂàõÂª∫ semaphore ÈôêÂà∂Âπ∂ÂèëÊï∞ÔºàÈÅøÂÖç API ÈôêÊµÅÔºâ
        max_concurrent = self.config.get('max_concurrent_images', 10)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        logger.info(f"ÂºÄÂßãÂπ∂ÂèëÂ§ÑÁêÜ {len(images_data)} Âº†ÂõæÁâáÔºàÊúÄÂ§ßÂπ∂ÂèëÊï∞: {max_concurrent}Ôºâ")
        
        # ÂàõÂª∫ÊâÄÊúâ‰ªªÂä°
        tasks = [
            self._process_single_image(
                img_data=img_data,
                idx=idx,
                total=len(images_data),
                user_id=user_id,
                pdf_name=pdf_name,
                semaphore=semaphore
            )
            for idx, img_data in enumerate(images_data)
        ]
        
        # Âπ∂ÂèëÊâßË°åÊâÄÊúâ‰ªªÂä°
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ËøáÊª§ÊéâÂ§±Ë¥•ÁöÑÁªìÊûú
        image_records = [
            result for result in results 
            if result is not None and not isinstance(result, Exception)
        ]
        
        logger.info(f"‚úÖ ÂõæÁâáÂ§ÑÁêÜÂÆåÊàê: {len(image_records)}/{len(images_data)} ÊàêÂäü")
        return image_records
    
    async def _generate_image_description(self, image_bytes: bytes) -> str:
        """‰ΩøÁî® VLM ÁîüÊàêÂõæÁâáÊèèËø∞"""
        try:
            # Ê£ÄÊü•ÂõæÁâáÂ§ßÂ∞èÔºàOpenAI ÈôêÂà∂ 20MBÔºåÊàë‰ª¨ËÆæÁΩÆ‰∏∫ 5MB ÂÆâÂÖ®ÂÄºÔºâ
            max_size_mb = 5
            size_mb = len(image_bytes) / (1024 * 1024)
            
            if size_mb > max_size_mb:
                logger.warning(f"ÂõæÁâáÂ§™Â§ß ({size_mb:.2f}MB > {max_size_mb}MB)ÔºåË∑≥ËøáÂàÜÊûê")
                return f"ÂõæÁâáÂÜÖÂÆπÔºàÊñá‰ª∂ËøáÂ§ß: {size_mb:.2f}MBÔºâ"
            
            # ‰ΩøÁî®Áé∞ÊúâÁöÑ ImageAnalyzer
            prompt = """ËØ∑ËØ¶ÁªÜÊèèËø∞ËøôÂº†ÂõæÁâáÁöÑÂÜÖÂÆπÔºåÂåÖÊã¨Ôºö
1. ‰∏ªË¶ÅÂÜÖÂÆπÂíåÂÖÉÁ¥†
2. ÂõæË°®Á±ªÂûãÔºàÂ¶ÇÊûúÊòØÂõæË°®Ôºâ
3. ÂÖ≥ÈîÆ‰ø°ÊÅØÂíåÊï∞ÊçÆ
4. Êìç‰ΩúËØ¥ÊòéÔºàÂ¶ÇÊûúÊòØÁïåÈù¢Êà™ÂõæÔºâ
5. ‰ªª‰ΩïÂèØËßÅÁöÑÊñáÂ≠óÂÜÖÂÆπ

ËØ∑Áî®‰∏≠ÊñáÊèèËø∞Ôºå‰øùÊåÅÁÆÄÊ¥ÅÊ∏ÖÊô∞„ÄÇ"""
            
            # Â∞ÜÂ≠óËäÇÊï∞ÊçÆÁºñÁ†Å‰∏∫ base64ÔºàImageAnalyzer ÊîØÊåÅÔºâ
            import base64
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            
            result = await vlm_analyze(
                image=image_base64,
                prompt=prompt,
                model="gpt-4o-mini",  # ‰ΩøÁî®ÊÄß‰ª∑ÊØîÈ´òÁöÑÊ®°Âûã
                provider="yyds"  # ‰ΩøÁî® yyds provider ÈÅøÂÖç OpenAI Rate Limit
            )
            
            if result.success:
                return result.response.strip()
            else:
                error_msg = str(result.error) if hasattr(result, 'error') else "Êú™Áü•ÈîôËØØ"
                logger.warning(f"VLM ÂàÜÊûêÂ§±Ë¥•: {error_msg}")
                return f"ÂõæÁâáÂÜÖÂÆπÔºàÊèèËø∞ÁîüÊàêÂ§±Ë¥•: {error_msg[:50]}Ôºâ"
                
        except Exception as e:
            logger.error(f"ÂõæÁâáÊèèËø∞ÁîüÊàêÂ§±Ë¥•: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return f"ÂõæÁâáÂÜÖÂÆπÔºàÂ§ÑÁêÜÂºÇÂ∏∏: {str(e)[:50]}Ôºâ"
    
    async def _process_text_chunks(
        self,
        text_chunks: List[Dict[str, Any]],
        user_id: str,
        pdf_name: str,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Â§ÑÁêÜÊñáÊú¨ÂùóÔºöÁîüÊàê embedding
        
        Args:
            text_chunks: ÊñáÊú¨ÂùóÂàóË°®ÔºåÊØè‰∏™ÂÖÉÁ¥†ÂåÖÂê´ {'text': str, 'page_number': int}
            
        Returns:
            ÊñáÊú¨ËÆ∞ÂΩïÂàóË°®ÔºåÂåÖÂê´ÊñáÊú¨„ÄÅembedding„ÄÅmetadata
        """
        text_records = []
        
        for idx, chunk_data in enumerate(text_chunks):
            try:
                # ÊèêÂèñÊñáÊú¨ÂíåÈ°µÁ†Å
                chunk_text = chunk_data.get('text', '')
                page_number = chunk_data.get('page_number', 0)
                
                if not chunk_text.strip():
                    continue
                
                # ÁîüÊàê embedding
                embedding = await self.embedding_integration.embed_text(chunk_text)
                
                # ÂàõÂª∫ÊñáÊú¨ËÆ∞ÂΩï
                text_record = {
                    'type': 'text',
                    'chunk_index': idx,
                    'page_number': page_number,  # ‚úÖ Ê∑ªÂä†È°µÁ†Å
                    'text': chunk_text,
                    'embedding': embedding,
                    'metadata': {
                        'pdf_name': pdf_name,
                        'chunk_index': idx,
                        'page_number': page_number,  # ‚úÖ metadata ‰∏≠‰πü‰øùÂ≠ò
                        'total_chunks': len(text_chunks),
                        'content_type': 'text',
                        **metadata
                    }
                }
                
                text_records.append(text_record)
                
            except Exception as e:
                logger.error(f"ÊñáÊú¨Âùó {idx} Â§ÑÁêÜÂ§±Ë¥•: {e}")
                continue
        
        return text_records
    
    async def _store_pages_to_vector_db(
        self,
        user_id: str,
        page_records: List[Dict[str, Any]],
        source_document: str
    ) -> Dict[str, Any]:
        """
        Â≠òÂÇ®È°µÈù¢ËÆ∞ÂΩïÂà∞ Supabase pgvector
        
        ÊØè‰∏™È°µÈù¢‰Ωú‰∏∫‰∏ÄÊù°ËÆ∞ÂΩïÔºåÂåÖÂê´Ôºö
        - text: VLM summary + page text + photo descriptions
        - embedding: ÊñáÊú¨ÁöÑ embedding
        - metadata: page_number, photo_urls, etc.
        """
        try:
            vector_db = self.vector_db_integration.vector_db
            
            # ÂáÜÂ§áÊâπÈáèÊèíÂÖ•Êï∞ÊçÆ
            insert_data = []
            for page_record in page_records:
                insert_data.append({
                    'user_id': user_id,
                    'text': page_record.get('text'),
                    'embedding_vector': page_record.get('embedding'),
                    'metadata': {
                        **page_record.get('metadata', {}),
                        'source_document': source_document,
                        'record_type': 'page',  # È°µÈù¢Á∫ßËÆ∞ÂΩï
                        'photo_urls': page_record.get('photo_urls', []),
                        'page_number': page_record.get('page_number'),
                    },
                    'source_document': source_document,
                    'created_at': datetime.now().isoformat()
                })
            
            # ÊâπÈáèÊèíÂÖ•
            logger.info(f"ÊâπÈáèÊèíÂÖ• {len(insert_data)} ‰∏™È°µÈù¢Âà∞ÂêëÈáèÊï∞ÊçÆÂ∫ì...")
            
            # ‰ΩøÁî® vector_db ÁöÑÊèíÂÖ•ÊñπÊ≥ï
            success_count = 0
            for idx, data in enumerate(insert_data):
                try:
                    # ÁîüÊàê UUID Ê†ºÂºèÁöÑÂîØ‰∏Ä ID
                    record_id = str(uuid.uuid4())
                    
                    await vector_db.store_vector(
                        id=record_id,
                        user_id=data['user_id'],
                        text=data['text'],
                        embedding=data['embedding_vector'],
                        metadata=data['metadata']
                    )
                    success_count += 1
                except Exception as e:
                    logger.warning(f"ÂçïÊù°ËÆ∞ÂΩïÊèíÂÖ•Â§±Ë¥•: {e}")
                    continue
            
            logger.info(f"‚úÖ ÊàêÂäüÊèíÂÖ• {success_count}/{len(insert_data)} Êù°ËÆ∞ÂΩï")
            
            return {
                'success': True,
                'total_records': len(insert_data),
                'success_count': success_count,
                'failed_count': len(insert_data) - success_count
            }
            
        except Exception as e:
            logger.error(f"ÂêëÈáèÊï∞ÊçÆÂ∫ìÂ≠òÂÇ®Â§±Ë¥•: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _store_to_vector_db(
        self,
        user_id: str,
        text_records: List[Dict[str, Any]],
        image_records: List[Dict[str, Any]],
        source_document: str
    ) -> Dict[str, Any]:
        """
        Â≠òÂÇ®ÊâÄÊúâËÆ∞ÂΩïÂà∞ Supabase pgvector
        
        ÊñáÊú¨ÂíåÂõæÁâáÊèèËø∞ÈÉΩÂ≠òÂÇ®‰∏∫Áã¨Á´ãÁöÑ embeddingÔºåmetadata ‰∏≠Ê†áËÆ∞Á±ªÂûã
        """
        try:
            vector_db = self.vector_db_integration.vector_db
            
            # ÂêàÂπ∂ÊâÄÊúâËÆ∞ÂΩï
            all_records = text_records + image_records
            
            # ÂáÜÂ§áÊâπÈáèÊèíÂÖ•Êï∞ÊçÆ
            insert_data = []
            for record in all_records:
                insert_data.append({
                    'user_id': user_id,
                    'text': record.get('text') or record.get('description'),
                    'embedding_vector': record.get('embedding'),
                    'metadata': {
                        **record.get('metadata', {}),
                        'source_document': source_document,
                        'record_type': record.get('type'),  # 'text' or 'image'
                        'minio_url': record.get('minio_url'),  # ‰ªÖÂõæÁâáÊúâ
                        'minio_path': record.get('minio_path'),  # ‰ªÖÂõæÁâáÊúâ
                        'page_number': record.get('page_number'),
                        'chunk_index': record.get('chunk_index'),
                        'image_index': record.get('image_index')
                    },
                    'source_document': source_document,
                    'created_at': datetime.now().isoformat()
                })
            
            # ÊâπÈáèÊèíÂÖ•
            logger.info(f"ÊâπÈáèÊèíÂÖ• {len(insert_data)} Êù°ËÆ∞ÂΩïÂà∞ÂêëÈáèÊï∞ÊçÆÂ∫ì...")
            
            # ‰ΩøÁî® vector_db ÁöÑÊèíÂÖ•ÊñπÊ≥ï
            success_count = 0
            for idx, data in enumerate(insert_data):
                try:
                    # ÁîüÊàê UUID Ê†ºÂºèÁöÑÂîØ‰∏Ä ID
                    record_id = str(uuid.uuid4())
                    
                    await vector_db.store_vector(
                        id=record_id,
                        user_id=data['user_id'],
                        text=data['text'],
                        embedding=data['embedding_vector'],
                        metadata=data['metadata']
                    )
                    success_count += 1
                except Exception as e:
                    logger.warning(f"ÂçïÊù°ËÆ∞ÂΩïÊèíÂÖ•Â§±Ë¥•: {e}")
                    continue
            
            logger.info(f"‚úÖ ÊàêÂäüÊèíÂÖ• {success_count}/{len(insert_data)} Êù°ËÆ∞ÂΩï")
            
            return {
                'success': True,
                'total_records': len(insert_data),
                'success_count': success_count,
                'failed_count': len(insert_data) - success_count
            }
            
        except Exception as e:
            logger.error(f"ÂêëÈáèÊï∞ÊçÆÂ∫ìÂ≠òÂÇ®Â§±Ë¥•: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def retrieve(
        self,
        user_id: str,
        query: str,
        top_k: Optional[int] = None,
        filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÂÜÖÂÆπÔºàÊñáÊú¨ + ÂõæÁâáÔºâ
        
        Args:
            user_id: Áî®Êà∑ ID
            query: Êü•ËØ¢ÊñáÊú¨
            top_k: ËøîÂõûÁªìÊûúÊï∞Èáè
            filters: È¢ùÂ§ñËøáÊª§Êù°‰ª∂ÔºàÂ¶Ç source_documentÔºâ
            
        Returns:
            Ê£ÄÁ¥¢ÁªìÊûúÔºåÂåÖÂê´ÊñáÊú¨ÂùóÂíåÁõ∏ÂÖ≥ÂõæÁâá
        """
        try:
            top_k = top_k or self.top_k
            
            logger.info(f"üîç Ê£ÄÁ¥¢Êü•ËØ¢: {query} (user: {user_id}, top_k: {top_k})")
            
            # 1. ÁîüÊàêÊü•ËØ¢ embedding
            query_embedding = await self.embedding_integration.embed_text(query)
            
            # 2. ‰ªéÂêëÈáèÊï∞ÊçÆÂ∫ìÊ£ÄÁ¥¢
            from tools.services.intelligence_service.vector_db.base_vector_db import VectorSearchConfig
            
            vector_db = self.vector_db_integration.vector_db
            search_config = VectorSearchConfig(
                top_k=top_k * 2,  # Â§öÊ£ÄÁ¥¢‰∏Ä‰∫õÔºåÂêéÁª≠ËøáÊª§
                filter_metadata=filters
            )
            search_results = await vector_db.search_vectors(
                query_embedding=query_embedding,
                user_id=user_id,
                config=search_config
            )
            
            # 3. Â§ÑÁêÜÊ£ÄÁ¥¢ÁªìÊûúÔºàÈ°µÈù¢Á∫ßÔºâ
            page_results = []
            
            for result in search_results:
                # SearchResult ÊòØÂØπË±°Ôºå‰ΩøÁî®Â±ûÊÄßËÆøÈóÆ
                metadata = result.metadata or {}
                record_type = metadata.get('record_type', 'page')
                
                if record_type == 'page':
                    # È°µÈù¢Á∫ßËÆ∞ÂΩï
                    page_results.append({
                        'text': result.text,
                        'page_number': metadata.get('page_number'),
                        'page_summary': metadata.get('page_summary'),
                        'similarity_score': result.score,
                        'photo_urls': metadata.get('photo_urls', []),
                        'num_photos': metadata.get('num_photos', 0),
                        'metadata': metadata
                    })
                else:
                    # ÂÖºÂÆπÊóßÊ†ºÂºèÔºàtext/imageÔºâ
                    page_results.append({
                        'text': result.text,
                        'page_number': metadata.get('page_number'),
                        'similarity_score': result.score,
                        'photo_urls': [metadata.get('minio_url')] if metadata.get('minio_url') else [],
                        'metadata': metadata
                    })
            
            # 4. ÈôêÂà∂ËøîÂõûÊï∞Èáè
            page_results = page_results[:top_k]
            
            # ÁªüËÆ°ÂõæÁâáÊï∞Èáè
            total_photos = sum(len(p.get('photo_urls', [])) for p in page_results)
            
            logger.info(f"‚úÖ Ê£ÄÁ¥¢ÂÆåÊàê: {len(page_results)} ‰∏™È°µÈù¢, {total_photos} Âº†ÂõæÁâá")
            
            return {
                'success': True,
                'query': query,
                'page_results': page_results,  # È°µÈù¢Á∫ßÁªìÊûú
                'total_pages': len(page_results),
                'total_photos': total_photos,
                # ÂÖºÂÆπÊóßÊé•Âè£
                'text_results': page_results,
                'image_results': []
            }
            
        except Exception as e:
            logger.error(f"Ê£ÄÁ¥¢Â§±Ë¥•: {e}")
            return {
                'success': False,
                'error': str(e),
                'page_results': [],
                'text_results': [],
                'image_results': []
            }
    
    async def generate(
        self,
        user_id: str,
        query: str,
        retrieval_result: Dict[str, Any],
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ÁîüÊàêÁ≠îÊ°àÔºàÂü∫‰∫éÊ£ÄÁ¥¢ÁªìÊûúÔºâ
        
        Args:
            user_id: Áî®Êà∑ ID
            query: Êü•ËØ¢ÈóÆÈ¢ò
            retrieval_result: Ê£ÄÁ¥¢ÁªìÊûúÔºàÊù•Ëá™ retrieve()Ôºâ
            generation_config: ÁîüÊàêÈÖçÁΩÆÔºàÊ®°Âûã„ÄÅÊ∏©Â∫¶Á≠âÔºâ
            
        Returns:
            ÁîüÊàêÁöÑÁ≠îÊ°àÔºåÂåÖÂê´ÂõæÁâáÂºïÁî®
        """
        try:
            generation_config = generation_config or {}
            
            logger.info(f"ü§ñ ÁîüÊàêÁ≠îÊ°à: {query}")
            
            # 1. ÊèêÂèñÊ£ÄÁ¥¢ÁªìÊûúÔºàÈ°µÈù¢Á∫ßÔºâ
            page_results = retrieval_result.get('page_results', [])
            
            # ÂÖºÂÆπÊóßÊ†ºÂºè
            if not page_results:
                page_results = retrieval_result.get('text_results', [])
            
            # 2. ÊûÑÂª∫‰∏ä‰∏ãÊñáÔºàÈ°µÈù¢Á∫ßÔºåÂåÖÂê´ÂõæÁâáÔºâ
            context_parts = []
            
            context_parts.append("## Áõ∏ÂÖ≥È°µÈù¢ÂÜÖÂÆπÔºö\n")
            for idx, result in enumerate(page_results, 1):
                page_num = result.get('page_number', 'Êú™Áü•')
                page_summary = result.get('page_summary', '')
                text = result.get('text', '')
                photo_urls = result.get('photo_urls', [])
                
                context_parts.append(f"\n[È°µÈù¢ {idx}] (È°µÁ†Å: {page_num})")
                
                # È°µÈù¢ÊëòË¶Å
                if page_summary:
                    context_parts.append(f"ÊëòË¶Å: {page_summary}")
                
                # È°µÈù¢ÂÜÖÂÆπÔºàÊà™ÂèñÈÉ®ÂàÜÔºâ
                if text:
                    content_preview = text[:500] if len(text) > 500 else text
                    context_parts.append(f"\nÂÜÖÂÆπ:\n{content_preview}...")
                
                # È°µÈù¢ÂõæÁâá
                if photo_urls:
                    context_parts.append(f"\nÂåÖÂê´ {len(photo_urls)} Âº†ÂõæÁâá:")
                    for photo_idx, url in enumerate(photo_urls, 1):
                        context_parts.append(f"  ÂõæÁâá{photo_idx}: {url}")
            
            context = "\n".join(context_parts)
            
            # 3. ÊûÑÂª∫ prompt
            system_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑ CRM Á≥ªÁªüÂä©Êâã„ÄÇÂü∫‰∫éÊèê‰æõÁöÑPDFÈ°µÈù¢ÂÜÖÂÆπÔºàÂåÖÂê´ÊñáÂ≠óÂíåÂõæÁâáÔºâÔºåÂõûÁ≠îÁî®Êà∑ÁöÑÈóÆÈ¢ò„ÄÇ

Ê≥®ÊÑèÔºö
1. ‰ºòÂÖà‰ΩøÁî®ÊñáÊ°£‰∏≠ÁöÑÂáÜÁ°Æ‰ø°ÊÅØ
2. ÊØè‰∏™È°µÈù¢ÂèØËÉΩÂåÖÂê´Â§öÂº†ÂõæÁâáÔºåÊ≥®ÊÑèÂºïÁî®ÂõæÁâáURL
3. Êèê‰æõÊ∏ÖÊô∞ÁöÑÊìç‰ΩúÊ≠•È™§ÂíåËØ¥Êòé
4. Â¶ÇÊûú‰ø°ÊÅØ‰∏çË∂≥ÔºåËØöÂÆûËØ¥Êòé
5. Âú®Á≠îÊ°à‰∏≠Êèê‰æõÂõæÁâáÈìæÊé•ÔºåÊñπ‰æøÁî®Êà∑Êü•Áúã"""
            
            user_prompt = f"""Âü∫‰∫é‰ª•‰∏ãÂÜÖÂÆπÔºåÂõûÁ≠îÈóÆÈ¢òÔºö

{context}

**Áî®Êà∑ÈóÆÈ¢ò**: {query}

ËØ∑Êèê‰æõËØ¶ÁªÜ„ÄÅÂáÜÁ°ÆÁöÑÁ≠îÊ°à„ÄÇÂ¶ÇÊûúÊúâÁõ∏ÂÖ≥ÂõæÁâáÔºåËØ∑Âú®Á≠îÊ°à‰∏≠ÂºïÁî®Âπ∂ËØ¥ÊòéÂ¶Ç‰ΩïÊü•Áúã„ÄÇ"""
            
            # 4. Ë∞ÉÁî® LLM ÁîüÊàêÁ≠îÊ°à
            from core.isa_client_factory import get_isa_client
            
            isa_client = get_isa_client()
            llm_result = await isa_client.invoke(
                input_data=user_prompt,
                task="chat",
                service_type="text",
                model=generation_config.get('model', 'gpt-4o-mini'),
                system_prompt=system_prompt,
                temperature=generation_config.get('temperature', 0.3),
                provider=generation_config.get('provider', 'yyds')  # ‰ΩøÁî® yyds provider
            )
            
            if not llm_result.get('success'):
                return {
                    'success': False,
                    'error': f"LLM ÁîüÊàêÂ§±Ë¥•: {llm_result.get('error')}"
                }
            
            # ÊèêÂèñÁ≠îÊ°àÊñáÊú¨ÔºàÂÖºÂÆπ‰∏çÂêåÁöÑËøîÂõûÊ†ºÂºèÔºâ
            answer = ""
            if 'result' in llm_result:
                result = llm_result['result']
                if isinstance(result, dict):
                    answer = result.get('text', '') or result.get('response', '')
                elif isinstance(result, str):
                    answer = result
            
            if not answer:
                answer = llm_result.get('response', '')
            
            logger.info(f"‚úÖ Á≠îÊ°àÁîüÊàêÂÆåÊàê")
            
            # ÁªüËÆ°Êù•Ê∫ê
            total_photos = sum(len(p.get('photo_urls', [])) for p in page_results)
            
            return {
                'success': True,
                'query': query,
                'answer': answer,
                'sources': {
                    'page_count': len(page_results),
                    'photo_count': total_photos,
                    'page_sources': page_results,
                    # ÂÖºÂÆπÊóßÊ†ºÂºè
                    'text_count': len(page_results),
                    'image_count': 0,
                    'text_sources': page_results,
                    'image_sources': []
                },
                'context_used': context
            }
            
        except Exception as e:
            logger.error(f"Á≠îÊ°àÁîüÊàêÂ§±Ë¥•: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def query_with_generation(
        self,
        user_id: str,
        query: str,
        filters: Optional[Dict[str, Any]] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ÂÆåÊï¥ÁöÑ RAG ÊµÅÁ®ãÔºöÊ£ÄÁ¥¢ + ÁîüÊàê
        
        ËøôÊòØ‰∏Ä‰∏™‰æøÊç∑ÊñπÊ≥ïÔºåÁªÑÂêà‰∫Ü retrieve() Âíå generate()
        """
        try:
            # 1. Ê£ÄÁ¥¢
            retrieval_result = await self.retrieve(
                user_id=user_id,
                query=query,
                filters=filters
            )
            
            if not retrieval_result.get('success'):
                return {
                    'success': False,
                    'error': f"Ê£ÄÁ¥¢Â§±Ë¥•: {retrieval_result.get('error')}"
                }
            
            # 2. ÁîüÊàê
            generation_result = await self.generate(
                user_id=user_id,
                query=query,
                retrieval_result=retrieval_result,
                generation_config=generation_config
            )
            
            return generation_result
            
        except Exception as e:
            logger.error(f"RAG Êü•ËØ¢Â§±Ë¥•: {e}")
            return {
                'success': False,
                'error': str(e)
            }


# ÂÖ®Â±ÄÂçï‰æã
_custom_rag_service = None

def get_custom_rag_service(config: Optional[Dict[str, Any]] = None) -> CustomRAGService:
    """Ëé∑Âèñ CustomRAGService Âçï‰æã"""
    global _custom_rag_service
    if _custom_rag_service is None:
        _custom_rag_service = CustomRAGService(config)
    return _custom_rag_service

