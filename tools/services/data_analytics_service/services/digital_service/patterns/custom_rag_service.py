#!/usr/bin/env python3
"""
Custom RAG Service for Multimodal PDF Processing

å®Œæ•´çš„ PDF å¤šæ¨¡æ€ RAG æœåŠ¡ï¼ŒåŒ…å«ï¼š
1. Ingestion (å­˜å‚¨): PDF æå– â†’ å›¾ç‰‡ VLM æè¿° â†’ MinIO å­˜å‚¨ â†’ Embedding â†’ Supabase pgvector  
2. Retrieval (æ£€ç´¢): æŸ¥è¯¢ â†’ æ£€ç´¢ç›¸å…³æ–‡æœ¬å’Œå›¾ç‰‡
3. Generation (ç”Ÿæˆ): LLM ç”Ÿæˆç­”æ¡ˆï¼ˆåŒ…å«å›¾ç‰‡é“¾æ¥ï¼‰

ä½¿ç”¨ç°æœ‰ç»„ä»¶ï¼š
- PDFExtractService: PDF å¤„ç†å’Œå›¾åƒæå–
- ImageAnalyzer: VLM å›¾åƒæè¿°ç”Ÿæˆ
- MinIOIntegration: å›¾ç‰‡å­˜å‚¨å’Œ URL ç”Ÿæˆ
- EmbeddingIntegration: æ–‡æœ¬ embedding ç”Ÿæˆ
- VectorDBIntegration: Supabase pgvector å­˜å‚¨
"""

import asyncio
import logging
import time
import base64
import uuid
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from datetime import datetime

# å¯¼å…¥ç°æœ‰ç»„ä»¶
from ..pdf_extract_service import PDFExtractService
from tools.services.intelligence_service.vision.image_analyzer import analyze as vlm_analyze
from ..integrations.minio_integration import get_minio_integration
from ..integrations.embedding_integration import EmbeddingIntegration
from ..integrations.vector_db_integration import VectorDBIntegration
from ..config.analytics_config import VectorDBPolicy

# å¯¼å…¥ ChunkingService for hybrid approach
from tools.services.intelligence_service.vector_db.chunking_service import (
    ChunkingService, ChunkingStrategy, ChunkConfig
)

logger = logging.getLogger(__name__)


class CustomRAGService:
    """
    Custom Multimodal RAG Service for PDF Processing
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ingest_pdf() - å®Œæ•´çš„ PDF æ‘„å–æµç¨‹ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰
    2. retrieve() - æ£€ç´¢ç›¸å…³å†…å®¹ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰  
    3. generate() - ç”Ÿæˆç­”æ¡ˆï¼ˆå¸¦å›¾ç‰‡å¼•ç”¨ï¼‰
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.service_name = "custom_rag_service"
        self.version = "1.0.0"
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.pdf_extract_service = PDFExtractService(self.config)
        self.minio_integration = get_minio_integration(self.config)
        self.embedding_integration = EmbeddingIntegration()
        
        # å‘é‡æ•°æ®åº“é…ç½®
        vector_db_policy = VectorDBPolicy.STORAGE  # ä½¿ç”¨ Supabase
        self.vector_db_integration = VectorDBIntegration(
            policy=vector_db_policy,
            config=self.config
        )
        
        # é…ç½®å‚æ•°
        self.chunk_size = self.config.get('chunk_size', 1000)
        self.chunk_overlap = self.config.get('chunk_overlap', 100)
        self.top_k = self.config.get('top_k_results', 5)

        # Chunking strategy: "page" (default) or "recursive", "semantic", etc.
        self.chunking_strategy = self.config.get('chunking_strategy', 'page')

        # Initialize ChunkingService for hybrid approach
        if self.chunking_strategy != 'page':
            self.chunking_service = ChunkingService()
            logger.info(f"Hybrid chunking enabled: strategy={self.chunking_strategy}")
        else:
            self.chunking_service = None
            logger.info("Page-level chunking enabled (default)")

        logger.info(f"CustomRAGService initialized with vector DB: {vector_db_policy.value}")
    
    async def ingest_pdf(
        self,
        pdf_path: str,
        user_id: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        å®Œæ•´çš„ PDF æ‘„å–æµç¨‹
        
        æµç¨‹ï¼š
        1. ä½¿ç”¨ PDFExtractService æå– PDFï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
        2. å¯¹æ¯ä¸ªå›¾ç‰‡ä½¿ç”¨ VLM ç”Ÿæˆæè¿°
        3. ä¸Šä¼ å›¾ç‰‡åˆ° MinIOï¼Œè·å– URL
        4. æ–‡æœ¬åˆ†å— + Embedding
        5. å­˜å‚¨åˆ° Supabase pgvectorï¼ˆæ–‡æœ¬ chunk + å›¾ç‰‡æè¿° + å›¾ç‰‡ URLï¼‰
        
        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            user_id: ç”¨æˆ· ID
            metadata: é¢å¤–å…ƒæ•°æ®
            
        Returns:
            å­˜å‚¨ç»“æœç»Ÿè®¡
        """
        start_time = time.time()
        
        try:
            metadata = metadata or {}
            pdf_name = Path(pdf_path).name
            
            logger.info(f"ğŸ“¥ å¼€å§‹ PDF æ‘„å–: {pdf_name} (user: {user_id})")
            
            # ========== é˜¶æ®µ 1: å¤šæ¨¡æ€å¤„ç† ==========
            if self.chunking_strategy == 'page':
                logger.info("ğŸ”§ é˜¶æ®µ 1: é¡µé¢çº§å¤šæ¨¡æ€åˆ†æ (Page-level)...")
                # é¡µé¢çº§ï¼šæ¯é¡µä¸€ä¸ª chunk
                page_records = await self._process_pages_multimodal(
                    pdf_path, user_id, pdf_name, metadata
                )
            else:
                logger.info(f"ğŸ”§ é˜¶æ®µ 1: æ··åˆå¤šæ¨¡æ€åˆ†æ (Hybrid: {self.chunking_strategy})...")
                # æ··åˆæ¨¡å¼ï¼šVLM åˆ†æé¡µé¢ + æ–‡æœ¬åˆ†å—
                page_records = await self._process_pages_hybrid(
                    pdf_path, user_id, pdf_name, metadata
                )

            logger.info(f"âœ… å¤„ç†å®Œæˆ: {len(page_records)} ä¸ª chunks")
            
            # ========== é˜¶æ®µ 2: å­˜å‚¨åˆ° Supabase ==========
            logger.info("ğŸ’¾ é˜¶æ®µ 2: å­˜å‚¨åˆ° Supabase pgvector...")
            storage_result = await self._store_pages_to_vector_db(
                user_id, page_records, pdf_name
            )
            
            processing_time = time.time() - start_time
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_images = sum(len(p.get('photo_urls', [])) for p in page_records)
            
            return {
                'success': True,
                'pdf_name': pdf_name,
                'user_id': user_id,
                'statistics': {
                    'pages_stored': len(page_records),
                    'images_stored': total_images,
                    'total_records': len(page_records),
                    'processing_time': processing_time
                },
                'storage_result': storage_result,
                'processing_time': processing_time
            }
            
        except Exception as e:
            logger.error(f"âŒ PDF æ‘„å–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    async def _process_pages_multimodal(
        self,
        pdf_path: str,
        user_id: str,
        pdf_name: str,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        é¡µé¢çº§å¤šæ¨¡æ€å¤„ç†ï¼šæ¯é¡µä½œä¸ºä¸€ä¸ª chunk
        
        æµç¨‹ï¼š
        1. VLM åˆ†ææ•´é¡µï¼ˆpage as photoï¼‰ â†’ summary + photo descriptions
        2. PDF Processor æå–æ–‡å­—
        3. PDF Processor æå–å›¾ç‰‡ â†’ MinIO
        4. åˆå¹¶: VLM_output + page_text â†’ embedding
        
        Returns:
            é¡µé¢è®°å½•åˆ—è¡¨ï¼Œæ¯ä¸ªé¡µé¢ä¸€æ¡è®°å½•
        """
        from tools.services.data_analytics_service.processors.file_processors.pdf_processor import PDFProcessor
        
        pdf_processor = PDFProcessor(self.config)
        page_records = []
        
        # 1. æå– PDF å®Œæ•´ä¿¡æ¯ï¼ˆæ–‡å­— + å›¾ç‰‡ï¼‰
        logger.info("ğŸ“„ æå– PDF å†…å®¹...")
        result = await pdf_processor.process_pdf_unified(pdf_path, {
            'extract_text': True,
            'extract_images': True,
            'extract_tables': False
        })
        
        if not result.get('success'):
            logger.error(f"PDF æå–å¤±è´¥: {result.get('error')}")
            return []
        
        # æå–æ–‡å­—ï¼ˆæŒ‰é¡µï¼‰ - ä¿®å¤ï¼šä½¿ç”¨ text_extraction è€Œä¸æ˜¯ text_content
        text_extraction = result.get('text_extraction', {})
        pages_text_list = text_extraction.get('pages', [])  # å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¯ä¸ªå­—ç¬¦ä¸²æ˜¯ä¸€é¡µæ–‡å­—
        logger.info(f"æå–åˆ° {len(pages_text_list)} ä¸ªé¡µé¢çš„æ–‡å­—")
        
        # æå–å›¾ç‰‡ï¼ˆæŒ‰é¡µåˆ†ç»„ï¼‰
        image_analysis = result.get('image_analysis', {})
        all_images = image_analysis.get('extracted_images', [])
        logger.info(f"æå–åˆ° {len(all_images)} å¼ å›¾ç‰‡")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é¡µé¢
        if not pages_text_list:
            logger.warning("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•é¡µé¢ï¼")
            return []
        
        # é™åˆ¶å¤„ç†çš„é¡µé¢æ•°é‡
        max_pages = self.config.get('max_pages')
        if max_pages and len(pages_text_list) > max_pages:
            logger.info(f"âš ï¸ é™åˆ¶é¡µé¢å¤„ç†æ•°é‡: {len(pages_text_list)} -> {max_pages}")
            pages_text_list = pages_text_list[:max_pages]
        
        # 2. å¹¶å‘å¤„ç†æ¯ä¸€é¡µ
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç† {len(pages_text_list)} ä¸ªé¡µé¢ï¼ˆé¡µé¢çº§å¤šæ¨¡æ€åˆ†æï¼‰...")
        
        # åˆ›å»º semaphore é™åˆ¶å¹¶å‘æ•°
        max_concurrent = self.config.get('max_concurrent_pages', 3)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        tasks = []
        for page_idx, page_text in enumerate(pages_text_list, start=1):
            page_num = page_idx  # é¡µç ä»1å¼€å§‹
            
            # è·å–è¯¥é¡µçš„å›¾ç‰‡
            page_images = [
                img for img in all_images 
                if img.get('page_number') == page_num
            ]
            
            task = self._process_single_page_multimodal(
                pdf_path=pdf_path,
                page_number=page_num,
                page_text=page_text,
                page_images=page_images,
                user_id=user_id,
                pdf_name=pdf_name,
                metadata=metadata,
                semaphore=semaphore
            )
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ£€æŸ¥å¼‚å¸¸
        for idx, r in enumerate(results):
            if isinstance(r, Exception):
                logger.error(f"é¡µé¢{idx}å¤„ç†å¤±è´¥: {r}")
        
        # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
        page_records = [
            r for r in results 
            if r is not None and not isinstance(r, Exception)
        ]
        
        logger.info(f"âœ… é¡µé¢å¤„ç†å®Œæˆ: {len(page_records)}/{len(pages_text_list)} æˆåŠŸ")
        
        return page_records

    async def _process_pages_hybrid(
        self,
        pdf_path: str,
        user_id: str,
        pdf_name: str,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæ¨¡å¼å¤„ç†ï¼šVLM åˆ†æé¡µé¢ + æ–‡æœ¬ç»†ç²’åº¦åˆ†å—

        æµç¨‹ï¼š
        1. æŒ‰é¡µæå–æ–‡æœ¬
        2. VLM åˆ†ææ•´é¡µ (multimodal) + ä¸Šä¼ å›¾ç‰‡åˆ° MinIO
        3. å¯¹æ¯é¡µæ–‡æœ¬è¿›è¡Œç»†ç²’åº¦åˆ†å— (ChunkingService)
        4. æ¯ä¸ªæ–‡æœ¬ chunk å…³è” page_id + photo_urls

        Returns:
            chunk è®°å½•åˆ—è¡¨ï¼ˆæ¯”é¡µé¢æ•°é‡å¤šï¼Œå› ä¸ºæ¯é¡µè¢«åˆ†æˆå¤šä¸ª chunksï¼‰
        """
        from tools.services.data_analytics_service.processors.file_processors.pdf_processor import PDFProcessor

        pdf_processor = PDFProcessor(self.config)
        all_chunk_records = []

        # 1. æå– PDF å®Œæ•´ä¿¡æ¯
        logger.info("ğŸ“„ æå– PDF å†…å®¹...")
        result = await pdf_processor.process_pdf_unified(pdf_path, {
            'extract_text': True,
            'extract_images': True,
            'extract_tables': False
        })

        if not result.get('success'):
            logger.error(f"PDF æå–å¤±è´¥: {result.get('error')}")
            return []

        # æå–æ–‡å­—ï¼ˆæŒ‰é¡µï¼‰
        text_extraction = result.get('text_extraction', {})
        pages_text_list = text_extraction.get('pages', [])
        logger.info(f"æå–åˆ° {len(pages_text_list)} ä¸ªé¡µé¢çš„æ–‡å­—")

        # æå–å›¾ç‰‡ï¼ˆæŒ‰é¡µåˆ†ç»„ï¼‰
        image_analysis = result.get('image_analysis', {})
        all_images = image_analysis.get('extracted_images', [])
        logger.info(f"æå–åˆ° {len(all_images)} å¼ å›¾ç‰‡")

        if not pages_text_list:
            logger.warning("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•é¡µé¢ï¼")
            return []

        # é™åˆ¶å¤„ç†çš„é¡µé¢æ•°é‡
        max_pages = self.config.get('max_pages')
        if max_pages and len(pages_text_list) > max_pages:
            logger.info(f"âš ï¸ é™åˆ¶é¡µé¢å¤„ç†æ•°é‡: {len(pages_text_list)} -> {max_pages}")
            pages_text_list = pages_text_list[:max_pages]

        # 2. å¹¶å‘å¤„ç†æ¯ä¸€é¡µï¼ˆVLM + åˆ†å—ï¼‰
        logger.info(f"ğŸ”„ å¼€å§‹æ··åˆå¤„ç† {len(pages_text_list)} ä¸ªé¡µé¢...")

        max_concurrent = self.config.get('max_concurrent_pages', 3)
        semaphore = asyncio.Semaphore(max_concurrent)

        tasks = []
        for page_idx, page_text in enumerate(pages_text_list, start=1):
            page_num = page_idx

            # è·å–è¯¥é¡µçš„å›¾ç‰‡
            page_images = [
                img for img in all_images
                if img.get('page_number') == page_num
            ]

            task = self._process_single_page_hybrid(
                pdf_path=pdf_path,
                page_number=page_num,
                page_text=page_text,
                page_images=page_images,
                user_id=user_id,
                pdf_name=pdf_name,
                metadata=metadata,
                semaphore=semaphore
            )
            tasks.append(task)

        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # æ”¶é›†æ‰€æœ‰ chunks (æ¯é¡µå¯èƒ½äº§ç”Ÿå¤šä¸ª chunks)
        for idx, r in enumerate(results):
            if isinstance(r, Exception):
                logger.error(f"é¡µé¢ {idx+1} å¤„ç†å¤±è´¥: {r}")
            elif r and isinstance(r, list):
                # r æ˜¯è¯¥é¡µçš„ chunk åˆ—è¡¨
                all_chunk_records.extend(r)

        logger.info(f"âœ… æ··åˆå¤„ç†å®Œæˆ: {len(all_chunk_records)} ä¸ª chunks from {len(pages_text_list)} pages")

        return all_chunk_records

    async def _process_single_page_hybrid(
        self,
        pdf_path: str,
        page_number: int,
        page_text: str,
        page_images: List[Dict[str, Any]],
        user_id: str,
        pdf_name: str,
        metadata: Dict[str, Any],
        semaphore: asyncio.Semaphore
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆå¤„ç†å•ä¸ªé¡µé¢ï¼šVLM åˆ†æ + æ–‡æœ¬ç»†ç²’åº¦åˆ†å—

        Returns:
            è¯¥é¡µçš„ chunk è®°å½•åˆ—è¡¨ (å¯èƒ½æœ‰å¤šä¸ª chunks)
        """
        async with semaphore:
            try:
                logger.info(f"ğŸ“„ å¤„ç†é¡µé¢ {page_number} (hybrid)...")

                # 1. VLM åˆ†ææ•´é¡µ (multimodal context)
                enable_vlm = self.config.get('enable_vlm_analysis', True)
                if enable_vlm:
                    page_summary, photo_descriptions = await self._analyze_page_with_vlm(
                        pdf_path, page_number, page_text, len(page_images)
                    )
                else:
                    page_summary = f"ç¬¬{page_number}é¡µ"
                    photo_descriptions = [f"å›¾ç‰‡{i+1}" for i in range(len(page_images))]

                # 2. ä¸Šä¼ é¡µé¢å›¾ç‰‡åˆ° MinIO
                enable_minio = self.config.get('enable_minio_upload', True)
                photo_urls = []
                if enable_minio:
                    for idx, img_data in enumerate(page_images):
                        try:
                            photo_url = await self._upload_image_to_minio(
                                img_data, user_id, pdf_name, page_number, idx
                            )
                            if photo_url:
                                photo_urls.append(photo_url)
                        except Exception as e:
                            logger.warning(f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥ (page={page_number}, img={idx}): {e}")
                            continue
                else:
                    photo_urls = [f"placeholder_url_{i}" for i in range(len(page_images))]

                # 3. ä½¿ç”¨ ChunkingService åˆ†å—æ–‡æœ¬
                if not page_text or not page_text.strip():
                    logger.warning(f"é¡µé¢ {page_number} æ— æ–‡æœ¬ï¼Œè·³è¿‡åˆ†å—")
                    return []

                # é…ç½®åˆ†å—ç­–ç•¥
                chunk_config = ChunkConfig(
                    strategy=ChunkingStrategy(self.chunking_strategy),
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap,
                    min_chunk_size=100,
                    max_chunk_size=self.chunk_size * 2
                )

                # åˆ†å—
                chunks = await self.chunking_service.get_chunker(chunk_config).chunk(
                    text=page_text,
                    metadata={'page_number': page_number}
                )

                logger.info(f"ğŸ“ é¡µé¢ {page_number} åˆ†æˆ {len(chunks)} ä¸ª chunks")

                # 4. ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ embedding å’Œè®°å½•
                chunk_records = []
                for chunk_idx, chunk in enumerate(chunks):
                    # åˆå¹¶æ–‡æœ¬ï¼špage_summary + chunk_text + photo_descriptions
                    combined_parts = []

                    # æ·»åŠ é¡µé¢æ‘˜è¦ (for context)
                    if page_summary and chunk_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ª chunk åŠ æ‘˜è¦
                        combined_parts.append(f"ã€é¡µé¢æ¦‚è¦ã€‘{page_summary}")

                    # æ·»åŠ  chunk æ–‡æœ¬
                    combined_parts.append(chunk.text)

                    # æ·»åŠ å›¾ç‰‡æè¿° (for context)
                    if photo_descriptions and chunk_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ª chunk åŠ å›¾ç‰‡
                        combined_parts.append("\nã€é¡µé¢å›¾ç‰‡ã€‘")
                        for idx, desc in enumerate(photo_descriptions, 1):
                            combined_parts.append(f"å›¾ç‰‡{idx}: {desc}")

                    combined_text = "\n".join(combined_parts)

                    # ç”Ÿæˆ embedding
                    embedding = await self.embedding_integration.embed_text(combined_text)

                    # åˆ›å»º chunk è®°å½•
                    chunk_record = {
                        'page_number': page_number,
                        'chunk_index': chunk_idx,
                        'text': combined_text,
                        'embedding': embedding,
                        'photo_urls': photo_urls,  # è¯¥é¡µçš„æ‰€æœ‰å›¾ç‰‡
                        'metadata': {
                            'pdf_name': pdf_name,
                            'page_number': page_number,
                            'chunk_index': chunk_idx,
                            'total_page_chunks': len(chunks),
                            'page_summary': page_summary,
                            'num_photos': len(photo_urls),
                            'photo_descriptions': photo_descriptions,
                            'chunking_strategy': self.chunking_strategy,
                            'content_type': 'text_chunk',
                            'chunk_id': chunk.chunk_id,
                            **chunk.metadata,
                            **metadata
                        }
                    }

                    chunk_records.append(chunk_record)

                logger.info(f"âœ… é¡µé¢ {page_number} å¤„ç†å®Œæˆ: {len(chunk_records)} chunks, {len(photo_urls)} å¼ å›¾ç‰‡")
                return chunk_records

            except Exception as e:
                logger.error(f"âŒ é¡µé¢ {page_number} æ··åˆå¤„ç†å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return []

    async def _process_single_page_multimodal(
        self,
        pdf_path: str,
        page_number: int,
        page_text: str,
        page_images: List[Dict[str, Any]],
        user_id: str,
        pdf_name: str,
        metadata: Dict[str, Any],
        semaphore: asyncio.Semaphore
    ) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ªé¡µé¢ï¼ˆå¤šæ¨¡æ€ï¼‰
        
        Returns:
            é¡µé¢è®°å½•: {
                'page_number': int,
                'text': str,  # VLM summary + page text + photo descriptions
                'embedding': List[float],
                'photo_urls': List[str],  # MinIO URLs
                'metadata': {...}
            }
        """
        async with semaphore:
            try:
                logger.info(f"ğŸ“„ å¤„ç†é¡µé¢ {page_number}...")
                
                # 1. VLM åˆ†ææ•´é¡µï¼ˆå¯é…ç½®æ˜¯å¦å¯ç”¨ï¼‰
                enable_vlm = self.config.get('enable_vlm_analysis', True)
                if enable_vlm:
                    page_summary, photo_descriptions = await self._analyze_page_with_vlm(
                        pdf_path, page_number, page_text, len(page_images)
                    )
                else:
                    # ç®€åŒ–æ¨¡å¼ï¼šä¸ä½¿ç”¨ VLM
                    page_summary = f"ç¬¬{page_number}é¡µ"
                    photo_descriptions = [f"å›¾ç‰‡{i+1}" for i in range(len(page_images))]
                    logger.info(f"âš ï¸ VLM åˆ†æå·²ç¦ç”¨ï¼ˆç®€åŒ–æ¨¡å¼ï¼‰")
                
                # 2. ä¸Šä¼ é¡µé¢å›¾ç‰‡åˆ° MinIOï¼ˆå¯é…ç½®æ˜¯å¦å¯ç”¨ï¼‰
                enable_minio = self.config.get('enable_minio_upload', True)
                photo_urls = []
                if enable_minio:
                    for idx, img_data in enumerate(page_images):
                        try:
                            photo_url = await self._upload_image_to_minio(
                                img_data, user_id, pdf_name, page_number, idx
                            )
                            if photo_url:
                                photo_urls.append(photo_url)
                        except Exception as e:
                            logger.warning(f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥ (page={page_number}, img={idx}): {e}")
                            continue
                else:
                    # ç®€åŒ–æ¨¡å¼ï¼šä¸ä¸Šä¼  MinIOï¼Œåªè®°å½•æ•°é‡
                    photo_urls = [f"placeholder_url_{i}" for i in range(len(page_images))]
                    logger.info(f"âš ï¸ MinIO ä¸Šä¼ å·²ç¦ç”¨ï¼ˆç®€åŒ–æ¨¡å¼ï¼‰")
                
                # 3. åˆå¹¶æ–‡æœ¬ï¼šVLM summary + page text + photo descriptions
                combined_text = self._combine_page_content(
                    page_summary, page_text, photo_descriptions, photo_urls
                )
                
                # 4. ç”Ÿæˆ embedding
                embedding = await self.embedding_integration.embed_text(combined_text)
                
                # 5. åˆ›å»ºé¡µé¢è®°å½•
                page_record = {
                    'page_number': page_number,
                    'text': combined_text,
                    'embedding': embedding,
                    'photo_urls': photo_urls,
                    'metadata': {
                        'pdf_name': pdf_name,
                        'page_number': page_number,
                        'page_summary': page_summary,
                        'num_photos': len(photo_urls),
                        'photo_descriptions': photo_descriptions,
                        'content_type': 'page',
                        **metadata
                    }
                }
                
                logger.info(f"âœ… é¡µé¢ {page_number} å¤„ç†å®Œæˆ ({len(photo_urls)} å¼ å›¾ç‰‡)")
                return page_record
                
            except Exception as e:
                logger.error(f"âŒ é¡µé¢ {page_number} å¤„ç†å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return None
    
    async def _analyze_page_with_vlm(
        self,
        pdf_path: str,
        page_number: int,
        page_text: str,
        num_photos: int
    ) -> Tuple[str, List[str]]:
        """
        VLM åˆ†ææ•´ä¸ªé¡µé¢
        
        Returns:
            (page_summary, [photo_1_description, photo_2_description, ...])
        """
        try:
            # æ¸²æŸ“é¡µé¢ä¸ºå›¾ç‰‡
            page_image_bytes = await self._render_pdf_page_as_image(pdf_path, page_number)
            
            if not page_image_bytes:
                # å¦‚æœæ¸²æŸ“å¤±è´¥ï¼Œä½¿ç”¨æ–‡å­—ç”Ÿæˆç®€å• summary
                return f"é¡µé¢ {page_number}", []
            
            # VLM åˆ†æ
            prompt = f"""
åˆ†æè¿™ä¸ªPDFé¡µé¢ï¼ˆç¬¬{page_number}é¡µï¼‰ã€‚é¡µé¢åŒ…å« {num_photos} å¼ å›¾ç‰‡ã€‚

é¡µé¢æ–‡å­—å†…å®¹ï¼š
{page_text[:800] if page_text else '(æ— æ–‡å­—)'}...

è¯·æä¾›ï¼š
1. page_summary: ç”¨1-2å¥è¯æ¦‚æ‹¬é¡µé¢ä¸»é¢˜
2. photo_1: ç¬¬1å¼ å›¾ç‰‡çš„å†…å®¹æè¿°ï¼ˆå¦‚æœæœ‰ï¼‰
3. photo_2: ç¬¬2å¼ å›¾ç‰‡çš„å†…å®¹æè¿°ï¼ˆå¦‚æœæœ‰ï¼‰
...

æ ¼å¼ï¼š
page_summary: xxx
photo_1: xxx  
photo_2: xxx
"""
            
            result = await vlm_analyze(
                image=page_image_bytes,
                prompt=prompt,
                model="gpt-4o-mini",
                provider="yyds"
            )
            
            # è§£æ VLM è¾“å‡º
            if result.success:
                response_text = result.response.strip()
                page_summary, photo_descriptions = self._parse_vlm_response(response_text, num_photos)
                return page_summary, photo_descriptions
            else:
                logger.warning(f"VLM åˆ†æå¤±è´¥: {result.error}")
                return f"é¡µé¢ {page_number}", []
                
        except Exception as e:
            logger.error(f"VLM é¡µé¢åˆ†æå¤±è´¥: {e}")
            return f"é¡µé¢ {page_number}", []
    
    async def _render_pdf_page_as_image(
        self,
        pdf_path: str,
        page_number: int
    ) -> Optional[bytes]:
        """å°† PDF é¡µé¢æ¸²æŸ“ä¸ºå›¾ç‰‡ï¼ˆç”¨äº VLM åˆ†æï¼‰"""
        try:
            import fitz  # PyMuPDF
            
            doc = fitz.open(pdf_path)
            page = doc[page_number - 1]  # PyMuPDF uses 0-based indexing
            
            # æ¸²æŸ“ä¸ºå›¾ç‰‡ï¼ˆé«˜è´¨é‡ï¼‰
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom for quality
            img_bytes = pix.tobytes("png")
            
            doc.close()
            return img_bytes
            
        except Exception as e:
            logger.error(f"é¡µé¢æ¸²æŸ“å¤±è´¥ (page={page_number}): {e}")
            return None
    
    def _parse_vlm_response(self, response: str, num_photos: int) -> Tuple[str, List[str]]:
        """è§£æ VLM è¾“å‡º"""
        page_summary = ""
        photo_descriptions = []
        
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('page_summary:'):
                page_summary = line.split(':', 1)[1].strip()
            elif line.startswith('photo_'):
                desc = line.split(':', 1)[1].strip() if ':' in line else ""
                photo_descriptions.append(desc)
        
        # å¦‚æœæ²¡æœ‰è§£æåˆ° summaryï¼Œä½¿ç”¨ç¬¬ä¸€è¡Œ
        if not page_summary and lines:
            page_summary = lines[0][:200]
        
        return page_summary, photo_descriptions
    
    def _combine_page_content(
        self,
        page_summary: str,
        page_text: str,
        photo_descriptions: List[str],
        photo_urls: List[str]
    ) -> str:
        """åˆå¹¶é¡µé¢å†…å®¹ä¸ºæœ€ç»ˆçš„æ–‡æœ¬ï¼ˆç”¨äº embeddingï¼‰"""
        parts = []
        
        # 1. é¡µé¢æ‘˜è¦
        if page_summary:
            parts.append(f"ã€é¡µé¢æ¦‚è¦ã€‘{page_summary}")
        
        # 2. é¡µé¢æ–‡å­—
        if page_text.strip():
            parts.append(f"\nã€é¡µé¢æ–‡å­—ã€‘\n{page_text}")
        
        # 3. å›¾ç‰‡æè¿° + URL
        if photo_descriptions:
            parts.append("\nã€é¡µé¢å›¾ç‰‡ã€‘")
            for idx, (desc, url) in enumerate(zip(photo_descriptions, photo_urls), 1):
                parts.append(f"å›¾ç‰‡{idx}: {desc}")
                if url:
                    parts.append(f"  é“¾æ¥: {url}")
        
        return "\n".join(parts)
    
    async def _upload_image_to_minio(
        self,
        img_data: Dict[str, Any],
        user_id: str,
        pdf_name: str,
        page_number: int,
        image_index: int
    ) -> Optional[str]:
        """ä¸Šä¼ å›¾ç‰‡åˆ° MinIO"""
        try:
            # è§£ç å›¾ç‰‡æ•°æ®
            image_data_str = img_data.get('image_data', '')
            if not image_data_str:
                return None
            
            # ç§»é™¤ Data URL å‰ç¼€
            if image_data_str.startswith('data:'):
                if ';base64,' in image_data_str:
                    image_data_base64 = image_data_str.split(';base64,', 1)[1]
                else:
                    return None
            else:
                image_data_base64 = image_data_str
            
            # è§£ç  base64
            image_bytes = base64.b64decode(image_data_base64)
            
            # ä¸Šä¼ åˆ° MinIO
            image_format = img_data.get('format', 'png')
            image_name = f"{Path(pdf_name).stem}_page{page_number}_img{image_index}.{image_format}"
            
            minio_result = await self.minio_integration.upload_image(
                image_data=image_bytes,
                user_id=user_id,
                image_name=image_name
            )
            
            if minio_result.get('success'):
                return minio_result.get('presigned_url')
            else:
                return None
                
        except Exception as e:
            logger.warning(f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {e}")
            return None
    
    async def _extract_pdf_images(self, pdf_path: str) -> Dict[str, Any]:
        """æå– PDF ä¸­çš„æ‰€æœ‰å›¾ç‰‡ï¼ˆä½¿ç”¨ PDFProcessorï¼‰"""
        try:
            from tools.services.data_analytics_service.processors.file_processors.pdf_processor import PDFProcessor
            
            pdf_processor = PDFProcessor(self.config)
            result = await pdf_processor.process_pdf_unified(pdf_path, {
                'extract_text': False,
                'extract_images': True,
                'extract_tables': False
            })
            
            if not result.get('success'):
                return {'success': False, 'images': []}
            
            # æå–æ‰€æœ‰å›¾ç‰‡æ•°æ®
            image_analysis = result.get('image_analysis', {})
            extracted_images = image_analysis.get('extracted_images', [])
            
            images_data = []
            for img in extracted_images:
                image_data_str = img.get('image_data', '')
                if image_data_str:
                    # è§£ç  base64 ä¸ºå­—èŠ‚
                    try:
                        # ç§»é™¤ Data URL å‰ç¼€ (data:image/xxx;base64,)
                        if image_data_str.startswith('data:'):
                            # æ‰¾åˆ° base64 æ•°æ®éƒ¨åˆ†
                            if ';base64,' in image_data_str:
                                image_data_base64 = image_data_str.split(';base64,', 1)[1]
                            else:
                                logger.warning(f"å›¾ç‰‡æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {image_data_str[:50]}")
                                continue
                        else:
                            image_data_base64 = image_data_str
                        
                        # è§£ç  base64
                        image_bytes = base64.b64decode(image_data_base64)
                        images_data.append({
                            'page_number': img.get('page_number', 0),
                            'image_index': img.get('image_index', 0),
                            'image_bytes': image_bytes,
                            'format': img.get('format', 'png'),
                            'size': f"{img.get('width', 0)}x{img.get('height', 0)}"
                        })
                    except Exception as e:
                        logger.warning(f"å›¾ç‰‡è§£ç å¤±è´¥ (page={img.get('page_number')}, idx={img.get('image_index')}): {e}")
                        continue
            
            return {
                'success': True,
                'images': images_data
            }
            
        except Exception as e:
            logger.error(f"PDF å›¾ç‰‡æå–å¤±è´¥: {e}")
            return {'success': False, 'images': []}
    
    async def _process_single_image(
        self,
        img_data: Dict[str, Any],
        idx: int,
        total: int,
        user_id: str,
        pdf_name: str,
        semaphore: asyncio.Semaphore
    ) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•å¼ å›¾ç‰‡ï¼ˆå¹¶å‘å®‰å…¨ï¼‰"""
        async with semaphore:
            try:
                page_num = img_data.get('page_number', 0)
                image_bytes = img_data.get('image_bytes')
                image_format = img_data.get('format', 'png')
                
                logger.info(f"å¤„ç†å›¾ç‰‡ {idx+1}/{total}: é¡µ {page_num}")
                
                # 1. ä½¿ç”¨ VLM ç”Ÿæˆå›¾ç‰‡æè¿°
                description = await self._generate_image_description(image_bytes)
                
                # 2. ä¸Šä¼ åˆ° MinIO
                image_name = f"{Path(pdf_name).stem}_page{page_num}_img{idx}.{image_format}"
                minio_result = await self.minio_integration.upload_image(
                    image_data=image_bytes,
                    user_id=user_id,
                    image_name=image_name
                )
                
                if not minio_result.get('success'):
                    logger.warning(f"å›¾ç‰‡ä¸Šä¼  MinIO å¤±è´¥: {image_name}")
                    return None
                
                # 3. ç”Ÿæˆæè¿°çš„ embedding
                description_embedding = await self.embedding_integration.embed_text(description)
                
                # 4. åˆ›å»ºå›¾ç‰‡è®°å½•
                image_record = {
                    'type': 'image',
                    'page_number': page_num,
                    'image_index': idx,
                    'description': description,
                    'embedding': description_embedding,
                    'minio_url': minio_result.get('presigned_url'),
                    'minio_path': minio_result.get('object_path'),
                    'metadata': {
                        'pdf_name': pdf_name,
                        'image_name': image_name,
                        'size': img_data.get('size', 'unknown'),
                        'format': image_format,
                        'content_type': 'image'
                    }
                }
                
                logger.info(f"âœ… å›¾ç‰‡ {idx+1} å¤„ç†å®Œæˆ")
                return image_record
                
            except Exception as e:
                logger.error(f"å›¾ç‰‡ {idx+1} å¤„ç†å¤±è´¥: {e}")
                return None
    
    async def _process_images(
        self,
        images_data: List[Dict[str, Any]],
        user_id: str,
        pdf_name: str
    ) -> List[Dict[str, Any]]:
        """
        å¹¶å‘å¤„ç†å›¾ç‰‡ï¼šVLM æè¿°ç”Ÿæˆ + MinIO å­˜å‚¨
        
        ä½¿ç”¨ asyncio.gather + semaphore å®ç°å¹¶å‘æ§åˆ¶
        
        Returns:
            å›¾ç‰‡è®°å½•åˆ—è¡¨ï¼ŒåŒ…å«æè¿°ã€URLã€embedding
        """
        # åˆ›å»º semaphore é™åˆ¶å¹¶å‘æ•°ï¼ˆé¿å… API é™æµï¼‰
        max_concurrent = self.config.get('max_concurrent_images', 10)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(images_data)} å¼ å›¾ç‰‡ï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_concurrent}ï¼‰")
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [
            self._process_single_image(
                img_data=img_data,
                idx=idx,
                total=len(images_data),
                user_id=user_id,
                pdf_name=pdf_name,
                semaphore=semaphore
            )
            for idx, img_data in enumerate(images_data)
        ]
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ
        image_records = [
            result for result in results 
            if result is not None and not isinstance(result, Exception)
        ]
        
        logger.info(f"âœ… å›¾ç‰‡å¤„ç†å®Œæˆ: {len(image_records)}/{len(images_data)} æˆåŠŸ")
        return image_records
    
    async def _generate_image_description(self, image_bytes: bytes) -> str:
        """ä½¿ç”¨ VLM ç”Ÿæˆå›¾ç‰‡æè¿°"""
        try:
            # æ£€æŸ¥å›¾ç‰‡å¤§å°ï¼ˆOpenAI é™åˆ¶ 20MBï¼Œæˆ‘ä»¬è®¾ç½®ä¸º 5MB å®‰å…¨å€¼ï¼‰
            max_size_mb = 5
            size_mb = len(image_bytes) / (1024 * 1024)
            
            if size_mb > max_size_mb:
                logger.warning(f"å›¾ç‰‡å¤ªå¤§ ({size_mb:.2f}MB > {max_size_mb}MB)ï¼Œè·³è¿‡åˆ†æ")
                return f"å›¾ç‰‡å†…å®¹ï¼ˆæ–‡ä»¶è¿‡å¤§: {size_mb:.2f}MBï¼‰"
            
            # ä½¿ç”¨ç°æœ‰çš„ ImageAnalyzer
            prompt = """è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
1. ä¸»è¦å†…å®¹å’Œå…ƒç´ 
2. å›¾è¡¨ç±»å‹ï¼ˆå¦‚æœæ˜¯å›¾è¡¨ï¼‰
3. å…³é”®ä¿¡æ¯å’Œæ•°æ®
4. æ“ä½œè¯´æ˜ï¼ˆå¦‚æœæ˜¯ç•Œé¢æˆªå›¾ï¼‰
5. ä»»ä½•å¯è§çš„æ–‡å­—å†…å®¹

è¯·ç”¨ä¸­æ–‡æè¿°ï¼Œä¿æŒç®€æ´æ¸…æ™°ã€‚"""
            
            # å°†å­—èŠ‚æ•°æ®ç¼–ç ä¸º base64ï¼ˆImageAnalyzer æ”¯æŒï¼‰
            import base64
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            
            result = await vlm_analyze(
                image=image_base64,
                prompt=prompt,
                model="gpt-4o-mini",  # ä½¿ç”¨æ€§ä»·æ¯”é«˜çš„æ¨¡å‹
                provider="yyds"  # ä½¿ç”¨ yyds provider é¿å… OpenAI Rate Limit
            )
            
            if result.success:
                return result.response.strip()
            else:
                error_msg = str(result.error) if hasattr(result, 'error') else "æœªçŸ¥é”™è¯¯"
                logger.warning(f"VLM åˆ†æå¤±è´¥: {error_msg}")
                return f"å›¾ç‰‡å†…å®¹ï¼ˆæè¿°ç”Ÿæˆå¤±è´¥: {error_msg[:50]}ï¼‰"
                
        except Exception as e:
            logger.error(f"å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return f"å›¾ç‰‡å†…å®¹ï¼ˆå¤„ç†å¼‚å¸¸: {str(e)[:50]}ï¼‰"
    
    async def _process_text_chunks(
        self,
        text_chunks: List[Dict[str, Any]],
        user_id: str,
        pdf_name: str,
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        å¤„ç†æ–‡æœ¬å—ï¼šç”Ÿæˆ embedding
        
        Args:
            text_chunks: æ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'text': str, 'page_number': int}
            
        Returns:
            æ–‡æœ¬è®°å½•åˆ—è¡¨ï¼ŒåŒ…å«æ–‡æœ¬ã€embeddingã€metadata
        """
        text_records = []
        
        for idx, chunk_data in enumerate(text_chunks):
            try:
                # æå–æ–‡æœ¬å’Œé¡µç 
                chunk_text = chunk_data.get('text', '')
                page_number = chunk_data.get('page_number', 0)
                
                if not chunk_text.strip():
                    continue
                
                # ç”Ÿæˆ embedding
                embedding = await self.embedding_integration.embed_text(chunk_text)
                
                # åˆ›å»ºæ–‡æœ¬è®°å½•
                text_record = {
                    'type': 'text',
                    'chunk_index': idx,
                    'page_number': page_number,  # âœ… æ·»åŠ é¡µç 
                    'text': chunk_text,
                    'embedding': embedding,
                    'metadata': {
                        'pdf_name': pdf_name,
                        'chunk_index': idx,
                        'page_number': page_number,  # âœ… metadata ä¸­ä¹Ÿä¿å­˜
                        'total_chunks': len(text_chunks),
                        'content_type': 'text',
                        **metadata
                    }
                }
                
                text_records.append(text_record)
                
            except Exception as e:
                logger.error(f"æ–‡æœ¬å— {idx} å¤„ç†å¤±è´¥: {e}")
                continue
        
        return text_records
    
    async def _store_pages_to_vector_db(
        self,
        user_id: str,
        page_records: List[Dict[str, Any]],
        source_document: str
    ) -> Dict[str, Any]:
        """
        å­˜å‚¨é¡µé¢è®°å½•åˆ° Supabase pgvector
        
        æ¯ä¸ªé¡µé¢ä½œä¸ºä¸€æ¡è®°å½•ï¼ŒåŒ…å«ï¼š
        - text: VLM summary + page text + photo descriptions
        - embedding: æ–‡æœ¬çš„ embedding
        - metadata: page_number, photo_urls, etc.
        """
        try:
            vector_db = self.vector_db_integration.vector_db
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
            insert_data = []
            for page_record in page_records:
                insert_data.append({
                    'user_id': user_id,
                    'text': page_record.get('text'),
                    'embedding_vector': page_record.get('embedding'),
                    'metadata': {
                        **page_record.get('metadata', {}),
                        'source_document': source_document,
                        'record_type': 'page',  # é¡µé¢çº§è®°å½•
                        'photo_urls': page_record.get('photo_urls', []),
                        'page_number': page_record.get('page_number'),
                    },
                    'source_document': source_document,
                    'created_at': datetime.now().isoformat()
                })
            
            # æ‰¹é‡æ’å…¥
            logger.info(f"æ‰¹é‡æ’å…¥ {len(insert_data)} ä¸ªé¡µé¢åˆ°å‘é‡æ•°æ®åº“...")
            
            # ä½¿ç”¨ vector_db çš„æ’å…¥æ–¹æ³•
            success_count = 0
            for idx, data in enumerate(insert_data):
                try:
                    # ç”Ÿæˆ UUID æ ¼å¼çš„å”¯ä¸€ ID
                    record_id = str(uuid.uuid4())
                    
                    await vector_db.store_vector(
                        id=record_id,
                        user_id=data['user_id'],
                        text=data['text'],
                        embedding=data['embedding_vector'],
                        metadata=data['metadata']
                    )
                    success_count += 1
                except Exception as e:
                    logger.warning(f"å•æ¡è®°å½•æ’å…¥å¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {success_count}/{len(insert_data)} æ¡è®°å½•")
            
            return {
                'success': True,
                'total_records': len(insert_data),
                'success_count': success_count,
                'failed_count': len(insert_data) - success_count
            }
            
        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“å­˜å‚¨å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _store_to_vector_db(
        self,
        user_id: str,
        text_records: List[Dict[str, Any]],
        image_records: List[Dict[str, Any]],
        source_document: str
    ) -> Dict[str, Any]:
        """
        å­˜å‚¨æ‰€æœ‰è®°å½•åˆ° Supabase pgvector
        
        æ–‡æœ¬å’Œå›¾ç‰‡æè¿°éƒ½å­˜å‚¨ä¸ºç‹¬ç«‹çš„ embeddingï¼Œmetadata ä¸­æ ‡è®°ç±»å‹
        """
        try:
            vector_db = self.vector_db_integration.vector_db
            
            # åˆå¹¶æ‰€æœ‰è®°å½•
            all_records = text_records + image_records
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
            insert_data = []
            for record in all_records:
                insert_data.append({
                    'user_id': user_id,
                    'text': record.get('text') or record.get('description'),
                    'embedding_vector': record.get('embedding'),
                    'metadata': {
                        **record.get('metadata', {}),
                        'source_document': source_document,
                        'record_type': record.get('type'),  # 'text' or 'image'
                        'minio_url': record.get('minio_url'),  # ä»…å›¾ç‰‡æœ‰
                        'minio_path': record.get('minio_path'),  # ä»…å›¾ç‰‡æœ‰
                        'page_number': record.get('page_number'),
                        'chunk_index': record.get('chunk_index'),
                        'image_index': record.get('image_index')
                    },
                    'source_document': source_document,
                    'created_at': datetime.now().isoformat()
                })
            
            # æ‰¹é‡æ’å…¥
            logger.info(f"æ‰¹é‡æ’å…¥ {len(insert_data)} æ¡è®°å½•åˆ°å‘é‡æ•°æ®åº“...")
            
            # ä½¿ç”¨ vector_db çš„æ’å…¥æ–¹æ³•
            success_count = 0
            for idx, data in enumerate(insert_data):
                try:
                    # ç”Ÿæˆ UUID æ ¼å¼çš„å”¯ä¸€ ID
                    record_id = str(uuid.uuid4())
                    
                    await vector_db.store_vector(
                        id=record_id,
                        user_id=data['user_id'],
                        text=data['text'],
                        embedding=data['embedding_vector'],
                        metadata=data['metadata']
                    )
                    success_count += 1
                except Exception as e:
                    logger.warning(f"å•æ¡è®°å½•æ’å…¥å¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {success_count}/{len(insert_data)} æ¡è®°å½•")
            
            return {
                'success': True,
                'total_records': len(insert_data),
                'success_count': success_count,
                'failed_count': len(insert_data) - success_count
            }
            
        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“å­˜å‚¨å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def retrieve(
        self,
        user_id: str,
        query: str,
        top_k: Optional[int] = None,
        filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æ£€ç´¢ç›¸å…³å†…å®¹ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
        
        Args:
            user_id: ç”¨æˆ· ID
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            filters: é¢å¤–è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚ source_documentï¼‰
            
        Returns:
            æ£€ç´¢ç»“æœï¼ŒåŒ…å«æ–‡æœ¬å—å’Œç›¸å…³å›¾ç‰‡
        """
        try:
            top_k = top_k or self.top_k
            
            logger.info(f"ğŸ” æ£€ç´¢æŸ¥è¯¢: {query} (user: {user_id}, top_k: {top_k})")
            
            # 1. ç”ŸæˆæŸ¥è¯¢ embedding
            query_embedding = await self.embedding_integration.embed_text(query)
            
            # 2. ä»å‘é‡æ•°æ®åº“æ£€ç´¢
            from tools.services.intelligence_service.vector_db.base_vector_db import VectorSearchConfig
            
            vector_db = self.vector_db_integration.vector_db
            search_config = VectorSearchConfig(
                top_k=top_k * 2,  # å¤šæ£€ç´¢ä¸€äº›ï¼Œåç»­è¿‡æ»¤
                filter_metadata=filters
            )
            search_results = await vector_db.search_vectors(
                query_embedding=query_embedding,
                user_id=user_id,
                config=search_config
            )
            
            # 3. å¤„ç†æ£€ç´¢ç»“æœï¼ˆé¡µé¢çº§ï¼‰
            page_results = []
            
            for result in search_results:
                # SearchResult æ˜¯å¯¹è±¡ï¼Œä½¿ç”¨å±æ€§è®¿é—®
                metadata = result.metadata or {}
                record_type = metadata.get('record_type', 'page')
                
                if record_type == 'page':
                    # é¡µé¢çº§è®°å½•
                    page_results.append({
                        'text': result.text,
                        'page_number': metadata.get('page_number'),
                        'page_summary': metadata.get('page_summary'),
                        'similarity_score': result.score,
                        'photo_urls': metadata.get('photo_urls', []),
                        'num_photos': metadata.get('num_photos', 0),
                        'metadata': metadata
                    })
                else:
                    # å…¼å®¹æ—§æ ¼å¼ï¼ˆtext/imageï¼‰
                    page_results.append({
                        'text': result.text,
                        'page_number': metadata.get('page_number'),
                        'similarity_score': result.score,
                        'photo_urls': [metadata.get('minio_url')] if metadata.get('minio_url') else [],
                        'metadata': metadata
                    })
            
            # 4. é™åˆ¶è¿”å›æ•°é‡
            page_results = page_results[:top_k]
            
            # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
            total_photos = sum(len(p.get('photo_urls', [])) for p in page_results)
            
            logger.info(f"âœ… æ£€ç´¢å®Œæˆ: {len(page_results)} ä¸ªé¡µé¢, {total_photos} å¼ å›¾ç‰‡")
            
            return {
                'success': True,
                'query': query,
                'page_results': page_results,  # é¡µé¢çº§ç»“æœ
                'total_pages': len(page_results),
                'total_photos': total_photos,
                # å…¼å®¹æ—§æ¥å£
                'text_results': page_results,
                'image_results': []
            }
            
        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'page_results': [],
                'text_results': [],
                'image_results': []
            }
    
    async def generate(
        self,
        user_id: str,
        query: str,
        retrieval_result: Dict[str, Any],
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆç­”æ¡ˆï¼ˆåŸºäºæ£€ç´¢ç»“æœï¼‰
        
        Args:
            user_id: ç”¨æˆ· ID
            query: æŸ¥è¯¢é—®é¢˜
            retrieval_result: æ£€ç´¢ç»“æœï¼ˆæ¥è‡ª retrieve()ï¼‰
            generation_config: ç”Ÿæˆé…ç½®ï¼ˆæ¨¡å‹ã€æ¸©åº¦ç­‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆï¼ŒåŒ…å«å›¾ç‰‡å¼•ç”¨
        """
        try:
            generation_config = generation_config or {}
            
            logger.info(f"ğŸ¤– ç”Ÿæˆç­”æ¡ˆ: {query}")
            
            # 1. æå–æ£€ç´¢ç»“æœï¼ˆé¡µé¢çº§ï¼‰
            page_results = retrieval_result.get('page_results', [])
            
            # å…¼å®¹æ—§æ ¼å¼
            if not page_results:
                page_results = retrieval_result.get('text_results', [])
            
            # 2. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆé¡µé¢çº§ï¼ŒåŒ…å«å›¾ç‰‡ï¼‰
            context_parts = []
            
            context_parts.append("## ç›¸å…³é¡µé¢å†…å®¹ï¼š\n")
            for idx, result in enumerate(page_results, 1):
                page_num = result.get('page_number', 'æœªçŸ¥')
                page_summary = result.get('page_summary', '')
                text = result.get('text', '')
                photo_urls = result.get('photo_urls', [])
                
                context_parts.append(f"\n[é¡µé¢ {idx}] (é¡µç : {page_num})")
                
                # é¡µé¢æ‘˜è¦
                if page_summary:
                    context_parts.append(f"æ‘˜è¦: {page_summary}")
                
                # é¡µé¢å†…å®¹ï¼ˆæˆªå–éƒ¨åˆ†ï¼‰
                if text:
                    content_preview = text[:500] if len(text) > 500 else text
                    context_parts.append(f"\nå†…å®¹:\n{content_preview}...")
                
                # é¡µé¢å›¾ç‰‡
                if photo_urls:
                    context_parts.append(f"\nåŒ…å« {len(photo_urls)} å¼ å›¾ç‰‡:")
                    for photo_idx, url in enumerate(photo_urls, 1):
                        context_parts.append(f"  å›¾ç‰‡{photo_idx}: {url}")
            
            context = "\n".join(context_parts)
            
            # 3. æ„å»º prompt
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ CRM ç³»ç»ŸåŠ©æ‰‹ã€‚åŸºäºæä¾›çš„PDFé¡µé¢å†…å®¹ï¼ˆåŒ…å«æ–‡å­—å’Œå›¾ç‰‡ï¼‰ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ³¨æ„ï¼š
1. ä¼˜å…ˆä½¿ç”¨æ–‡æ¡£ä¸­çš„å‡†ç¡®ä¿¡æ¯
2. æ¯ä¸ªé¡µé¢å¯èƒ½åŒ…å«å¤šå¼ å›¾ç‰‡ï¼Œæ³¨æ„å¼•ç”¨å›¾ç‰‡URL
3. æä¾›æ¸…æ™°çš„æ“ä½œæ­¥éª¤å’Œè¯´æ˜
4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯šå®è¯´æ˜
5. åœ¨ç­”æ¡ˆä¸­æä¾›å›¾ç‰‡é“¾æ¥ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥çœ‹"""
            
            user_prompt = f"""åŸºäºä»¥ä¸‹å†…å®¹ï¼Œå›ç­”é—®é¢˜ï¼š

{context}

**ç”¨æˆ·é—®é¢˜**: {query}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ç­”æ¡ˆã€‚å¦‚æœæœ‰ç›¸å…³å›¾ç‰‡ï¼Œè¯·åœ¨ç­”æ¡ˆä¸­å¼•ç”¨å¹¶è¯´æ˜å¦‚ä½•æŸ¥çœ‹ã€‚"""
            
            # 4. è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
            from core.isa_client_factory import get_isa_client
            
            isa_client = get_isa_client()
            llm_result = await isa_client.invoke(
                input_data=user_prompt,
                task="chat",
                service_type="text",
                model=generation_config.get('model', 'gpt-4o-mini'),
                system_prompt=system_prompt,
                temperature=generation_config.get('temperature', 0.3),
                provider=generation_config.get('provider', 'yyds')  # ä½¿ç”¨ yyds provider
            )
            
            if not llm_result.get('success'):
                return {
                    'success': False,
                    'error': f"LLM ç”Ÿæˆå¤±è´¥: {llm_result.get('error')}"
                }
            
            # æå–ç­”æ¡ˆæ–‡æœ¬ï¼ˆå…¼å®¹ä¸åŒçš„è¿”å›æ ¼å¼ï¼‰
            answer = ""
            if 'result' in llm_result:
                result = llm_result['result']
                if isinstance(result, dict):
                    answer = result.get('text', '') or result.get('response', '')
                elif isinstance(result, str):
                    answer = result
            
            if not answer:
                answer = llm_result.get('response', '')
            
            logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            
            # ç»Ÿè®¡æ¥æº
            total_photos = sum(len(p.get('photo_urls', [])) for p in page_results)
            
            return {
                'success': True,
                'query': query,
                'answer': answer,
                'sources': {
                    'page_count': len(page_results),
                    'photo_count': total_photos,
                    'page_sources': page_results,
                    # å…¼å®¹æ—§æ ¼å¼
                    'text_count': len(page_results),
                    'image_count': 0,
                    'text_sources': page_results,
                    'image_sources': []
                },
                'context_used': context
            }
            
        except Exception as e:
            logger.error(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def query_with_generation(
        self,
        user_id: str,
        query: str,
        filters: Optional[Dict[str, Any]] = None,
        generation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        å®Œæ•´çš„ RAG æµç¨‹ï¼šæ£€ç´¢ + ç”Ÿæˆ
        
        è¿™æ˜¯ä¸€ä¸ªä¾¿æ·æ–¹æ³•ï¼Œç»„åˆäº† retrieve() å’Œ generate()
        """
        try:
            # 1. æ£€ç´¢
            retrieval_result = await self.retrieve(
                user_id=user_id,
                query=query,
                filters=filters
            )
            
            if not retrieval_result.get('success'):
                return {
                    'success': False,
                    'error': f"æ£€ç´¢å¤±è´¥: {retrieval_result.get('error')}"
                }
            
            # 2. ç”Ÿæˆ
            generation_result = await self.generate(
                user_id=user_id,
                query=query,
                retrieval_result=retrieval_result,
                generation_config=generation_config
            )
            
            return generation_result
            
        except Exception as e:
            logger.error(f"RAG æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }


# å…¨å±€å•ä¾‹
_custom_rag_service = None

def get_custom_rag_service(config: Optional[Dict[str, Any]] = None) -> CustomRAGService:
    """è·å– CustomRAGService å•ä¾‹"""
    global _custom_rag_service
    if _custom_rag_service is None:
        _custom_rag_service = CustomRAGService(config)
    return _custom_rag_service

