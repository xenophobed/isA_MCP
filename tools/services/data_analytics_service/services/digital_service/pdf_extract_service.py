#!/usr/bin/env python3
"""
Simple PDF Extract Service - Optimized with Dask

Simple, fast PDF extraction service that:
- Input: PDF file, user_id, optional metadata/options
- Options: "text" (text only) or "full" (text + image extraction)
- Output: chunks of text (nothing more)

No preprocessing, no graph analytics, no complex workflows.
Just PDF â†’ text chunks with Dask parallel processing.

Uses existing production services:
- PDFProcessor: For unified PDF processing (text + images)
- ImageAnalyzer: For parallel image text extraction with Dask
"""

import asyncio
import time
import logging
from typing import Dict, Any, List, Optional, Union
from pathlib import Path
from datetime import datetime
import dask
from dask.distributed import Client, as_completed
from dask import delayed

# Use existing production services
from ...processors.file_processors.pdf_processor import PDFProcessor
from ....intelligence_service.vision.image_analyzer import analyze

logger = logging.getLogger(__name__)

class PDFExtractService:
    """
    Simple PDF Extract Service - PDF to text chunks only
    
    Features:
    - Fast PDF text extraction
    - Optional image text extraction (with Dask parallelization)
    - Simple chunking (no intelligent preprocessing)
    - Dask parallel processing for image analysis
    - Clean, minimal interface
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.service_name = "simple_pdf_extract_service"
        self.version = "1.0.0"
        self.config = config or {}
        
        # Simple configuration
        self.chunk_size = self.config.get('chunk_size', 2000)
        self.chunk_overlap = self.config.get('chunk_overlap', 200)
        
        # Core processor
        self.pdf_processor = PDFProcessor(self.config)
        
        # Dask client for parallel processing
        self.dask_client = None
        
        logger.info("Simple PDF Extract Service initialized")
    
    async def initialize_dask(self, workers: int = 4, threads_per_worker: int = 2):
        """Initialize Dask client for parallel processing"""
        try:
            self.dask_client = Client(
                processes=True, 
                n_workers=workers,
                threads_per_worker=threads_per_worker,
                silence_logs=False
            )
            logger.info(f"Dask client initialized: {workers} workers, {threads_per_worker} threads each")
            return True
        except Exception as e:
            logger.warning(f"Dask initialization failed: {e}")
            return False
    
    def close_dask(self):
        """Close Dask client"""
        if self.dask_client:
            try:
                self.dask_client.close()
                self.dask_client = None
                logger.info("Dask client closed")
            except Exception as e:
                logger.warning(f"Dask close error: {e}")
    
    async def extract_pdf_to_chunks(
        self,
        pdf_path: str,
        user_id: int,
        options: Optional[Dict[str, Any]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Extract PDF to text chunks
        
        Args:
            pdf_path: Path to PDF file
            user_id: User ID
            options: {"mode": "text" or "full", "max_workers": int}
            metadata: Optional metadata
            
        Returns:
            {"success": bool, "chunks": List[str], "processing_time": float}
        """
        start_time = time.time()
        
        try:
            options = options or {}
            metadata = metadata or {}
            mode = options.get('mode', 'text')  # "text" or "full"
            
            # Validate input
            if not Path(pdf_path).exists():
                return {
                    'success': False,
                    'error': f'PDF file not found: {pdf_path}',
                    'processing_time': time.time() - start_time
                }
            
            logger.info(f"Extracting PDF: {pdf_path} (mode: {mode})")
            
            if mode == "text":
                # Step 1: Text-only mode (fast)
                text_result = await self._extract_text_only(pdf_path)
                
                if not text_result.get('success'):
                    return {
                        'success': False,
                        'error': f"Text extraction failed: {text_result.get('error')}",
                        'processing_time': time.time() - start_time
                    }
                
                full_text = text_result.get('text', '')
                pages_processed = text_result.get('pages', 0)
                images_processed = 0
                
            else:
                # Step 2: Full mode - integrate text and image content by page
                combined_result = await self._extract_text_and_images_integrated(pdf_path, options)
                
                if not combined_result.get('success'):
                    return {
                        'success': False,
                        'error': f"Full extraction failed: {combined_result.get('error')}",
                        'processing_time': time.time() - start_time
                    }
                
                full_text = combined_result.get('combined_text', '')
                pages_processed = combined_result.get('pages_processed', 0)
                images_processed = combined_result.get('images_processed', 0)
            
            # Step 3: Create simple chunks
            chunks = self._create_simple_chunks(full_text)
            
            processing_time = time.time() - start_time
            
            return {
                'success': True,
                'chunks': chunks,
                'chunk_count': len(chunks),
                'total_characters': len(full_text),
                'pages_processed': pages_processed,
                'images_processed': images_processed,
                'mode': mode,
                'user_id': user_id,
                'processing_time': processing_time,
                'metadata': {
                    'pdf_path': pdf_path,
                    'extracted_at': datetime.now().isoformat(),
                    **metadata
                }
            }
            
        except Exception as e:
            logger.error(f"PDF extraction failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    async def _extract_text_only(self, pdf_path: str) -> Dict[str, Any]:
        """Extract text only (fast) using existing PDFProcessor"""
        try:
            # Use PDFProcessor unified processing - text only mode
            result = await self.pdf_processor.process_pdf_unified(pdf_path, {
                'extract_text': True,
                'extract_images': False,
                'extract_tables': False
            })
            
            if result.get('success'):
                text_extraction = result.get('text_extraction', {})
                return {
                    'success': True,
                    'text': text_extraction.get('full_text', ''),
                    'pages': result.get('total_pages', 0),
                    'processing_time': text_extraction.get('processing_time', 0)
                }
            else:
                return {
                    'success': False,
                    'error': result.get('error', 'Text extraction failed')
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _extract_text_and_images_integrated(self, pdf_path: str, options: Dict[str, Any]) -> Dict[str, Any]:
        """Extract and integrate text and image content by page"""
        try:
            # Use PDFProcessor unified processing to get everything
            result = await self.pdf_processor.process_pdf_unified(pdf_path, {
                'extract_text': True,
                'extract_images': True,
                'extract_tables': False
            })
            
            if not result.get('success'):
                return {
                    'success': False,
                    'error': f"PDF processing failed: {result.get('error')}"
                }
            
            # Get text extraction results
            text_extraction = result.get('text_extraction', {})
            page_texts = text_extraction.get('pages', [])
            
            # Get image analysis results
            image_analysis = result.get('image_analysis', {})
            vision_analyses = image_analysis.get('vision_analyses', [])  # Full page images
            extracted_images = image_analysis.get('extracted_images', [])  # Embedded images
            
            # Organize images by page
            images_by_page = {}
            
            # Add full page images
            for vision_page in vision_analyses:
                page_num = vision_page.get('page_number', 0)
                if page_num not in images_by_page:
                    images_by_page[page_num] = []
                images_by_page[page_num].append({
                    'type': 'page',
                    'image_data': vision_page.get('page_image_data', ''),
                    'reason': vision_page.get('analysis_reason', 'unknown')
                })
            
            # Add embedded images
            for img in extracted_images:
                page_num = img.get('page_number', 0)
                if page_num not in images_by_page:
                    images_by_page[page_num] = []
                images_by_page[page_num].append({
                    'type': 'embedded',
                    'image_data': img.get('image_data', ''),
                    'size': f"{img.get('width', 0)}x{img.get('height', 0)}"
                })
            
            # Process images in parallel with Dask
            all_images = []
            image_page_mapping = []
            
            for page_num, page_images in images_by_page.items():
                for img in page_images:
                    all_images.append(img)
                    image_page_mapping.append(page_num)
            
            if not all_images:
                # No images to process, return text only
                return {
                    'success': True,
                    'combined_text': '\n\n'.join(page_texts),
                    'pages_processed': len(page_texts),
                    'images_processed': 0
                }
            
            logger.info(f"Processing {len(all_images)} images for text extraction and integration")
            
            # Process images in parallel
            if self.dask_client:
                image_texts = await self._process_images_with_dask(all_images)
            else:
                image_texts = await self._process_images_with_asyncio(all_images)
            
            # Integrate image texts with page texts
            page_image_texts = {}
            for i, image_text in enumerate(image_texts):
                if image_text and image_text.strip():
                    page_num = image_page_mapping[i]
                    if page_num not in page_image_texts:
                        page_image_texts[page_num] = []
                    page_image_texts[page_num].append({
                        'text': image_text.strip(),
                        'type': all_images[i]['type']
                    })
            
            # Combine text and image content by page
            combined_pages = []
            total_images_processed = sum(1 for text in image_texts if text and text.strip())
            
            for page_num, page_text in enumerate(page_texts, 1):
                combined_page_content = page_text
                
                # Add image text for this page
                if page_num in page_image_texts:
                    page_image_content = []
                    for img_text in page_image_texts[page_num]:
                        page_image_content.append(f"[{img_text['type'].title()} Image Text]: {img_text['text']}")
                    
                    if page_image_content:
                        combined_page_content += "\n\n" + "\n\n".join(page_image_content)
                
                combined_pages.append(combined_page_content)
            
            # Join all pages
            combined_text = '\n\n'.join(combined_pages)
            
            return {
                'success': True,
                'combined_text': combined_text,
                'pages_processed': len(page_texts),
                'images_processed': total_images_processed
            }
            
        except Exception as e:
            logger.error(f"Integrated text and image extraction failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _extract_image_text_parallel(self, pdf_path: str, options: Dict[str, Any]) -> Dict[str, Any]:
        """Extract text from images using existing PDFProcessor + ImageAnalyzer with Dask"""
        try:
            # Use PDFProcessor to get all images (page images + embedded images)
            result = await self.pdf_processor.process_pdf_unified(pdf_path, {
                'extract_text': False,
                'extract_images': True,
                'extract_tables': False
            })
            
            if not result.get('success'):
                return {
                    'success': False,
                    'error': f"PDF image extraction failed: {result.get('error')}"
                }
            
            # Extract images from PDFProcessor result
            image_analysis = result.get('image_analysis', {})
            vision_analyses = image_analysis.get('vision_analyses', [])  # Full page images
            extracted_images = image_analysis.get('extracted_images', [])  # Embedded images
            
            all_images = []
            
            # Add full page images (for pages with no text or containing images)
            for vision_page in vision_analyses:
                all_images.append({
                    'type': 'page',
                    'page_number': vision_page.get('page_number', 0),
                    'image_data': vision_page.get('page_image_data', ''),
                    'index': len(all_images)
                })
            
            # Add embedded images
            for img in extracted_images:
                all_images.append({
                    'type': 'embedded',
                    'page_number': img.get('page_number', 0),
                    'image_data': img.get('image_data', ''),
                    'index': len(all_images)
                })
            
            if not all_images:
                return {
                    'success': True,
                    'text': '',
                    'images_processed': 0
                }
            
            logger.info(f"Processing {len(all_images)} images for text extraction using ImageAnalyzer")
            
            # Process images in parallel with Dask
            if self.dask_client:
                image_texts = await self._process_images_with_dask(all_images)
            else:
                image_texts = await self._process_images_with_asyncio(all_images)
            
            # Combine all image text
            combined_text = ""
            successful_count = 0
            
            for i, text in enumerate(image_texts):
                if text and text.strip():
                    img_info = all_images[i]
                    combined_text += f"\n--- {img_info['type'].title()} {i+1} (Page {img_info['page_number']}) ---\n"
                    combined_text += text.strip() + "\n"
                    successful_count += 1
            
            return {
                'success': True,
                'text': combined_text,
                'images_processed': successful_count,
                'total_images': len(all_images)
            }
            
        except Exception as e:
            logger.error(f"Image text extraction failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _process_images_with_dask(self, images: List[Dict[str, Any]]) -> List[str]:
        """Process images in parallel using Dask + ImageAnalyzer"""
        try:
            @delayed
            def analyze_image_dask(image_data: str) -> str:
                """Dask delayed function for image analysis"""
                import asyncio
                async def _analyze():
                    try:
                        # Use existing ImageAnalyzer
                        result = await analyze(
                            image=image_data, 
                            prompt="Extract all text visible in this image. Return only the text content, maintaining original formatting."
                        )
                        if result.success:
                            return result.response.strip()
                        else:
                            logger.warning(f"Image analysis failed: {result.error}")
                            return ""
                    except Exception as e:
                        logger.warning(f"Image analysis exception: {e}")
                        return ""
                
                # Run async function in new event loop for Dask
                try:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    return loop.run_until_complete(_analyze())
                finally:
                    loop.close()
            
            # Create delayed tasks for each image
            tasks = []
            for img in images:
                if img['image_data']:  # Only process if image data exists
                    task = analyze_image_dask(img['image_data'])
                    tasks.append(task)
                else:
                    tasks.append(delayed(lambda: "")())
            
            # Execute in parallel with Dask
            logger.info(f"Executing {len(tasks)} image analysis tasks with Dask + ImageAnalyzer...")
            results = dask.compute(*tasks)
            
            return list(results)
            
        except Exception as e:
            logger.error(f"Dask image processing failed: {e}")
            # Fallback to asyncio
            return await self._process_images_with_asyncio(images)
    
    async def _process_images_with_asyncio(self, images: List[Dict[str, Any]]) -> List[str]:
        """Process images with asyncio + ImageAnalyzer (fallback)"""
        try:
            semaphore = asyncio.Semaphore(5)  # Limit concurrent VLM requests
            
            async def analyze_single(img: Dict[str, Any]) -> str:
                async with semaphore:
                    try:
                        if not img['image_data']:
                            return ""
                        
                        # Use existing ImageAnalyzer
                        result = await analyze(
                            image=img['image_data'],
                            prompt="Extract all text visible in this image. Return only the text content, maintaining original formatting."
                        )
                        if result.success:
                            return result.response.strip()
                        else:
                            logger.warning(f"Image analysis failed: {result.error}")
                            return ""
                    except Exception as e:
                        logger.warning(f"Image analysis exception: {e}")
                        return ""
            
            # Execute with asyncio
            logger.info(f"Executing {len(images)} image analysis tasks with asyncio + ImageAnalyzer...")
            tasks = [analyze_single(img) for img in images]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Handle exceptions in results
            processed_results = []
            for result in results:
                if isinstance(result, Exception):
                    logger.warning(f"Image processing exception: {result}")
                    processed_results.append("")
                else:
                    processed_results.append(result)
            
            return processed_results
            
        except Exception as e:
            logger.error(f"Asyncio image processing failed: {e}")
            return [""] * len(images)
    
    def _create_simple_chunks(self, text: str) -> List[str]:
        """Create simple text chunks (no complex preprocessing)"""
        if not text or len(text) <= self.chunk_size:
            return [text.strip()] if text.strip() else []
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            if end >= len(text):
                # Last chunk
                chunk = text[start:].strip()
                if chunk:
                    chunks.append(chunk)
                break
            else:
                # Find word boundary
                chunk_end = text.rfind(' ', start + self.chunk_size - 200, end)
                if chunk_end <= start:
                    chunk_end = end
                
                chunk = text[start:chunk_end].strip()
                if chunk:
                    chunks.append(chunk)
                
                # Move to next chunk with overlap
                start = chunk_end - self.chunk_overlap
                if start < 0:
                    start = chunk_end
        
        return chunks
    
    async def batch_extract_pdfs(
        self,
        pdf_paths: List[str],
        user_id: int,
        options: Optional[Dict[str, Any]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Extract multiple PDFs in parallel
        
        Args:
            pdf_paths: List of PDF file paths
            user_id: User ID
            options: Processing options
            metadata: Optional metadata
            
        Returns:
            {"success": bool, "results": List[Dict], "summary": Dict}
        """
        start_time = time.time()
        
        try:
            options = options or {}
            max_concurrent = options.get('max_concurrent', 4)
            
            logger.info(f"Batch processing {len(pdf_paths)} PDFs")
            
            # Process PDFs concurrently
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_single_pdf(pdf_path: str) -> Dict[str, Any]:
                async with semaphore:
                    return await self.extract_pdf_to_chunks(
                        pdf_path=pdf_path,
                        user_id=user_id,
                        options=options,
                        metadata=metadata
                    )
            
            # Execute all tasks
            tasks = [process_single_pdf(pdf_path) for pdf_path in pdf_paths]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Analyze results
            successful_results = []
            failed_results = []
            total_chunks = 0
            total_characters = 0
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    failed_results.append({
                        'pdf_path': pdf_paths[i] if i < len(pdf_paths) else 'unknown',
                        'error': str(result)
                    })
                elif result.get('success'):
                    successful_results.append(result)
                    total_chunks += result.get('chunk_count', 0)
                    total_characters += result.get('total_characters', 0)
                else:
                    failed_results.append(result)
            
            processing_time = time.time() - start_time
            
            return {
                'success': True,
                'results': successful_results,
                'failed_results': failed_results,
                'summary': {
                    'total_pdfs': len(pdf_paths),
                    'successful_count': len(successful_results),
                    'failed_count': len(failed_results),
                    'total_chunks': total_chunks,
                    'total_characters': total_characters,
                    'processing_time': processing_time,
                    'max_concurrent': max_concurrent
                }
            }
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }