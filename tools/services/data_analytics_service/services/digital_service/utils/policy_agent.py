#!/usr/bin/env python3
"""
Policy Agent - ç­–ç•¥ä»£ç†ï¼ˆDeep-Thinking RAGçš„æ ¸å¿ƒå¤§è„‘ï¼‰

å®ç°è‡ªæˆ‘åæ€å’Œå¾ªç¯æ¨ç†ï¼š
1. Reflection: æ€»ç»“å½“å‰æ­¥éª¤çš„å‘ç°
2. Policy Decision: å†³å®šæ˜¯å¦ç»§ç»­ç ”ç©¶ï¼ˆCONTINUEï¼‰æˆ–ç»“æŸï¼ˆFINISHï¼‰
3. Plan Revision: å¦‚æœé‡åˆ°æ­»èƒ¡åŒï¼Œä¿®æ”¹è®¡åˆ’

åŸºäºDeep-Thinking RAGè®ºæ–‡çš„æ ¸å¿ƒå¾ªç¯ï¼š
Plan â†’ Retrieve â†’ Reflect â†’ Policy Decision â†’ Loop (or Finish)
"""

import logging
from typing import Dict, Any, List, Optional, Literal
from dataclasses import dataclass
from enum import Enum


logger = logging.getLogger(__name__)


class PolicyAction(Enum):
    """ç­–ç•¥å†³ç­–åŠ¨ä½œ"""
    CONTINUE = "continue"  # ç»§ç»­æ‰§è¡Œè®¡åˆ’
    FINISH = "finish"      # å®Œæˆç ”ç©¶ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    REVISE = "revise"      # ä¿®æ”¹è®¡åˆ’ï¼ˆé‡åˆ°æ­»èƒ¡åŒï¼‰


@dataclass
class ResearchStep:
    """å•ä¸ªç ”ç©¶æ­¥éª¤çš„è®°å½•"""
    step_index: int
    sub_question: str
    summary: str  # è¯¥æ­¥éª¤çš„å‘ç°æ€»ç»“
    sources_count: int
    quality_score: float  # 0-1, è¯¥æ­¥éª¤æ£€ç´¢è´¨é‡
    timestamp: str


@dataclass
class PolicyDecision:
    """ç­–ç•¥å†³ç­–ç»“æœ"""
    action: PolicyAction
    confidence: float  # 0-1, å†³ç­–ç½®ä¿¡åº¦
    justification: str  # å†³ç­–ç†ç”±
    next_step_index: Optional[int] = None  # å¦‚æœCONTINUEï¼Œä¸‹ä¸€æ­¥çš„ç´¢å¼•
    plan_revision: Optional[Dict[str, Any]] = None  # å¦‚æœREVISEï¼Œä¿®æ”¹åçš„è®¡åˆ’


class PolicyAgent:
    """
    ç­–ç•¥ä»£ç† - Deep-Thinking RAGçš„è‡ªæˆ‘åæ€å’Œæ§åˆ¶æµæ ¸å¿ƒ

    æ ¸å¿ƒèŒè´£ï¼š
    1. åœ¨æ¯ä¸ªç ”ç©¶æ­¥éª¤åè¿›è¡Œåæ€ï¼ˆReflectionï¼‰
    2. å†³å®šæ˜¯å¦ç»§ç»­ã€å®Œæˆã€æˆ–ä¿®æ”¹è®¡åˆ’ï¼ˆPolicy Decisionï¼‰
    3. é˜²æ­¢æ— é™å¾ªç¯ï¼ˆmax_iterationsä¿æŠ¤ï¼‰
    """

    def __init__(
        self,
        max_iterations: int = 5,
        min_quality_threshold: float = 0.3,
        use_llm: bool = False,
        llm_generator: Optional[Any] = None
    ):
        """
        Args:
            max_iterations: æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
            min_quality_threshold: æœ€ä½è´¨é‡é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼å¯èƒ½è§¦å‘REVISEï¼‰
            use_llm: æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œç­–ç•¥å†³ç­–
            llm_generator: LLMç”Ÿæˆå™¨
        """
        self.max_iterations = max_iterations
        self.min_quality_threshold = min_quality_threshold
        self.use_llm = use_llm
        self.llm_generator = llm_generator
        self.logger = logging.getLogger(self.__class__.__name__)

    async def reflect(
        self,
        sub_question: str,
        retrieved_context: str,
        llm_generator: Optional[Any] = None
    ) -> str:
        """
        åæ€æ­¥éª¤ - æ€»ç»“å½“å‰ç ”ç©¶æ­¥éª¤çš„å‘ç°

        Args:
            sub_question: å­é—®é¢˜
            retrieved_context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            llm_generator: LLMç”Ÿæˆå™¨ï¼ˆå¦‚æœä¸ä½¿ç”¨self.llm_generatorï¼‰

        Returns:
            ç®€æ´çš„ä¸€å¥è¯æ€»ç»“
        """
        generator = llm_generator or self.llm_generator

        if not generator:
            # é™çº§ï¼šä½¿ç”¨ç®€å•çš„æˆªæ–­æ€»ç»“
            summary = retrieved_context[:200].replace('\n', ' ') + "..."
            self.logger.warning("No LLM available for reflection, using truncated context")
            return f"Found information about: {summary}"

        try:
            reflection_prompt = f"""You are a research assistant. Based on the retrieved context for the sub-question, write a concise, one-sentence summary of the key findings.

Sub-question: {sub_question}

Retrieved Context:
{retrieved_context[:1000]}

Summary (one sentence, max 150 chars):"""

            if hasattr(generator, 'generate'):
                summary = await generator.generate(reflection_prompt)
            elif callable(generator):
                summary = await generator(reflection_prompt)
            else:
                raise ValueError("Invalid LLM generator")

            # æ¸…ç†å’Œæˆªæ–­
            summary = summary.strip().replace('\n', ' ')
            if len(summary) > 200:
                summary = summary[:197] + "..."

            self.logger.info(f"ğŸ¤” Reflected: {summary}")
            return summary

        except Exception as e:
            self.logger.error(f"Reflection failed: {e}")
            return f"Found relevant information for: {sub_question}"

    async def decide(
        self,
        original_query: str,
        plan_steps: List[Dict[str, Any]],
        research_history: List[ResearchStep],
        current_step_index: int
    ) -> PolicyDecision:
        """
        ç­–ç•¥å†³ç­– - Deep-Thinking RAGçš„æ ¸å¿ƒæ§åˆ¶æµ

        åˆ†æç ”ç©¶å†å²ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼šCONTINUEã€FINISHã€æˆ–REVISE

        Args:
            original_query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            plan_steps: å®Œæ•´çš„ç ”ç©¶è®¡åˆ’
            research_history: å·²å®Œæˆçš„ç ”ç©¶æ­¥éª¤å†å²
            current_step_index: å½“å‰æ­¥éª¤ç´¢å¼•

        Returns:
            PolicyDecisionå¯¹è±¡
        """
        # åŸºæœ¬åœæ­¢æ¡ä»¶æ£€æŸ¥
        basic_decision = self._check_basic_stop_conditions(
            plan_steps, research_history, current_step_index
        )
        if basic_decision:
            return basic_decision

        # ä½¿ç”¨LLMæˆ–è§„åˆ™è¿›è¡Œç­–ç•¥å†³ç­–
        if self.use_llm and self.llm_generator:
            return await self._llm_based_decision(
                original_query, plan_steps, research_history, current_step_index
            )
        else:
            return await self._rule_based_decision(
                original_query, plan_steps, research_history, current_step_index
            )

    def _check_basic_stop_conditions(
        self,
        plan_steps: List[Dict[str, Any]],
        research_history: List[ResearchStep],
        current_step_index: int
    ) -> Optional[PolicyDecision]:
        """æ£€æŸ¥åŸºæœ¬åœæ­¢æ¡ä»¶"""
        # æ¡ä»¶1ï¼šè¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°
        if len(research_history) >= self.max_iterations:
            self.logger.warning(f"ğŸš¦ Max iterations ({self.max_iterations}) reached, forcing FINISH")
            return PolicyDecision(
                action=PolicyAction.FINISH,
                confidence=1.0,
                justification=f"Reached maximum iteration limit ({self.max_iterations})",
                next_step_index=None
            )

        # æ¡ä»¶2ï¼šè®¡åˆ’ä¸­æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ
        if current_step_index >= len(plan_steps):
            self.logger.info("ğŸš¦ Plan completed, all steps finished")
            return PolicyDecision(
                action=PolicyAction.FINISH,
                confidence=0.95,
                justification="All planned research steps have been completed",
                next_step_index=None
            )

        # æ¡ä»¶3ï¼šæœ€åä¸€æ­¥æ£€ç´¢å¤±è´¥ï¼ˆæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼‰
        if research_history and research_history[-1].sources_count == 0:
            self.logger.warning("ğŸš¦ Last retrieval failed, attempting to continue with next step")
            # ä¸ç›´æ¥å¤±è´¥ï¼Œè€Œæ˜¯å°è¯•ç»§ç»­ä¸‹ä¸€æ­¥
            if current_step_index < len(plan_steps) - 1:
                return PolicyDecision(
                    action=PolicyAction.CONTINUE,
                    confidence=0.5,
                    justification="Previous step found no results, trying next step in plan",
                    next_step_index=current_step_index + 1
                )

        return None  # æ²¡æœ‰åŸºæœ¬åœæ­¢æ¡ä»¶ï¼Œç»§ç»­è¿›è¡Œç­–ç•¥å†³ç­–

    async def _rule_based_decision(
        self,
        original_query: str,
        plan_steps: List[Dict[str, Any]],
        research_history: List[ResearchStep],
        current_step_index: int
    ) -> PolicyDecision:
        """
        åŸºäºè§„åˆ™çš„ç­–ç•¥å†³ç­–ï¼ˆå¿«é€Ÿã€æ— LLMè°ƒç”¨ï¼‰

        è§„åˆ™é€»è¾‘ï¼š
        1. å¦‚æœæ‰€æœ‰æ­¥éª¤è´¨é‡>é˜ˆå€¼ï¼Œä¸”æ”¶é›†äº†è¶³å¤Ÿä¿¡æ¯ â†’ FINISH
        2. å¦‚æœå½“å‰æ­¥éª¤è´¨é‡ä½ï¼Œä½†è¿˜æœ‰å…¶ä»–æ­¥éª¤ â†’ CONTINUE
        3. å¦‚æœå¤šä¸ªæ­¥éª¤è´¨é‡éƒ½ä½ â†’ REVISEï¼ˆé‡æ–°è§„åˆ’ï¼‰
        """
        # è®¡ç®—æ•´ä½“è´¨é‡
        avg_quality = sum(s.quality_score for s in research_history) / len(research_history) if research_history else 0
        low_quality_count = sum(1 for s in research_history if s.quality_score < self.min_quality_threshold)

        # å†³ç­–é€»è¾‘
        # Case 1: è´¨é‡è‰¯å¥½ï¼Œå·²æ”¶é›†è¶³å¤Ÿä¿¡æ¯ â†’ FINISH
        if avg_quality > 0.6 and len(research_history) >= 2:
            return PolicyDecision(
                action=PolicyAction.FINISH,
                confidence=0.85,
                justification=f"Sufficient high-quality information collected (avg quality: {avg_quality:.2f})",
                next_step_index=None
            )

        # Case 2: è¿˜æœ‰æ­¥éª¤æœªå®Œæˆ â†’ CONTINUE
        if current_step_index < len(plan_steps):
            confidence = 0.8 if avg_quality > 0.5 else 0.6
            return PolicyDecision(
                action=PolicyAction.CONTINUE,
                confidence=confidence,
                justification=f"Continuing with step {current_step_index + 1}/{len(plan_steps)}",
                next_step_index=current_step_index
            )

        # Case 3: å¤šä¸ªæ­¥éª¤è´¨é‡ä½ â†’ REVISE
        if low_quality_count >= 2:
            self.logger.warning(f"ğŸ”„ {low_quality_count} steps with low quality, consider revision")
            return PolicyDecision(
                action=PolicyAction.REVISE,
                confidence=0.7,
                justification=f"{low_quality_count} steps had low retrieval quality",
                next_step_index=None,
                plan_revision={'suggestion': 'expand search scope or simplify query'}
            )

        # Default: FINISH with moderate confidence
        return PolicyDecision(
            action=PolicyAction.FINISH,
            confidence=0.7,
            justification="Plan completed, proceeding to final synthesis",
            next_step_index=None
        )

    async def _llm_based_decision(
        self,
        original_query: str,
        plan_steps: List[Dict[str, Any]],
        research_history: List[ResearchStep],
        current_step_index: int
    ) -> PolicyDecision:
        """
        åŸºäºLLMçš„ç­–ç•¥å†³ç­–ï¼ˆæ›´æ™ºèƒ½ä½†æ›´æ…¢ï¼‰

        è®©LLMä½œä¸º"master strategist"åˆ†æç ”ç©¶è¿›å±•å¹¶åšå‡ºå†³ç­–
        """
        try:
            # æ„å»ºç ”ç©¶å†å²æ€»ç»“
            history_summary = "\n".join([
                f"Step {s.step_index + 1}: {s.sub_question}\n  Summary: {s.summary}\n  Quality: {s.quality_score:.2f}"
                for s in research_history
            ])

            # æ„å»ºè®¡åˆ’æ€»ç»“
            plan_summary = "\n".join([
                f"{i + 1}. {step.get('sub_question', step.get('description', 'N/A'))}"
                for i, step in enumerate(plan_steps)
            ])

            prompt = f"""You are a master research strategist. Analyze the research progress and decide the next action.

Original Question: {original_query}

Research Plan:
{plan_summary}

Completed Steps ({len(research_history)}/{len(plan_steps)}):
{history_summary}

Current Step Index: {current_step_index}

Based on the research history, decide:
- **FINISH**: If sufficient information has been gathered to answer the original question
- **CONTINUE**: If more research steps are needed
- **REVISE**: If the current plan is not working (multiple low-quality results)

Output format (JSON):
{{
    "action": "FINISH|CONTINUE|REVISE",
    "confidence": 0.0-1.0,
    "justification": "brief reason (max 150 chars)"
}}"""

            if hasattr(self.llm_generator, 'generate_json'):
                response = await self.llm_generator.generate_json(prompt)
            elif callable(self.llm_generator):
                response_text = await self.llm_generator(prompt)
                import json
                response = json.loads(response_text)
            else:
                raise ValueError("Invalid LLM generator")

            action = PolicyAction(response['action'].lower())
            confidence = float(response.get('confidence', 0.8))
            justification = response.get('justification', 'LLM-based decision')

            next_step = current_step_index if action == PolicyAction.CONTINUE else None

            self.logger.info(
                f"ğŸ¤– LLM Policy Decision: {action.value.upper()} "
                f"(confidence={confidence:.2f})"
            )

            return PolicyDecision(
                action=action,
                confidence=confidence,
                justification=justification,
                next_step_index=next_step
            )

        except Exception as e:
            self.logger.error(f"LLM policy decision failed: {e}, falling back to rule-based")
            return await self._rule_based_decision(
                original_query, plan_steps, research_history, current_step_index
            )


# ===== ä¾¿æ·å·¥å‚å‡½æ•° =====

def create_default_policy_agent() -> PolicyAgent:
    """åˆ›å»ºé»˜è®¤ç­–ç•¥ä»£ç†ï¼ˆåŸºäºè§„åˆ™ï¼‰"""
    return PolicyAgent(
        max_iterations=5,
        min_quality_threshold=0.3,
        use_llm=False
    )


def create_llm_policy_agent(llm_generator: Any) -> PolicyAgent:
    """åˆ›å»ºLLMé©±åŠ¨çš„ç­–ç•¥ä»£ç†ï¼ˆæ›´æ™ºèƒ½ï¼‰"""
    return PolicyAgent(
        max_iterations=7,  # LLMæ¨¡å¼å¯ä»¥å…è®¸æ›´å¤šè¿­ä»£
        min_quality_threshold=0.3,
        use_llm=True,
        llm_generator=llm_generator
    )


def create_conservative_policy_agent() -> PolicyAgent:
    """åˆ›å»ºä¿å®ˆçš„ç­–ç•¥ä»£ç†ï¼ˆæ›´å¿«ç»“æŸï¼‰"""
    return PolicyAgent(
        max_iterations=3,  # æ›´å¿«ç»“æŸ
        min_quality_threshold=0.5,  # æ›´é«˜è´¨é‡è¦æ±‚
        use_llm=False
    )
