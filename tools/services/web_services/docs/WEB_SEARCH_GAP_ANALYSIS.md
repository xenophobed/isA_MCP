# Web Search ç¼ºé™·åˆ†æ - åŸºäº API èƒ½åŠ›

## ğŸ¯ ç¬¬ä¸€æ€§åŸç†ï¼šSearch API èƒ½åšä»€ä¹ˆï¼Ÿ

### Brave Search API è¿”å›çš„å†…å®¹ï¼š

```json
{
  "web": {
    "results": [
      {
        "title": "é¡µé¢æ ‡é¢˜",
        "url": "https://example.com/page",
        "description": "ä¸»è¦ç‰‡æ®µ",
        "extra_snippets": ["é¢å¤–ç‰‡æ®µ1", "é¢å¤–ç‰‡æ®µ2"],  // âœ… å·²æœ‰
        "age": "2 days ago",
        "language": "en",
        "meta_url": {
          "scheme": "https",
          "netloc": "example.com",
          "path": "/page"
        }
      }
    ]
  },
  "news": { ... },  // æ–°é—»èšç±»
  "videos": { ... }, // è§†é¢‘ç»“æœ
  "locations": { ... }, // åœ°ç†ä½ç½®
  "infobox": { ... },  // âŒ éœ€è¦é«˜çº§è®¢é˜…
  "faq": { ... },      // âŒ éœ€è¦é«˜çº§è®¢é˜…
  "discussions": { ... } // âŒ éœ€è¦é«˜çº§è®¢é˜…
}
```

### Tavily API è¿”å›çš„å†…å®¹ï¼š

```json
{
  "results": [
    {
      "title": "é¡µé¢æ ‡é¢˜",
      "url": "https://example.com",
      "content": "é¢„æå–çš„ç›¸å…³å†…å®¹ç‰‡æ®µï¼ˆå·²ä¼˜åŒ–ï¼‰", // â­ å…³é”®å·®å¼‚
      "score": 0.95,
      "raw_content": "å®Œæ•´é¡µé¢å†…å®¹ï¼ˆå¯é€‰ï¼‰"  // â­ å¯ä»¥ç›´æ¥æ‹¿åˆ°å†…å®¹
    }
  ],
  "images": [ ... ],
  "query": "åŸå§‹æŸ¥è¯¢"
}
```

---

## ğŸ” å…³é”®å‘ç°ï¼šAPI çš„èƒ½åŠ›è¾¹ç•Œ

### Brave Search APIï¼š

**âœ… èƒ½åšï¼š**
1. è¿”å›æœç´¢ç»“æœï¼ˆtitle, URL, snippetsï¼‰
2. æä¾› extra_snippetsï¼ˆé¢å¤–ä¸Šä¸‹æ–‡ï¼‰
3. æ”¯æŒè¿‡æ»¤å™¨ï¼ˆæ—¶é—´ã€ç±»å‹ã€Gogglesï¼‰
4. è¿”å›ç»“æ„åŒ–æ•°æ®ï¼ˆnews, videos, locationsï¼‰

**âŒ ä¸èƒ½åšï¼š**
1. **ä¸è¿”å›å®Œæ•´é¡µé¢å†…å®¹** - åªæœ‰ snippets
2. ä¸åšå†…å®¹æå–/è§£æ
3. ä¸æ£€æŸ¥ robots.txt
4. ä¸å¤„ç†åçˆ¬è™«

### Tavily APIï¼š

**âœ… èƒ½åšï¼š**
1. è¿”å›æœç´¢ç»“æœ
2. **é¢„æå–ç›¸å…³å†…å®¹ç‰‡æ®µ** - å·²ä¼˜åŒ–ç»™ LLM
3. **å¯é€‰è¿”å›å®Œæ•´é¡µé¢å†…å®¹** (`include_raw_content=true`)
4. é’ˆå¯¹ RAG ä¼˜åŒ–

**âŒ ä¸èƒ½åšï¼š**
1. ä¸æ·±åº¦å®šåˆ¶å†…å®¹æå–ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
2. ä»·æ ¼è¾ƒè´µï¼ˆ$8/1000 æ¬¡ vs Brave $3/1000 æ¬¡ï¼‰

---

## ğŸ’¡ æˆ‘ä»¬å½“å‰çš„å®ç°ï¼š

### å½“å‰æµç¨‹ï¼ˆ`web_search_service.py`ï¼‰ï¼š

```python
# Step 1: è°ƒç”¨ Brave Search API
search_result = await search_engine.search(query, count=10)
# è¿”å›ï¼š[{title, url, snippet}, {title, url, snippet}, ...]

# Step 2: å¦‚æœéœ€è¦ summarize
if summarize:
    for url in top_N_urls:
        # ğŸ‘‡ è¿™é‡Œæ˜¯é—®é¢˜ï¼éœ€è¦è‡ªå·±æŠ“å–å®Œæ•´å†…å®¹
        crawl_result = await crawl_service.crawl_and_analyze(url)
        content = crawl_result.get("content")
        fetched_contents.append(content)

    # Step 3: ç”¨ RAG ç”Ÿæˆæ‘˜è¦
    summary = await rag_service.generate_summary(fetched_contents)
```

---

## ğŸ¯ çœŸæ­£çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ

### ç”¨æˆ·éœ€æ±‚å±‚æ¬¡ï¼š

#### Level 1: åŸºç¡€æœç´¢ï¼ˆå·²æ»¡è¶³ âœ…ï¼‰
```
ç”¨æˆ·ï¼šæœç´¢ "python tutorial"
è¿”å›ï¼š10 ä¸ªæœç´¢ç»“æœï¼ˆtitle + URL + snippetï¼‰
```
**å½“å‰çŠ¶æ€ï¼š** âœ… å®Œç¾æ”¯æŒï¼ˆBrave API ç›´æ¥è¿”å›ï¼‰

---

#### Level 2: æœç´¢ + AI æ‘˜è¦ï¼ˆå½“å‰æœ‰é—®é¢˜ âš ï¸ï¼‰
```
ç”¨æˆ·ï¼šæœç´¢ "AI agents 2024" å¹¶ç”Ÿæˆæ‘˜è¦
éœ€è¦ï¼š
  1. æœç´¢ç»“æœï¼ˆBrave APIï¼‰âœ…
  2. æŠ“å– top 5 URLs çš„å®Œæ•´å†…å®¹ âš ï¸
  3. ç”¨ RAG ç”Ÿæˆæ‘˜è¦ âœ…
```

**é—®é¢˜æ‰€åœ¨ï¼š**
- Step 2 éœ€è¦ `WebCrawlService` æŠ“å–å†…å®¹
- ä½† Brave API **åªè¿”å› snippets**ï¼Œä¸å¤Ÿç”¨äºæ‘˜è¦
- å¿…é¡»è‡ªå·±çˆ¬å–å®Œæ•´é¡µé¢

**ä¸ºä»€ä¹ˆä¸ç”¨ Tavilyï¼Ÿ**
- Tavily å·²ç»æå–å†…å®¹ï¼Œä½†ï¼š
  - ä»·æ ¼è´µ 2.7xï¼ˆ$8 vs $3/1000ï¼‰
  - å†…å®¹æ ¼å¼å›ºå®šï¼Œæ— æ³•å®šåˆ¶
  - æˆ‘ä»¬éœ€è¦æ›´çµæ´»çš„æå–ï¼ˆreadability, BS4, VLMï¼‰

---

## ğŸ”§ ç¼ºé™·åˆ†æï¼šæˆ‘ä»¬ç¼ºä»€ä¹ˆï¼Ÿ

### å½“å‰ `WebCrawlService` çš„é—®é¢˜ï¼š

#### é—®é¢˜ 1ï¼šä¸æ£€æŸ¥ robots.txt âŒ
```python
# å½“å‰ä»£ç 
crawl_result = await crawl_service.crawl_and_analyze(url)
# ğŸ‘† ç›´æ¥æŠ“å–ï¼Œä¸æ£€æŸ¥æ˜¯å¦å…è®¸
```

**å½±å“ï¼š**
- å¯èƒ½è¿åç½‘ç«™è§„åˆ™ï¼ˆå¦‚ Twitter/X ç¦æ­¢çˆ¬è™«ï¼‰
- å¯èƒ½å¯¼è‡´ IP è¢«å°
- ä¸ç¬¦åˆé“å¾·è§„èŒƒ

**è§£å†³ï¼š**
```python
# æ·»åŠ  robots.txt æ£€æŸ¥
from utils.robots_checker import get_robots_checker

robots = get_robots_checker()
can_fetch, reason = await robots.can_fetch(url, autonomous=True)
if not can_fetch:
    return {"error": "robots_txt_blocked", "reason": reason}
```

---

#### é—®é¢˜ 2ï¼šå†…å®¹æå–è´¨é‡å·® âš ï¸
```python
# å½“å‰ä»£ç ï¼šç›´æ¥ç”¨ BS4
bs4_result = await bs4_extract(url, enhanced=True)
# è¿”å›ï¼šåŸå§‹æ–‡æœ¬ï¼ŒåŒ…å«å¹¿å‘Šã€å¯¼èˆªã€footer ç­‰å™ªéŸ³
```

**å½±å“ï¼š**
- æå–å†…å®¹åŒ…å«å¤§é‡å™ªéŸ³
- æµªè´¹ LLM tokensï¼ˆå™ªéŸ³å†…å®¹å  40-60%ï¼‰
- æ‘˜è¦è´¨é‡ä¸‹é™

**å¯¹æ¯”ï¼š**

| æ–¹æ³• | å†…å®¹è´¨é‡ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|---------|------|---------|
| **BS4ï¼ˆå½“å‰ï¼‰** | â­â­ åŒ…å«å™ªéŸ³ | âš¡ å¿« | ç®€å•é¡µé¢ |
| **Readability** | â­â­â­â­ æ¸…æ´å†…å®¹ | âš¡ å¿« | æ–‡ç« /åšå®¢ |
| **VLM Vision** | â­â­â­â­â­ æœ€å‡†ç¡® | ğŸŒ æ…¢ | å¤æ‚é¡µé¢ |
| **Tavily API** | â­â­â­ é¢„å¤„ç†å†…å®¹ | âš¡âš¡ å¾ˆå¿« | é€šç”¨ï¼ˆä½†è´µï¼‰|

**è§£å†³ï¼š**
```python
# ä½¿ç”¨ readability ä¼˜å…ˆ
from utils.content_extractor import get_content_extractor

extractor = get_content_extractor()
result = extractor.extract(html, url)  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•

if result["method"] == "readability":
    # å†…å®¹è´¨é‡æå‡ 40-60%
    clean_content = result["content"]  # Markdown æ ¼å¼ï¼Œæ— å™ªéŸ³
```

---

#### é—®é¢˜ 3ï¼šå¤§é¡µé¢å¤„ç†ä¸å½“ âš ï¸
```python
# å½“å‰ä»£ç 
content = crawl_result.get("content", "")
fetched_contents.append({
    "content": content[:2000]  # ç¡¬æˆªæ–­ï¼
})
```

**å½±å“ï¼š**
- é‡è¦å†…å®¹å¯èƒ½åœ¨ 2000 å­—ç¬¦å
- ç”¨æˆ·ä¸çŸ¥é“å†…å®¹è¢«æˆªæ–­
- æ— æ³•è·å–åç»­å†…å®¹

**è§£å†³ï¼š**
```python
# æ·»åŠ åˆ†é¡µæ”¯æŒ
extractor = get_content_extractor()
result = extractor.extract_with_pagination(
    html, url,
    max_length=5000,
    start_index=0
)

if result["pagination"]["has_more"]:
    # å‘Šè¯‰ç”¨æˆ·è¿˜æœ‰æ›´å¤šå†…å®¹
    content += f"\n\nğŸ’¡ è¿˜æœ‰ {result['pagination']['remaining_chars']} å­—ç¬¦"
    content += f"ä½¿ç”¨ start_index={result['pagination']['next_start_index']} ç»§ç»­"
```

---

#### é—®é¢˜ 4ï¼šUser-Agent ä¸é€æ˜ âš ï¸
```python
# å½“å‰ä»£ç ï¼šä½¿ç”¨é»˜è®¤ UA
response = requests.get(url)  # User-Agent: python-requests/2.x
```

**å½±å“ï¼š**
- è¢«è¯†åˆ«ä¸ºçˆ¬è™«ï¼Œå¯èƒ½è¢«å°
- ä¸é€æ˜ï¼Œç½‘ç«™ç®¡ç†å‘˜æ— æ³•è”ç³»
- ä¸ç¬¦åˆæœ€ä½³å®è·µ

**è§£å†³ï¼š**
```python
from utils.user_agents import get_user_agent

ua = get_user_agent(autonomous=True)
# "isA_MCP/1.0 (Autonomous; AI Agent; +https://github.com/...)"

response = requests.get(url, headers={"User-Agent": ua})
```

---

## ğŸ“Š çœŸæ­£çš„ç¼ºé™·æ€»ç»“

### ä¸æ˜¯æ¶æ„é—®é¢˜ï¼Œæ˜¯å®ç°ç»†èŠ‚é—®é¢˜ï¼š

| ç¼ºé™· | å½±å“ | ä¼˜å…ˆçº§ | è§£å†³æ–¹æ¡ˆ | å·¥ä½œé‡ |
|------|------|--------|---------|--------|
| **1. æ—  robots.txt æ£€æŸ¥** | æ³•å¾‹/é“å¾·é£é™© | ğŸ”´ HIGH | é›†æˆ RobotsChecker | 1h |
| **2. å†…å®¹æå–è´¨é‡å·®** | æ‘˜è¦è´¨é‡ä½ | ğŸ”´ HIGH | ç”¨ ContentExtractor | 2h |
| **3. å¤§é¡µé¢æˆªæ–­** | ç”¨æˆ·ä½“éªŒå·® | ğŸŸ¡ MEDIUM | æ·»åŠ åˆ†é¡µ | 2h |
| **4. User-Agent ä¸é€æ˜** | è¢«å°é£é™© | ğŸŸ¡ MEDIUM | ç”¨é€æ˜ UA | 0.5h |
| **5. æ— é”™è¯¯é‡è¯•** | å¯é æ€§å·® | ğŸŸ¢ LOW | æ·»åŠ é‡è¯•é€»è¾‘ | 1h |

---

## ğŸ¯ æˆ‘ä»¬çš„ç›®æ ‡ï¼ˆæ˜ç¡®åŒ–ï¼‰

### Goal 1: ä¿æŒ Brave API çš„ä¼˜åŠ¿
- âœ… ä»·æ ¼ä¾¿å®œï¼ˆ$3/1000 vs Tavily $8/1000ï¼‰
- âœ… éšç§ä¼˜å…ˆ
- âœ… ç‹¬ç«‹ç´¢å¼•ï¼ˆä¸ä¾èµ– Google/Bingï¼‰
- âœ… é«˜çº§åŠŸèƒ½ï¼ˆGoggles, è¿‡æ»¤å™¨ï¼‰

### Goal 2: å¼¥è¡¥ Brave API çš„ä¸è¶³
- âŒ Brave åªè¿”å› snippets â†’ âœ… æˆ‘ä»¬è‡ªå·±æŠ“å–å®Œæ•´å†…å®¹
- âŒ Brave ä¸æå–å†…å®¹ â†’ âœ… æˆ‘ä»¬ç”¨ readability æå–
- âŒ Brave ä¸æ£€æŸ¥ robots.txt â†’ âœ… æˆ‘ä»¬æ£€æŸ¥åˆè§„æ€§

### Goal 3: æä¾›æ¯” Tavily æ›´å¥½çš„ä½“éªŒ
- âœ… çµæ´»çš„å†…å®¹æå–ï¼ˆreadability, BS4, VLM å¯é€‰ï¼‰
- âœ… å®Œæ•´çš„é¡µé¢è®¿é—®ï¼ˆä¸åªæ˜¯é¢„å¤„ç†ç‰‡æ®µï¼‰
- âœ… æ›´ä½çš„æˆæœ¬
- âœ… æ›´å¼ºçš„å®šåˆ¶èƒ½åŠ›

---

## ğŸš€ æ­£ç¡®çš„ä¼˜åŒ–ç­–ç•¥

### ä¸æ˜¯"åˆå¹¶æœåŠ¡"ï¼Œè€Œæ˜¯"å¢å¼ºæŠ“å–"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Brave Search API (æœç´¢å±‚)               â”‚
â”‚  è¿”å›ï¼š[{title, url, snippet}, ...]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    WebCrawlService (å†…å®¹æŠ“å–å±‚) â† è¿™é‡Œä¼˜åŒ–ï¼    â”‚
â”‚  âœ… æ£€æŸ¥ robots.txt                             â”‚
â”‚  âœ… ç”¨ readability æå–å¹²å‡€å†…å®¹                  â”‚
â”‚  âœ… æ”¯æŒåˆ†é¡µ                                    â”‚
â”‚  âœ… é€æ˜ User-Agent                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RAG Service (æ‘˜è¦ç”Ÿæˆå±‚)                â”‚
â”‚  è¾“å…¥ï¼šå¹²å‡€çš„å†…å®¹ç‰‡æ®µ                            â”‚
â”‚  è¾“å‡ºï¼šå¸¦å¼•ç”¨çš„æ‘˜è¦                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… å…·ä½“è¡ŒåŠ¨è®¡åˆ’

### Phase 1: å¿«é€Ÿä¿®å¤ï¼ˆä»Šå¤©ï¼Œ2 å°æ—¶ï¼‰

**ç›®æ ‡ï¼š** è®©æŠ“å–ç¬¦åˆé“å¾·è§„èŒƒ + æå‡å†…å®¹è´¨é‡

**ä¿®æ”¹æ–‡ä»¶ï¼š** `services/web_crawl_service.py`

**ä¿®æ”¹å†…å®¹ï¼š**
```python
async def _bs4_extraction_path(self, url: str, ...):
    # 1. æ·»åŠ  robots.txt æ£€æŸ¥ï¼ˆ30 åˆ†é’Ÿï¼‰
    from utils.robots_checker import get_robots_checker
    from utils.user_agents import get_user_agent

    robots = get_robots_checker()
    can_fetch, reason = await robots.can_fetch(url, autonomous=True)
    if not can_fetch:
        return {"success": False, "error": reason, "method": "blocked"}

    # 2. ç”¨ readability æ›¿æ¢ BS4ï¼ˆ1 å°æ—¶ï¼‰
    from utils.content_extractor import get_content_extractor

    # Fetch HTML with proper UA
    ua = get_user_agent(autonomous=True)
    async with httpx.AsyncClient() as client:
        response = await client.get(url, headers={"User-Agent": ua})

    # Extract with readability
    extractor = get_content_extractor()
    result = extractor.extract(response.text, url, response.headers.get("content-type"))

    # Use extracted content (much cleaner!)
    content = result["content"]  # Markdown, no noise
    title = result.get("title", "")

    # 3. ç»§ç»­ç°æœ‰çš„åˆ†æé€»è¾‘...
```

**é¢„æœŸæ”¶ç›Šï¼š**
- âœ… ç¬¦åˆ robots.txt è§„èŒƒ
- âœ… å†…å®¹è´¨é‡æå‡ 40-60%
- âœ… æ‘˜è¦æ›´å‡†ç¡®
- âœ… å‡å°‘ token æµªè´¹

---

### Phase 2: ç”¨æˆ·ä½“éªŒä¼˜åŒ–ï¼ˆæœ¬å‘¨ï¼Œ3 å°æ—¶ï¼‰

**ç›®æ ‡ï¼š** å¤„ç†å¤§é¡µé¢ + æ›´å¥½çš„é”™è¯¯æç¤º

1. **æ·»åŠ åˆ†é¡µæ”¯æŒ**ï¼ˆ2 å°æ—¶ï¼‰
   - åœ¨ `web_search_tools.py` æ·»åŠ  `max_content_length`, `content_start_index` å‚æ•°
   - åœ¨ `web_crawl_service.py` ä½¿ç”¨ `extract_with_pagination()`
   - åœ¨å“åº”ä¸­æ·»åŠ åˆ†é¡µæç¤º

2. **æ”¹è¿›é”™è¯¯å¤„ç†**ï¼ˆ1 å°æ—¶ï¼‰
   - robots.txt é˜»æ­¢ â†’ æ¸…æ™°æç¤ºç”¨æˆ·
   - æå–å¤±è´¥ â†’ è‡ªåŠ¨é™çº§åˆ° VLM
   - ç½‘ç»œé”™è¯¯ â†’ é‡è¯• 3 æ¬¡

---

## ğŸ¯ æœ€ç»ˆç­”æ¡ˆ

### ä½ çš„é—®é¢˜ï¼š"æ ¸å¿ƒæ˜¯ä¸æ˜¯æŠŠ web_crawl_service åˆå¹¶åˆ° searchï¼Ÿ"

**ç­”æ¡ˆï¼šä¸æ˜¯ï¼**

### æ­£ç¡®çš„ç†è§£ï¼š

1. **Brave/Tavily API èƒ½åšä»€ä¹ˆï¼Ÿ**
   - è¿”å›æœç´¢ç»“æœï¼ˆURLs + snippetsï¼‰
   - Brave: åªæœ‰ç‰‡æ®µï¼Œéœ€è¦æˆ‘ä»¬æŠ“å–å®Œæ•´å†…å®¹
   - Tavily: æœ‰é¢„å¤„ç†å†…å®¹ï¼Œä½†è´µä¸”ä¸çµæ´»

2. **æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ**
   - ç”¨ Brave çš„ä½ä»·æ ¼ + é«˜è´¨é‡
   - è‡ªå·±æŠ“å–å†…å®¹ï¼Œä½†è¦åšå¾—**é“å¾·**å’Œ**é«˜è´¨é‡**

3. **ç¼ºé™·å¦‚ä½•å¼¥è¡¥ï¼Ÿ**
   - âœ… å¢å¼º `WebCrawlService`ï¼Œä¸æ˜¯åˆå¹¶
   - âœ… æ·»åŠ  robots.txtã€readabilityã€åˆ†é¡µ
   - âœ… ä¿æŒæ¶æ„æ¸…æ™°ï¼šæœç´¢ â†’ æŠ“å– â†’ æ‘˜è¦

---

## ğŸš€ ä¸‹ä¸€æ­¥

**æˆ‘å»ºè®®ï¼šç«‹å³å®æ–½ Phase 1ï¼ˆ2 å°æ—¶å¿«é€Ÿä¿®å¤ï¼‰**

å…·ä½“æ­¥éª¤ï¼š
1. ä¿®æ”¹ `web_crawl_service.py` çš„ `_bs4_extraction_path()` æ–¹æ³•
2. æ·»åŠ  robots.txt æ£€æŸ¥
3. ç”¨ ContentExtractor æ›¿æ¢åŸç”Ÿ BS4
4. æµ‹è¯•éªŒè¯

**è¦æˆ‘å¼€å§‹å®æ–½å—ï¼Ÿ** è¿˜æ˜¯ä½ è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Ÿ
