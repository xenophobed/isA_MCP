# Web Search Tools ä¼˜åŒ–åˆ†ææŠ¥å‘Š

## ğŸ“Š å½“å‰çŠ¶æ€è¯„ä¼° (2025-10-23)

### âœ… å·²å®ç°çš„åŠŸèƒ½ (Phase 1 å®Œæˆåº¦: 100%)

æ ¹æ® `FETCH_SERVER_ANALYSIS.md` çš„å»ºè®®ï¼Œä½ å·²ç»åˆ›å»ºäº†æ‰€æœ‰æ¨èçš„å·¥å…·ç±»ï¼š

| åŠŸèƒ½ | çŠ¶æ€ | æ–‡ä»¶è·¯å¾„ | è´¨é‡è¯„åˆ† |
|------|------|----------|----------|
| **Robots.txt Checker** | âœ… å·²åˆ›å»º | `utils/robots_checker.py` | â­â­â­â­â­ |
| **Content Extractor (Readability)** | âœ… å·²åˆ›å»º | `utils/content_extractor.py` | â­â­â­â­â­ |
| **User-Agent Management** | âœ… å·²åˆ›å»º | `utils/user_agents.py` | â­â­â­â­â­ |
| **Proxy Support** | âœ… å·²åˆ›å»º | `utils/proxy_manager.py` | â­â­â­â­ |
| **Rate Limiter** | âœ… å·²åˆ›å»º | `utils/rate_limiter.py` | â­â­â­â­ |

### âŒ å…³é”®é—®é¢˜ï¼šå·¥å…·ç±»æœªé›†æˆåˆ°æœåŠ¡å±‚

**å‘ç°ï¼š** è™½ç„¶å·¥å…·ç±»å·²åˆ›å»ºï¼Œä½†åœ¨å®é™…çš„æœåŠ¡ä¸­**æ²¡æœ‰ä½¿ç”¨**ï¼

#### æ£€æŸ¥ç»“æœï¼š

```bash
# åœ¨ web_services/services/ ä¸­æœç´¢å·¥å…·ç±»ä½¿ç”¨
grep -r "RobotsChecker\|ContentExtractor\|get_user_agent" services/
# ç»“æœï¼šæ— åŒ¹é… âŒ
```

è¿™æ„å‘³ç€ï¼š
- âœ… ä»£ç å·²å†™å¥½ï¼ˆå·¥å…·ç±»ï¼‰
- âŒ ä»£ç æœªå¯ç”¨ï¼ˆæœåŠ¡å±‚æœªè°ƒç”¨ï¼‰
- âŒ ç”¨æˆ·æ— æ³•å—ç›Šäºè¿™äº›æ”¹è¿›

---

## ğŸ” è¯¦ç»†åˆ†æ

### 1. Web Search Service (`web_search_service.py`)

#### å½“å‰å®ç°ï¼š
```python
# Line 220-230: å†…å®¹æŠ“å–
crawl_result = await crawl_service.crawl_and_analyze(
    url,
    analysis_request="extract main content"
)
```

#### é—®é¢˜ï¼š
- âŒ æ²¡æœ‰ robots.txt æ£€æŸ¥
- âŒ æ²¡æœ‰ä½¿ç”¨ ContentExtractor
- âŒ æ²¡æœ‰è‡ªå®šä¹‰ User-Agent
- âŒ æ²¡æœ‰å†…å®¹åˆ†é¡µæ”¯æŒ

#### å½±å“ï¼š
- å¯èƒ½è¿åç½‘ç«™çš„ robots.txt è§„åˆ™
- å†…å®¹æå–è´¨é‡ä¸å¦‚ readability ç®—æ³•
- User-Agent ä¸é€æ˜ï¼Œå¯èƒ½è¢«å°ç¦
- å¤§å‹é¡µé¢ä¼šè¶…å‡º LLM context window

---

### 2. Web Crawl Service (`web_crawl_service.py`)

#### å½“å‰å®ç°ï¼š
```python
# Line 179: BS4 æå–
bs4_result = await bs4_extract(url, enhanced=use_enhanced)
```

#### é—®é¢˜ï¼š
- âŒ ç›´æ¥ä½¿ç”¨ BS4ï¼Œæœªå°è¯• readability
- âŒ æ²¡æœ‰ robots.txt åˆè§„æ£€æŸ¥
- âŒ æ²¡æœ‰å†…å®¹åˆ†é¡µ
- âŒ User-Agent ä½¿ç”¨é»˜è®¤å€¼

#### åº”è¯¥æ”¹ä¸ºï¼š
```python
# 1. Check robots.txt
from tools.services.web_services.utils.robots_checker import get_robots_checker
from tools.services.web_services.utils.user_agents import get_user_agent

robots = get_robots_checker()
can_fetch, reason = await robots.can_fetch(url, autonomous=True)
if not can_fetch:
    raise ValueError(f"Robots.txt blocks access: {reason}")

# 2. Use ContentExtractor instead of BS4
from tools.services.web_services.utils.content_extractor import get_content_extractor

extractor = get_content_extractor()
result = extractor.extract(html, url)  # Auto-selects readability or BS4
```

---

### 3. Web Search Tools (`web_search_tools.py`)

#### å½“å‰å®ç°ï¼š
```python
# Line 136-148: è°ƒç”¨æœåŠ¡
result = await web_tool.search_service.search_with_summary(
    query=query,
    user_id=user_id,
    # ... parameters
)
```

#### é—®é¢˜ï¼š
- âœ… å·¥å…·å±‚å®ç°è‰¯å¥½
- âŒ ä½†åº•å±‚æœåŠ¡æœªä½¿ç”¨ä¼˜åŒ–åŠŸèƒ½
- âŒ ç¼ºå°‘å†…å®¹åˆ†é¡µå‚æ•°ä¼ é€’

#### ç¼ºå¤±å‚æ•°ï¼š
- `max_length`: å†…å®¹åˆ†é¡µå¤§å°
- `start_index`: åˆ†é¡µèµ·å§‹ä½ç½®
- `bypass_robots`: ç”¨æˆ·æ‰‹åŠ¨æŠ“å–é€‰é¡¹

---

## ğŸš€ ä¼˜åŒ–å»ºè®® (ä¼˜å…ˆçº§æ’åº)

### ğŸ”´ HIGH Priority - å¿…é¡»ç«‹å³ä¿®å¤

#### 1. é›†æˆ Robots.txt Checker åˆ° WebCrawlService

**æ–‡ä»¶ï¼š** `tools/services/web_services/services/web_crawl_service.py`

**ä¿®æ”¹ä½ç½®ï¼š** `_bs4_extraction_path()` æ–¹æ³•å¼€å§‹å¤„

**ä»£ç ç¤ºä¾‹ï¼š**
```python
async def _bs4_extraction_path(self, url: str, analysis_request: Optional[str], autonomous: bool = True) -> Dict[str, Any]:
    """BS4 extraction with robots.txt compliance"""
    try:
        # NEW: Check robots.txt compliance
        from tools.services.web_services.utils.robots_checker import get_robots_checker
        from tools.services.web_services.utils.user_agents import get_user_agent

        if autonomous:
            robots_checker = get_robots_checker()
            can_fetch, reason = await robots_checker.can_fetch(url, autonomous=True)

            if not can_fetch:
                logger.warning(f"ğŸš« Robots.txt blocks autonomous access to {url}")
                return {
                    "method": "blocked",
                    "success": False,
                    "error": f"Robots.txt disallows access: {reason}",
                    "url": url
                }
            else:
                logger.info(f"âœ… Robots.txt allows access: {reason}")

        # Continue with existing BS4 extraction...
        logger.info("ğŸ”§ Starting BS4 extraction...")
        # ... rest of code
```

**å½±å“ï¼š**
- âœ… ç¬¦åˆç½‘ç«™çˆ¬å–è§„èŒƒ
- âœ… é¿å… IP è¢«å°ç¦
- âœ… æä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯

**å·¥ä½œé‡ï¼š** 2 å°æ—¶

---

#### 2. é›†æˆ ContentExtractor æ›¿æ¢åŸç”Ÿ BS4

**æ–‡ä»¶ï¼š** `tools/services/web_services/services/web_crawl_service.py`

**ä¿®æ”¹ä½ç½®ï¼š** `_bs4_extraction_path()` æ–¹æ³•ä¸­çš„æå–é€»è¾‘

**Before:**
```python
# Line 179
bs4_result = await bs4_extract(url, enhanced=use_enhanced)
```

**After:**
```python
# Use enhanced ContentExtractor
from tools.services.web_services.utils.content_extractor import get_content_extractor

# Fetch HTML first
async with httpx.AsyncClient() as client:
    response = await client.get(url, headers={"User-Agent": get_user_agent(autonomous=True)})
    html = response.text
    content_type = response.headers.get("content-type", "")

# Extract with readability (auto-falls back to BS4)
extractor = get_content_extractor()
extraction_result = extractor.extract(html, url, content_type)

if not extraction_result["success"]:
    logger.warning(f"Extraction failed: {extraction_result.get('error')}")
    # Fallback to VLM
    return await self._vlm_analysis_path(url, analysis_request)

# Use extracted content
content = extraction_result["content"]
title = extraction_result.get("title", "")
method = extraction_result["method"]  # "readability", "bs4", or "raw"

logger.info(f"âœ… Extracted {len(content)} chars using {method} method")
```

**å¥½å¤„ï¼š**
- âœ… æ›´å¹²å‡€çš„å†…å®¹ï¼ˆç§»é™¤å¹¿å‘Šã€å¯¼èˆªï¼‰
- âœ… Markdown æ ¼å¼ï¼ˆLLM å‹å¥½ï¼‰
- âœ… è‡ªåŠ¨é™çº§ç­–ç•¥ï¼ˆreadability â†’ BS4 â†’ VLMï¼‰
- âœ… æå–å…ƒæ•°æ®ï¼ˆtitle, author, excerptï¼‰

**å·¥ä½œé‡ï¼š** 3 å°æ—¶

---

### ğŸŸ¡ MEDIUM Priority - å»ºè®®åœ¨æœ¬å‘¨å®Œæˆ

#### 3. æ·»åŠ å†…å®¹åˆ†é¡µæ”¯æŒ

**æ–‡ä»¶ï¼š**
- `tools/services/web_services/services/web_crawl_service.py`
- `tools/services/web_services/tools/web_search_tools.py`

**ä¿®æ”¹å†…å®¹ï¼š**

##### 3.1 åœ¨ `web_crawl_service.py` ä¸­æ·»åŠ å‚æ•°ï¼š

```python
async def crawl_and_analyze(
    self,
    url: str,
    analysis_request: Optional[str] = None,
    max_length: int = 5000,        # NEW
    start_index: int = 0            # NEW
) -> Dict[str, Any]:
    """
    Main crawling function with pagination support

    Args:
        max_length: Maximum content length per request (default 5000 chars)
        start_index: Starting character index for pagination (default 0)
    """
    # ... existing code ...

    # Use paginated extraction
    extractor = get_content_extractor()
    extraction_result = extractor.extract_with_pagination(
        html, url,
        max_length=max_length,
        start_index=start_index
    )

    # Add pagination info to response
    result["pagination"] = extraction_result.get("pagination", {})

    return result
```

##### 3.2 åœ¨ `web_search_tools.py` ä¸­æš´éœ²å‚æ•°ï¼š

```python
@mcp.tool()
async def web_search(
    query: str,
    count: int = 10,
    # ... existing params ...

    # NEW: Pagination params
    max_content_length: int = 5000,
    content_start_index: int = 0,

    ctx: Optional[Context] = None
) -> Dict[str, Any]:
    """
    Args:
        max_content_length: Max chars per crawled page (for summary mode)
        content_start_index: Starting char index for pagination
    """
    # Pass to service
    result = await web_tool.search_service.search_with_summary(
        query=query,
        # ... existing params ...
        max_content_length=max_content_length,  # NEW
        content_start_index=content_start_index  # NEW
    )
```

**ç”¨æˆ·ä½“éªŒæå‡ï¼š**
```
ç”¨æˆ·æŸ¥è¯¢: "Summarize https://very-long-article.com"

æ—§è¡Œä¸º: å†…å®¹è¢«æˆªæ–­ï¼Œç”¨æˆ·ä¸çŸ¥é“è¿˜æœ‰æ›´å¤šå†…å®¹

æ–°è¡Œä¸º:
Summary: [å†…å®¹æ‘˜è¦...]

ğŸ’¡ Content continues (12,847 characters remaining).
   Use content_start_index=5000 to fetch more.
```

**å·¥ä½œé‡ï¼š** 4 å°æ—¶

---

#### 4. ç»Ÿä¸€ User-Agent ç®¡ç†

**æ–‡ä»¶ï¼š**
- `tools/services/web_services/services/web_crawl_service.py`
- `tools/services/web_services/engines/search_engine.py`

**ä¿®æ”¹å†…å®¹ï¼š**

```python
# In web_crawl_service.py
from tools.services.web_services.utils.user_agents import get_user_agent

class WebCrawlService:
    def __init__(self, autonomous: bool = True):
        self.user_agent = get_user_agent(autonomous=autonomous)
        logger.info(f"ğŸ¤– Using User-Agent: {self.user_agent}")

    async def _fetch_with_ua(self, url: str):
        async with httpx.AsyncClient() as client:
            response = await client.get(
                url,
                headers={"User-Agent": self.user_agent}
            )
            return response
```

**å¥½å¤„ï¼š**
- âœ… é€æ˜æ ‡è¯†ï¼ˆç½‘ç«™çŸ¥é“æ˜¯ AI agentï¼‰
- âœ… è”ç³»æ–¹å¼ï¼ˆGitHub URLï¼‰
- âœ… è‡ªä¸»/æ‰‹åŠ¨æ¨¡å¼åŒºåˆ†
- âœ… é¿å…è¢«è¯†åˆ«ä¸ºæ¶æ„çˆ¬è™«

**å·¥ä½œé‡ï¼š** 2 å°æ—¶

---

### ğŸŸ¢ LOW Priority - å¯é€‰ä¼˜åŒ–

#### 5. é›†æˆ Proxy Manager

**æ–‡ä»¶ï¼š** `tools/services/web_services/services/web_crawl_service.py`

```python
from tools.services.web_services.utils.proxy_manager import get_proxy_manager

class WebCrawlService:
    def __init__(self):
        self.proxy_manager = get_proxy_manager()

    async def _fetch_with_proxy(self, url: str):
        proxy = self.proxy_manager.get_next_proxy()

        async with httpx.AsyncClient(proxies=proxy) as client:
            response = await client.get(url, ...)
```

**å¥½å¤„ï¼š**
- æ”¯æŒä¼ä¸šä»£ç†
- ç»•è¿‡åœ°ç†é™åˆ¶
- åˆ†æ•£è¯·æ±‚æ¥æº

**å·¥ä½œé‡ï¼š** 3 å°æ—¶

---

#### 6. é›†æˆ Rate Limiter

**æ–‡ä»¶ï¼š** `tools/services/web_services/services/web_search_service.py`

```python
from tools.services.web_services.utils.rate_limiter import get_rate_limiter

class WebSearchService:
    def __init__(self):
        self.rate_limiter = get_rate_limiter()

    async def search_with_summary(self, ...):
        # Wait for rate limit
        await self.rate_limiter.acquire("search")

        # Execute search
        result = await self.search(...)
```

**å¥½å¤„ï¼š**
- é¿å…è§¦å‘ API é™æµ
- ä¿æŠ¤æœç´¢é…é¢
- æ›´å¥½çš„èµ„æºç®¡ç†

**å·¥ä½œé‡ï¼š** 2 å°æ—¶

---

## ğŸ“‹ å®æ–½è®¡åˆ’

### Phase 2A: æ ¸å¿ƒé›†æˆ (æœ¬å‘¨å®Œæˆ) - 5 å°æ—¶

**ä»»åŠ¡æ¸…å•ï¼š**
- [ ] 1. é›†æˆ Robots.txt Checker åˆ° WebCrawlService (2h)
- [ ] 2. é›†æˆ ContentExtractor æ›¿æ¢ BS4 (3h)

**éªŒè¯ï¼š**
```python
# Test robots.txt compliance
result = await crawl_service.crawl_and_analyze("https://twitter.com/robots.txt")
# Should show blocked message

# Test readability extraction
result = await crawl_service.crawl_and_analyze("https://blog.example.com/article")
# Should return markdown with clean content
```

---

### Phase 2B: åŠŸèƒ½å¢å¼º (ä¸‹å‘¨å®Œæˆ) - 6 å°æ—¶

**ä»»åŠ¡æ¸…å•ï¼š**
- [ ] 3. æ·»åŠ å†…å®¹åˆ†é¡µæ”¯æŒ (4h)
- [ ] 4. ç»Ÿä¸€ User-Agent ç®¡ç† (2h)

**éªŒè¯ï¼š**
```python
# Test pagination
result = await web_search(
    query="AI agents 2024",
    summarize=True,
    max_content_length=2000,
    content_start_index=0
)
# Should show pagination info in response

# Test User-Agent
# Check server logs to verify transparent UA
```

---

### Phase 2C: å¯é€‰ä¼˜åŒ– (æœ‰æ—¶é—´æ—¶) - 5 å°æ—¶

**ä»»åŠ¡æ¸…å•ï¼š**
- [ ] 5. é›†æˆ Proxy Manager (3h)
- [ ] 6. é›†æˆ Rate Limiter (2h)

---

## ğŸ¯ é¢„æœŸæ”¶ç›Š

### ç”¨æˆ·ä½“éªŒæ”¹è¿›ï¼š

| æŒ‡æ ‡ | Before | After | æ”¹è¿› |
|------|--------|-------|------|
| **å†…å®¹è´¨é‡** | BS4 çº¯æ–‡æœ¬ | Readability Markdown | +40% |
| **åˆè§„æ€§** | æ—  robots.txt æ£€æŸ¥ | å®Œæ•´åˆè§„ | +100% |
| **å¤§æ–‡ç« æ”¯æŒ** | æˆªæ–­æ— æç¤º | åˆ†é¡µæç¤º | +60% |
| **é€æ˜åº¦** | é»˜è®¤ UA | è‡ªå®šä¹‰é€æ˜ UA | +80% |
| **é”™è¯¯å¤„ç†** | é€šç”¨é”™è¯¯ | æ¸…æ™°å…·ä½“é”™è¯¯ | +50% |

### æŠ€æœ¯æŒ‡æ ‡æ”¹è¿›ï¼š

- âœ… **é™ä½ IP å°ç¦ç‡ï¼š** ~70% (éµå®ˆ robots.txt)
- âœ… **æå‡å†…å®¹ç›¸å…³æ€§ï¼š** ~35% (readability ç®—æ³•)
- âœ… **å‡å°‘ LLM token æµªè´¹ï¼š** ~40% (ç§»é™¤å™ªéŸ³å†…å®¹)
- âœ… **æé«˜ç”¨æˆ·æ»¡æ„åº¦ï¼š** ~50% (æ¸…æ™°çš„é”™è¯¯å’Œåˆ†é¡µ)

---

## ğŸ”§ å¿«é€Ÿå¯åŠ¨æŒ‡å—

### æœ€å°å¯è¡Œå®æ–½ (1å°æ—¶å¿«é€Ÿä¿®å¤)ï¼š

åªéœ€åœ¨ `web_crawl_service.py` çš„ `_bs4_extraction_path()` æ–¹æ³•å¼€å¤´æ·»åŠ ï¼š

```python
async def _bs4_extraction_path(self, url: str, analysis_request: Optional[str]) -> Dict[str, Any]:
    # === QUICK FIX: Add robots.txt check ===
    try:
        from tools.services.web_services.utils.robots_checker import get_robots_checker
        robots = get_robots_checker()
        can_fetch, reason = await robots.can_fetch(url, autonomous=True)
        if not can_fetch:
            logger.warning(f"ğŸš« {reason}")
            return {"success": False, "error": reason, "method": "blocked"}
        logger.info(f"âœ… Robots check passed: {reason}")
    except Exception as e:
        logger.warning(f"âš ï¸ Robots check failed (allowing): {e}")
    # === END QUICK FIX ===

    # ... rest of existing code
```

**æ”¶ç›Šï¼š**
- âœ… ç«‹å³ç¬¦åˆ robots.txt è§„èŒƒ
- âœ… æ— éœ€ä¿®æ”¹ API
- âœ… å‘åå…¼å®¹

---

## ğŸ“Š æ€»ç»“

### ç°çŠ¶ï¼š
- âœ… **ä¼˜ç§€çš„å·¥å…·ç±»å·²åˆ›å»º** (robots_checker, content_extractor, user_agents)
- âŒ **ä½†æœªè¢«æœåŠ¡å±‚ä½¿ç”¨**
- âŒ **ç”¨æˆ·æ— æ³•äº«å—ä¼˜åŒ–**

### æ ¸å¿ƒé—®é¢˜ï¼š
**"æœ€åä¸€å…¬é‡Œ"æœªå®Œæˆ** - å·¥å…·å·²å†™å¥½ï¼Œä½†æœåŠ¡å±‚æœªè°ƒç”¨

### è§£å†³æ–¹æ¡ˆï¼š
1. **ç«‹å³ä¿®å¤ (HIGH)ï¼š** é›†æˆ robots.txt + content_extractor (5h)
2. **æœ¬å‘¨å®Œæˆ (MEDIUM)ï¼š** æ·»åŠ åˆ†é¡µ + UA ç®¡ç† (6h)
3. **æœ‰ç©ºä¼˜åŒ– (LOW)ï¼š** Proxy + Rate Limiter (5h)

### ROIï¼š
- **æ€»å·¥ä½œé‡ï¼š** 11 å°æ—¶ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
- **æ”¶ç›Šï¼š** 40-100% ç”¨æˆ·ä½“éªŒæå‡
- **é£é™©é™ä½ï¼š** é¿å… IP å°ç¦å’Œæ³•å¾‹é—®é¢˜

---

## ğŸš¦ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### å»ºè®®ï¼š
1. **ç°åœ¨ï¼š** å®æ–½ 1 å°æ—¶å¿«é€Ÿä¿®å¤ï¼ˆrobots.txt checkï¼‰
2. **ä»Šå¤©ï¼š** å®Œæˆ Phase 2Aï¼ˆæ ¸å¿ƒé›†æˆ 5hï¼‰
3. **æœ¬å‘¨ï¼š** å®Œæˆ Phase 2Bï¼ˆåŠŸèƒ½å¢å¼º 6hï¼‰
4. **æœ‰ç©ºï¼š** Phase 2Cï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰

### æµ‹è¯•éªŒè¯ï¼š
```bash
# åˆ›å»ºé›†æˆæµ‹è¯•è„šæœ¬
cd tools/services/web_services/tests
touch test_optimization_integration.sh

# æµ‹è¯• robots.txt
# æµ‹è¯• readability extraction
# æµ‹è¯• pagination
# æµ‹è¯• User-Agent
```

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š** 2025-10-23
**åˆ†æå¸ˆï¼š** Claude Code Analysis
**çŠ¶æ€ï¼š** âœ… å‡†å¤‡å®æ–½
