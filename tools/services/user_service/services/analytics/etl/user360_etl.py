#!/usr/bin/env python3
"""
User360 ETL Service - ç”¨æˆ·å¤§å®½è¡¨ETLå¤„ç†
ä»ç°æœ‰æ•°æ®æºè®¡ç®—æ±‡æ€»æŒ‡æ ‡ï¼Œä½¿ç”¨lang_extractoråˆ†ææ–‡æœ¬å†…å®¹ï¼Œç”ŸæˆUser360å¤§å®½è¡¨
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import json
from core.logging import get_logger
from core.database import get_supabase_client
from tools.services.data_analytics_service.services.data_service.transformation.lang_extractor import LangExtractor, ExtractionType

logger = get_logger(__name__)

class User360ETL:
    """
    User360 ETLæœåŠ¡ - ç®€å•ç›´æ¥çš„è®¾è®¡
    
    åŠŸèƒ½:
    1. ä»ç°æœ‰è¡¨(users, sessions, memory_*, etc.)è¯»å–åŸå§‹æ•°æ®
    2. ä½¿ç”¨lang_extractoråˆ†ææ–‡æœ¬å†…å®¹
    3. è®¡ç®—å„ç§æ±‡æ€»æŒ‡æ ‡
    4. å†™å…¥user_360_profileè¡¨
    """
    
    def __init__(self):
        self.db_client = get_supabase_client()
        self.lang_extractor = LangExtractor()
        
    async def process_user(self, user_id: str, force_refresh: bool = False) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªç”¨æˆ·çš„User360æ•°æ®"""
        try:
            logger.info(f"ğŸ”„ Processing User360 for user: {user_id}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
            if not force_refresh:
                recent_profile = await self._check_recent_profile(user_id)
                if recent_profile:
                    logger.info(f"âš ï¸ User {user_id} already processed recently, skipping")
                    return {"processed": False, "reason": "recent_profile_exists"}
            
            # 1. è·å–åŸºç¡€ç”¨æˆ·ä¿¡æ¯
            user_profile = await self._get_user_profile(user_id)
            if not user_profile:
                return {"processed": False, "reason": "user_not_found"}
            
            # 2. è®¡ç®—ä¼šè¯ç»Ÿè®¡
            session_stats = await self._calculate_session_stats(user_id)
            
            # 3. è®¡ç®—Memoryç»Ÿè®¡
            memory_stats = await self._calculate_memory_stats(user_id)
            
            # 4. æ”¶é›†å¹¶åˆ†ææ–‡æœ¬å†…å®¹
            content_text = await self._collect_user_text_content(user_id)
            content_insights = await self._analyze_content_with_lang_extractor(content_text)
            
            # 5. æå–æ—¶é—´è¡Œä¸ºç»Ÿè®¡æŒ‡æ ‡
            time_behavior_stats = await self._extract_time_behavior_stats(user_id)
            
            # 6. ç»„è£…User360è®°å½•
            user360_record = {
                # åŸºç¡€ä¿¡æ¯
                "user_id": user_id,
                "org_id": user_profile.get("organization_id"),
                "email": user_profile.get("email"),
                "username": user_profile.get("name"),
                "registration_date": user_profile.get("created_at"),
                "last_login_at": user_profile.get("updated_at"),
                "account_status": "active" if user_profile.get("is_active") else "inactive",
                "subscription_tier": user_profile.get("subscription_status", "free"),
                
                # ä¼šè¯ç»Ÿè®¡
                **session_stats,
                
                # Memoryç»Ÿè®¡  
                **memory_stats,
                
                # å†…å®¹æ´å¯Ÿ
                **content_insights,
                
                # æ—¶é—´è¡Œä¸ºç»Ÿè®¡æŒ‡æ ‡
                **time_behavior_stats,
                
                # ETLå…ƒæ•°æ®
                "last_etl_run_at": datetime.now().isoformat(),
                "data_completeness_score": self._calculate_completeness_score(session_stats, memory_stats, content_insights),
                "schema_version": "1.0"
            }
            
            # 6. å†™å…¥User360è¡¨
            await self._upsert_user360_profile(user360_record)
            
            logger.info(f"âœ… Successfully processed User360 for user: {user_id}")
            return {"processed": True, "user_id": user_id, "record": user360_record}
            
        except Exception as e:
            logger.error(f"âŒ Failed to process User360 for user {user_id}: {e}")
            raise
    
    async def _check_recent_profile(self, user_id: str, hours_threshold: int = 6) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ€è¿‘å·²å¤„ç†è¿‡"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours_threshold)
            response = self.db_client.table('user_360_profile')\
                .select('last_etl_run_at')\
                .eq('user_id', user_id)\
                .gte('last_etl_run_at', cutoff_time.isoformat())\
                .execute()
            return bool(response.data)
        except Exception:
            return False
    
    async def _get_user_profile(self, user_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ç”¨æˆ·åŸºç¡€ä¿¡æ¯"""
        try:
            response = self.db_client.table('users').select('*').eq('user_id', user_id).single().execute()
            return response.data
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to get user profile for {user_id}: {e}")
            return None
    
    async def _calculate_session_stats(self, user_id: str) -> Dict[str, Any]:
        """è®¡ç®—ä¼šè¯ç»Ÿè®¡æ•°æ®"""
        try:
            # è·å–æ‰€æœ‰ä¼šè¯æ•°æ®
            response = self.db_client.table('sessions').select('*').eq('user_id', user_id).execute()
            sessions = response.data or []
            
            if not sessions:
                return {
                    "total_sessions": 0,
                    "total_session_duration_minutes": 0.0,
                    "avg_session_duration_minutes": 0.0,
                    "max_session_duration_minutes": 0.0,
                    "sessions_last_7_days": 0,
                    "sessions_last_30_days": 0,
                    "last_session_date": None,
                    "peak_usage_hours": [],
                    "session_frequency_score": 0.0
                }
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            total_sessions = len(sessions)
            durations = []
            session_hours = []
            recent_sessions_7d = 0
            recent_sessions_30d = 0
            last_session = None
            
            cutoff_7d = datetime.now() - timedelta(days=7)
            cutoff_30d = datetime.now() - timedelta(days=30)
            
            for session in sessions:
                # æ—¶é•¿è®¡ç®—
                if session.get('duration'):
                    durations.append(float(session['duration']) / 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿ
                
                # æ—¶é—´åˆ†æ
                if session.get('created_at'):
                    session_time = datetime.fromisoformat(session['created_at'].replace('Z', '+00:00'))
                    session_hours.append(session_time.hour)
                    
                    if session_time >= cutoff_7d:
                        recent_sessions_7d += 1
                    if session_time >= cutoff_30d:
                        recent_sessions_30d += 1
                    
                    if not last_session or session_time > last_session:
                        last_session = session_time
            
            # è®¡ç®—æ´»è·ƒæ—¶æ®µ
            hour_counts = {}
            for hour in session_hours:
                hour_counts[hour] = hour_counts.get(hour, 0) + 1
            peak_hours = sorted(hour_counts.items(), key=lambda x: x[1], reverse=True)[:3]
            peak_usage_hours = [hour for hour, _ in peak_hours]
            
            return {
                "total_sessions": total_sessions,
                "total_session_duration_minutes": sum(durations),
                "avg_session_duration_minutes": sum(durations) / len(durations) if durations else 0.0,
                "max_session_duration_minutes": max(durations) if durations else 0.0,
                "sessions_last_7_days": recent_sessions_7d,
                "sessions_last_30_days": recent_sessions_30d,
                "last_session_date": last_session.isoformat() if last_session else None,
                "peak_usage_hours": peak_usage_hours,
                "session_frequency_score": min(1.0, recent_sessions_7d / 7.0)  # ç®€å•çš„é¢‘ç‡è¯„åˆ†
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to calculate session stats for {user_id}: {e}")
            return {"total_sessions": 0}
    
    async def _calculate_memory_stats(self, user_id: str) -> Dict[str, Any]:
        """è®¡ç®—Memoryç›¸å…³ç»Ÿè®¡"""
        try:
            # å¹¶è¡ŒæŸ¥è¯¢å„ç§memoryè¡¨
            tasks = [
                self._count_table_records('session_memories', user_id),
                self._count_table_records('factual_memories', user_id),
                self._count_table_records('episodic_memories', user_id),
                self._count_table_records('procedural_memories', user_id),
                self._count_table_records('semantic_memories', user_id),
                self._count_table_records('working_memories', user_id)
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            session_memory_count = results[0] if not isinstance(results[0], Exception) else 0
            factual_memory_count = results[1] if not isinstance(results[1], Exception) else 0
            episodic_memory_count = results[2] if not isinstance(results[2], Exception) else 0
            procedural_memory_count = results[3] if not isinstance(results[3], Exception) else 0
            semantic_memory_count = results[4] if not isinstance(results[4], Exception) else 0
            working_memory_count = results[5] if not isinstance(results[5], Exception) else 0
            
            # è®¡ç®—ä¼šè¯æ¶ˆæ¯ç»Ÿè®¡
            total_messages = await self._count_table_records('session_messages', user_id)
            
            return {
                "session_memory_count": session_memory_count,
                "factual_memory_count": factual_memory_count,
                "episodic_memory_count": episodic_memory_count,
                "procedural_memory_count": procedural_memory_count,
                "semantic_memory_count": semantic_memory_count,
                "working_memory_usage_score": min(1.0, working_memory_count / 10.0),
                "total_messages": total_messages,
                "total_conversations": session_memory_count  # session_memories çº¦ç­‰äº conversations
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to calculate memory stats for {user_id}: {e}")
            return {"total_messages": 0, "total_conversations": 0}
    
    async def _count_table_records(self, table_name: str, user_id: str) -> int:
        """è®¡ç®—è¡¨ä¸­ç”¨æˆ·è®°å½•æ•°"""
        try:
            response = self.db_client.table(table_name)\
                .select('*', count='exact')\
                .eq('user_id', user_id)\
                .execute()
            return response.count or 0
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to count {table_name} for user {user_id}: {e}")
            return 0
    
    async def _analyze_content_with_lang_extractor(self, content_text: str) -> Dict[str, Any]:
        """ä½¿ç”¨lang_extractoråˆ†æç”¨æˆ·å†…å®¹ - é‡æ–°è®¾è®¡ç‰ˆæœ¬ï¼Œæå–çœŸæ­£æœ‰ä»·å€¼çš„æ´å¯Ÿ"""
        try:
            if not content_text or len(content_text) < 50:
                logger.info(f"âš ï¸ Insufficient text content ({len(content_text)} chars), skipping analysis")
                return {
                    "knowledge_domains": [],
                    "primary_use_cases": [],
                    "communication_style": "unknown",
                    "programming_languages": {},
                    "frameworks_and_tools": {},
                    "domain_expertise": {}
                }
            
            # ä½¿ç”¨lang_extractorè¿›è¡Œæ·±åº¦åˆ†æ
            logger.info(f"ğŸ§  Analyzing {len(content_text)} characters of rich content")
            
            # ä½¿ç”¨æœ€æœ‰æ•ˆçš„å…³é”®ä¿¡æ¯æå–æ–¹æ³•
            key_info_result = await self.lang_extractor.extract(
                content_text, 
                ExtractionType.KEY_INFORMATION, 
                max_info=20
            )
            
            # å¤„ç†æå–ç»“æœ
            knowledge_domains = []
            primary_use_cases = []
            programming_languages = {}
            frameworks_tools = {}
            domain_expertise = {}
            communication_style = "conversational"
            
            # å…³é”®ä¿¡æ¯æå–ç»“æœ - æå–æŠ€æœ¯æ ˆå’Œå·¥å…·
            if key_info_result.success and key_info_result.data:
                key_info = key_info_result.data
                
                # ä»ä¸»é¢˜ä¸­æå–çŸ¥è¯†é¢†åŸŸ
                main_topics = key_info.get('main_topics', [])
                knowledge_domains = main_topics[:8]
                
                # ä»å…³é”®äº‹å®ä¸­åˆ†ææŠ€æœ¯æ ˆ
                key_facts = key_info.get('key_facts', [])
                prog_langs = ['Python', 'JavaScript', 'Java', 'C++', 'Go', 'Rust', 'TypeScript', 'SQL', 'R', 'Scala']
                frameworks = ['React', 'Vue', 'Angular', 'Django', 'Flask', 'FastAPI', 'Spring', 'Node.js', 'Express']
                tools = ['Docker', 'Kubernetes', 'Git', 'Jenkins', 'AWS', 'Azure', 'GCP', 'Redis', 'MongoDB', 'PostgreSQL']
                
                # åˆ†ææ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼ˆåŸå§‹å†…å®¹ + æå–çš„äº‹å®å’Œä¸»é¢˜ï¼‰
                full_text = (content_text + ' ' + ' '.join(key_facts) + ' ' + ' '.join(main_topics)).lower()
                
                # è¯†åˆ«ç¼–ç¨‹è¯­è¨€
                for lang in prog_langs:
                    if lang.lower() in full_text:
                        count = full_text.count(lang.lower())
                        confidence = min(0.9, 0.3 + count * 0.1)
                        programming_languages[lang] = round(confidence, 2)
                
                # è¯†åˆ«æ¡†æ¶å’Œå·¥å…·
                for item in frameworks + tools:
                    if item.lower() in full_text:
                        count = full_text.count(item.lower())
                        confidence = min(0.9, 0.4 + count * 0.1)
                        frameworks_tools[item] = round(confidence, 2)
                
                # åŸºäºå†…å®¹æ¨æ–­ä½¿ç”¨åœºæ™¯
                if any(lang in programming_languages for lang in ['Python', 'R', 'SQL']):
                    primary_use_cases.append('data_analysis')
                if any(lang in programming_languages for lang in ['Python', 'TensorFlow', 'PyTorch']):
                    primary_use_cases.append('machine_learning')
                if any(item in frameworks_tools for item in ['React', 'Vue', 'Angular', 'JavaScript']):
                    primary_use_cases.append('web_development')
                if any(item in frameworks_tools for item in ['Docker', 'Kubernetes', 'AWS', 'Azure']):
                    primary_use_cases.append('devops')
                
                # æ¨æ–­ä¸“ä¸šé¢†åŸŸæ°´å¹³
                total_tech_items = len(programming_languages) + len(frameworks_tools)
                if total_tech_items >= 5:
                    domain_expertise['programming'] = 'advanced'
                elif total_tech_items >= 2:
                    domain_expertise['programming'] = 'intermediate'
                elif total_tech_items > 0:
                    domain_expertise['programming'] = 'beginner'
                
                logger.info(f"ğŸ” Extracted: {len(knowledge_domains)} domains, {len(programming_languages)} languages, {len(frameworks_tools)} tools")
                logger.info(f"ğŸ“Š Languages: {list(programming_languages.keys())}")
                logger.info(f"ğŸ› ï¸ Tools: {list(frameworks_tools.keys())}")
                
            else:
                logger.warning("âš ï¸ Key information extraction failed")
            
            # æ„å»ºå®Œæ•´çš„åˆ†æç»“æœ
            analysis_result = {
                "knowledge_domains": knowledge_domains,
                "primary_use_cases": primary_use_cases,
                "communication_style": communication_style,
                "programming_languages": programming_languages,
                "frameworks_and_tools": frameworks_tools,
                "domain_expertise": domain_expertise
            }
            
            logger.info(f"âœ… Content analysis completed: {len(knowledge_domains)} domains, {len(primary_use_cases)} use cases")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ Content analysis failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "knowledge_domains": [],
                "primary_use_cases": [],
                "communication_style": "unknown",
                "programming_languages": {},
                "frameworks_and_tools": {},
                "domain_expertise": {}
            }
    
    async def _collect_user_text_content(self, user_id: str, limit: int = 4000) -> str:
        """æ”¶é›†ç”¨æˆ·çš„æ–‡æœ¬å†…å®¹ç”¨äºåˆ†æ - ä¿®æ­£ç‰ˆæœ¬ï¼Œä½¿ç”¨æ­£ç¡®çš„å­—æ®µå"""
        try:
            content_parts = []
            
            # ä»session_messagesè·å–ç”¨æˆ·æ¶ˆæ¯
            response = self.db_client.table('session_messages')\
                .select('content')\
                .eq('user_id', user_id)\
                .order('created_at', desc=True)\
                .limit(50)\
                .execute()
            
            if response.data:
                for msg in response.data:
                    if msg.get('content'):
                        content_parts.append(f"Message: {msg['content']}")
            
            # ä»factual_memoriesè·å–ç»“æ„åŒ–äº‹å®
            try:
                response = self.db_client.table('factual_memories')\
                    .select('subject, predicate, object_value, context')\
                    .eq('user_id', user_id)\
                    .limit(30)\
                    .execute()
                
                if response.data:
                    for fact in response.data:
                        # æ„é€ è‡ªç„¶è¯­è¨€æè¿°
                        fact_text = f"{fact.get('subject', '')} {fact.get('predicate', '')} {fact.get('object_value', '')}"
                        if fact.get('context'):
                            fact_text += f" ({fact['context']})"
                        content_parts.append(f"Fact: {fact_text}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to get factual memories: {e}")
            
            # ä»episodic_memoriesè·å–ç»å†å†…å®¹
            try:
                response = self.db_client.table('episodic_memories')\
                    .select('episode_title, summary, key_events, lessons_learned')\
                    .eq('user_id', user_id)\
                    .limit(20)\
                    .execute()
                
                if response.data:
                    for episode in response.data:
                        episode_text = episode.get('episode_title', '') + ': ' + episode.get('summary', '')
                        if episode.get('lessons_learned'):
                            episode_text += f" Learned: {episode.get('lessons_learned')}"
                        if episode_text.strip(':'):
                            content_parts.append(f"Experience: {episode_text}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to get episodic memories: {e}")
            
            # ä»semantic_memoriesè·å–æ¦‚å¿µçŸ¥è¯†
            try:
                response = self.db_client.table('semantic_memories')\
                    .select('concept_name, definition, properties')\
                    .eq('user_id', user_id)\
                    .limit(20)\
                    .execute()
                
                if response.data:
                    for concept in response.data:
                        concept_text = f"{concept.get('concept_name', '')} is {concept.get('definition', '')}"
                        if concept.get('properties'):
                            concept_text += f" Properties: {concept.get('properties')}"
                        content_parts.append(f"Concept: {concept_text}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to get semantic memories: {e}")
            
            # ä»procedural_memoriesè·å–æµç¨‹çŸ¥è¯†
            try:
                response = self.db_client.table('procedural_memories')\
                    .select('procedure_name, domain, steps, expected_outcome')\
                    .eq('user_id', user_id)\
                    .limit(15)\
                    .execute()
                
                if response.data:
                    for procedure in response.data:
                        proc_text = f"Procedure: {procedure.get('procedure_name', '')} in {procedure.get('domain', '')}"
                        if procedure.get('steps'):
                            proc_text += f" Steps: {procedure.get('steps')}"
                        if procedure.get('expected_outcome'):
                            proc_text += f" Outcome: {procedure.get('expected_outcome')}"
                        content_parts.append(proc_text)
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to get procedural memories: {e}")
            
            # åˆå¹¶å¹¶é™åˆ¶é•¿åº¦
            full_content = ' '.join(content_parts)
            logger.info(f"ğŸ“„ Collected {len(full_content)} characters from {len(content_parts)} content parts for user {user_id}")
            
            return full_content[:limit] if len(full_content) > limit else full_content
            
        except Exception as e:
            logger.error(f"âŒ Failed to collect user content for {user_id}: {e}")
            return ""
    
    def _calculate_completeness_score(self, session_stats: Dict, memory_stats: Dict, content_insights: Dict) -> float:
        """è®¡ç®—æ•°æ®å®Œæ•´æ€§è¯„åˆ†"""
        score = 0.0
        
        # ä¼šè¯æ•°æ®å®Œæ•´æ€§ (40%)
        if session_stats.get('total_sessions', 0) > 0:
            score += 0.4
        
        # Memoryæ•°æ®å®Œæ•´æ€§ (30%)
        total_memories = sum([
            memory_stats.get('session_memory_count', 0),
            memory_stats.get('factual_memory_count', 0),
            memory_stats.get('episodic_memory_count', 0)
        ])
        if total_memories > 0:
            score += 0.3
        
        # å†…å®¹åˆ†æå®Œæ•´æ€§ (30%)
        if (content_insights.get('knowledge_domains') and 
            len(content_insights.get('knowledge_domains', [])) > 0):
            score += 0.3
        
        return round(score, 2)
    
    async def _upsert_user360_profile(self, record: Dict[str, Any]):
        """æ’å…¥æˆ–æ›´æ–°User360æ¡£æ¡ˆ"""
        try:
            # Upsertæ“ä½œ
            response = self.db_client.table('user_360_profile')\
                .upsert(record, on_conflict='user_id')\
                .execute()
            
            if not response.data:
                raise Exception("Upsert failed - no data returned")
                
            logger.info(f"âœ… User360 profile upserted for user: {record['user_id']}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to upsert User360 profile: {e}")
            raise
    
    async def _extract_time_behavior_stats(self, user_id: str) -> Dict[str, Any]:
        """æå–æ—¶é—´è¡Œä¸ºç»Ÿè®¡æŒ‡æ ‡ - ä¸ºtime serieså’Œpersonaåˆ†ææä¾›æ•°æ®åŸºç¡€"""
        try:
            # 1. ä»sessionsæå–æ´»åŠ¨æ—¶é—´æ¨¡å¼ç»Ÿè®¡
            sessions_stats = await self._extract_sessions_time_stats(user_id)
            
            # 2. ä»user_eventsæå–äº‹ä»¶æ—¶é—´æ¨¡å¼ç»Ÿè®¡ 
            events_stats = await self._extract_events_time_stats(user_id)
            
            # 3. ä»session_messagesæå–æ¶ˆæ¯æ—¶é—´æ¨¡å¼ç»Ÿè®¡
            messages_stats = await self._extract_messages_time_stats(user_id)
            
            return {
                # é«˜å³°æ—¶æ®µç»Ÿè®¡
                "peak_usage_hours": sessions_stats.get("peak_hours", []),
                "preferred_work_hours": sessions_stats.get("preferred_hours", {}),
                
                # å‘¨/æ—¥æ¨¡å¼ç»Ÿè®¡  
                "weekly_usage_pattern": sessions_stats.get("weekly_pattern", {}),
                "cyclical_behaviors": sessions_stats.get("cyclical_patterns", {}),
                
                
                # ä¼šè¯ç»Ÿè®¡æŒ‡æ ‡
                "sessions_last_7_days": sessions_stats.get("sessions_7d", 0),
                "sessions_last_30_days": sessions_stats.get("sessions_30d", 0),
                "avg_session_duration_minutes": sessions_stats.get("avg_duration", 0.0),
                
                # æ¶ˆæ¯ç»Ÿè®¡æŒ‡æ ‡ - ä½¿ç”¨å·²å­˜åœ¨çš„å­—æ®µ
                "avg_messages_per_conversation": messages_stats.get("messages_per_session", 0.0),
                
                # æ—¶åŒºå’Œé€‚åº”æ€§æŒ‡æ ‡
                "timezone": sessions_stats.get("timezone", "UTC"),
                "timezone_adaptation_score": sessions_stats.get("timezone_score", 0.5)
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to extract time behavior stats for {user_id}: {e}")
            return {
                "peak_usage_hours": [],
                "preferred_work_hours": {},
                "weekly_usage_pattern": {},
                "sessions_last_7_days": 0,
                "sessions_last_30_days": 0
            }
    
    async def _extract_sessions_time_stats(self, user_id: str) -> Dict[str, Any]:
        """ä»sessionsè¡¨æå–æ—¶é—´ç»Ÿè®¡æŒ‡æ ‡ + æ—¶é—´-ä¸»é¢˜å…³è”åˆ†æ"""
        try:
            # 1. åŸºç¡€ä¼šè¯ç»Ÿè®¡
            sessions_response = self.db_client.table('sessions')\
                .select('created_at, session_id')\
                .eq('user_id', user_id)\
                .execute()
            
            if not sessions_response.data:
                return {}
            
            sessions = sessions_response.data
            from datetime import datetime, timedelta
            now = datetime.now()
            
            # åŸºç¡€æ—¶é—´ç»Ÿè®¡ - ä¿®å¤æ—¶åŒºé—®é¢˜
            from datetime import timezone
            now_utc = datetime.now(timezone.utc)
            
            sessions_7d = 0
            sessions_30d = 0
            
            for s in sessions:
                created_at_str = s['created_at']
                if created_at_str:
                    # å¤„ç†æ—¶åŒº
                    if created_at_str.endswith('Z'):
                        created_at_str = created_at_str[:-1] + '+00:00'
                    elif '+' not in created_at_str and 'T' in created_at_str:
                        created_at_str += '+00:00'
                    
                    try:
                        created_at = datetime.fromisoformat(created_at_str)
                        if created_at.tzinfo is None:
                            created_at = created_at.replace(tzinfo=timezone.utc)
                        
                        if created_at > now_utc - timedelta(days=7):
                            sessions_7d += 1
                        if created_at > now_utc - timedelta(days=30):
                            sessions_30d += 1
                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to parse timestamp {created_at_str}: {e}")
                        continue
            
            # 2. å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ® - ä¾›åç»­æ¨¡å‹åˆ†æä½¿ç”¨
            time_series_data = self._prepare_time_series_data(sessions)
            
            return {
                "sessions_7d": sessions_7d,
                "sessions_30d": sessions_30d,
                "peak_hours": time_series_data.get("peak_hours", []),
                "preferred_hours": time_series_data.get("hourly_distribution", {}),
                "weekly_pattern": time_series_data.get("daily_distribution", {}),
                "timezone": "UTC"
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to extract session time stats: {e}")
            return {}
    
    async def _extract_events_time_stats(self, user_id: str) -> Dict[str, Any]:
        """ä»user_eventsè¡¨æå–äº‹ä»¶æ—¶é—´ç»Ÿè®¡æŒ‡æ ‡"""
        try:
            response = self.db_client.table('user_events')\
                .select('event_name, timestamp, event_category')\
                .eq('user_id', user_id)\
                .execute()
            
            if response.data:
                events = response.data
                return {
                    "events_per_day": len(events) / 30.0 if events else 0.0,
                    "common_events": list(set([e['event_name'] for e in events[:5]]))
                }
            
            return {"events_per_day": 0.0, "common_events": []}
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to extract events time stats: {e}")
            return {"events_per_day": 0.0, "common_events": []}
    
    async def _extract_messages_time_stats(self, user_id: str) -> Dict[str, Any]:
        """ä»session_messagesè¡¨æå–æ¶ˆæ¯æ—¶é—´ç»Ÿè®¡æŒ‡æ ‡"""
        try:
            response = self.db_client.table('session_messages')\
                .select('created_at, session_id')\
                .eq('user_id', user_id)\
                .execute()
            
            if response.data:
                messages = response.data
                session_count = len(set([m['session_id'] for m in messages]))
                avg_messages_per_session = len(messages) / session_count if session_count > 0 else 0.0
                
                return {
                    "messages_per_session": avg_messages_per_session,
                    "timing_patterns": {}
                }
            
            return {"messages_per_session": 0.0, "timing_patterns": {}}
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to extract messages time stats: {e}")
            return {"messages_per_session": 0.0, "timing_patterns": {}}
    
    def _prepare_time_series_data(self, sessions: list) -> Dict[str, Any]:
        """å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ® - ä¾›æ¨¡å‹æœåŠ¡åˆ†æä½¿ç”¨"""
        try:
            if not sessions:
                return {}
            
            from datetime import datetime
            hourly_counts = {}
            daily_counts = {}
            
            for session in sessions:
                created_at_str = session['created_at']
                if not created_at_str:
                    continue
                    
                # å¤„ç†æ—¶åŒº
                if created_at_str.endswith('Z'):
                    created_at_str = created_at_str[:-1] + '+00:00'
                elif '+' not in created_at_str and 'T' in created_at_str:
                    created_at_str += '+00:00'
                
                try:
                    from datetime import timezone
                    created_at = datetime.fromisoformat(created_at_str)
                    if created_at.tzinfo is None:
                        created_at = created_at.replace(tzinfo=timezone.utc)
                    
                    hour = created_at.hour
                    day_of_week = created_at.weekday()  # 0=Monday
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to parse session timestamp {created_at_str}: {e}")
                    continue
                
                # ç»Ÿè®¡æ¯å°æ—¶æ´»åŠ¨
                hourly_counts[hour] = hourly_counts.get(hour, 0) + 1
                
                # ç»Ÿè®¡æ¯å¤©æ´»åŠ¨
                day_names = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
                day_name = day_names[day_of_week] if day_of_week < 7 else "Unknown"
                daily_counts[day_name] = daily_counts.get(day_name, 0) + 1
            
            # æ‰¾å‡ºé«˜å³°æ—¶æ®µ (æ´»åŠ¨é‡ > å¹³å‡å€¼)
            if hourly_counts:
                avg_activity = sum(hourly_counts.values()) / len(hourly_counts)
                peak_hours = [hour for hour, count in hourly_counts.items() if count > avg_activity]
            else:
                peak_hours = []
            
            return {
                "hourly_distribution": hourly_counts,
                "daily_distribution": daily_counts,
                "peak_hours": peak_hours,
                "total_sessions": len(sessions)
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to prepare time series data: {e}")
            return {}

# å…¨å±€å®ä¾‹
user360_etl = User360ETL()

# ä¾¿æ·å‡½æ•°
async def process_user(user_id: str, force_refresh: bool = False) -> Dict[str, Any]:
    """å¤„ç†å•ä¸ªç”¨æˆ·çš„User360æ•°æ®"""
    return await user360_etl.process_user(user_id, force_refresh)