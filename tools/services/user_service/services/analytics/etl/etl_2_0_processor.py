#!/usr/bin/env python3
"""
ETL 2.0 Processor - åŸºäºç°æœ‰user360_etl.pyé‡æ–°æ¶æ„çš„æ˜Ÿå½¢æ¨¡å¼ETLå¤„ç†å™¨
æ··åˆäº‹å®è¡¨æ¶æ„ï¼šsession_messages + memoryä¸ºæ ¸å¿ƒï¼Œuser_eventsä¸ºè¡¥å……
"""

import asyncio
from datetime import datetime, timedelta, timezone
from typing import Dict, Any, List, Optional, Tuple
import json
from core.logging import get_logger
from core.database import get_supabase_client
from tools.services.data_analytics_service.services.data_service.transformation.lang_extractor import LangExtractor, ExtractionType

logger = get_logger(__name__)

class ETL2Processor:
    """
    ETL 2.0å¤„ç†å™¨ - é‡æ–°æ¶æ„çš„æ˜Ÿå½¢æ¨¡å¼æ•°æ®ä»“åº“ETL
    
    æ ¸å¿ƒç†å¿µï¼š
    1. å¤ç”¨ç°æœ‰user360_etl.pyçš„æ•°æ®æ”¶é›†å’Œåˆ†æé€»è¾‘
    2. å®ç°æ··åˆäº‹å®è¡¨æ¶æ„ï¼šinteraction_facts + behavior_facts + time_behavior_facts
    3. æ”¯æŒå¢é‡å¤„ç†å’Œå®æ—¶æ›´æ–°
    4. ä¸ºpersonaç”Ÿæˆæä¾›ç»“æ„åŒ–æ•°æ®åŸºç¡€
    """
    
    def __init__(self):
        self.db_client = get_supabase_client()
        self.lang_extractor = LangExtractor()
        self.batch_id = f"etl2_batch_{int(datetime.now().timestamp())}"
        
    async def run_full_pipeline(self, user_ids: List[str] = None, batch_size: int = 50) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ETL 2.0ç®¡é“"""
        try:
            logger.info(f"ğŸš€ Starting ETL 2.0 Pipeline - Batch ID: {self.batch_id}")
            
            # 1. åˆ·æ–°ç»´åº¦è¡¨
            await self._refresh_dimensions()
            
            # 2. å¤„ç†ç”¨æˆ·äº¤äº’äº‹å®ï¼ˆæ ¸å¿ƒï¼‰
            interaction_results = await self._process_interaction_facts(user_ids, batch_size)
            
            # 3. å¤„ç†è¡Œä¸ºäº‹ä»¶äº‹å®ï¼ˆè¡¥å……ï¼‰
            behavior_results = await self._process_behavior_facts(user_ids, batch_size)
            
            # 4. ç”Ÿæˆæ—¶é—´è¡Œä¸ºèšåˆ
            time_behavior_results = await self._process_time_behavior_facts(user_ids)
            
            # 5. æ›´æ–°ç”¨æˆ·ç”»åƒå¿«ç…§
            snapshot_results = await self._update_profile_snapshots(user_ids)
            
            pipeline_results = {
                "batch_id": self.batch_id,
                "processed_at": datetime.now().isoformat(),
                "interaction_facts": interaction_results,
                "behavior_facts": behavior_results,
                "time_behavior_facts": time_behavior_results,
                "profile_snapshots": snapshot_results,
                "success": True
            }
            
            logger.info(f"âœ… ETL 2.0 Pipeline completed successfully")
            return pipeline_results
            
        except Exception as e:
            logger.error(f"âŒ ETL 2.0 Pipeline failed: {e}")
            raise
    
    async def _refresh_dimensions(self):
        """åˆ·æ–°ç»´åº¦è¡¨æ•°æ®"""
        try:
            logger.info("ğŸ”„ Refreshing dimension tables...")
            
            # åˆ·æ–°ç”¨æˆ·ç»´åº¦ï¼ˆSCD Type 2ï¼‰
            await self._refresh_user_dimension()
            
            # åˆ·æ–°ä¼šè¯ç»´åº¦
            await self._refresh_session_dimension()
            
            logger.info("âœ… Dimension tables refreshed")
            
        except Exception as e:
            logger.error(f"âŒ Failed to refresh dimensions: {e}")
            raise
    
    async def _refresh_user_dimension(self):
        """åˆ·æ–°ç”¨æˆ·ç»´åº¦è¡¨ - å¤ç”¨user360_etlçš„ç”¨æˆ·æ•°æ®è·å–é€»è¾‘"""
        try:
            # è·å–éœ€è¦æ›´æ–°çš„ç”¨æˆ·ï¼ˆæ–°ç”¨æˆ·æˆ–ä¿¡æ¯å˜æ›´çš„ç”¨æˆ·ï¼‰
            users_response = self.db_client.table('users').select('*').execute()
            
            if not users_response.data:
                return
            
            for user_data in users_response.data:
                user_id = user_data['user_id']
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å½“å‰è®°å½•
                existing_response = self.db_client.table('user_dimension')\
                    .select('*')\
                    .eq('user_id', user_id)\
                    .eq('is_current', True)\
                    .execute()
                
                needs_update = True
                if existing_response.data:
                    existing = existing_response.data[0]
                    # ç®€å•æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦å˜åŒ–
                    if (existing.get('email') == user_data.get('email') and
                        existing.get('subscription_tier') == user_data.get('subscription_status')):
                        needs_update = False
                
                if needs_update:
                    # å¤ç”¨user360_etlçš„å†…å®¹åˆ†æé€»è¾‘æ¥æ¨æ–­ç”¨æˆ·åˆ†ç±»
                    user_segment, user_persona = await self._infer_user_classification(user_id)
                    
                    # åˆ›å»ºæ–°çš„ç”¨æˆ·ç»´åº¦è®°å½•
                    user_dim_record = {
                        "user_id": user_id,
                        "email": user_data.get("email"),
                        "username": user_data.get("name"),
                        "organization_id": user_data.get("organization_id"),
                        "subscription_tier": user_data.get("subscription_status", "free"),
                        "account_status": "active" if user_data.get("is_active") else "inactive",
                        "registration_date": user_data.get("created_at"),
                        "timezone": "UTC",  # é»˜è®¤å€¼ï¼Œå®é™…ä¸­å¯ä»ç”¨æˆ·è®¾ç½®è·å–
                        "user_segment": user_segment,
                        "user_persona": user_persona,
                        "effective_date": datetime.now().date(),
                        "is_current": True,
                        "etl_batch_id": self.batch_id
                    }
                    
                    # å¦‚æœæœ‰ç°æœ‰è®°å½•ï¼Œå…ˆè®¾ç½®ä¸ºå†å²è®°å½•
                    if existing_response.data:
                        await self._expire_user_dimension(user_id)
                    
                    # æ’å…¥æ–°è®°å½•
                    self.db_client.table('user_dimension').insert(user_dim_record).execute()
                    
                    logger.info(f"ğŸ“Š Updated user dimension for: {user_id}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to refresh user dimension: {e}")
            raise
    
    async def _infer_user_classification(self, user_id: str) -> Tuple[str, str]:
        """æ¨æ–­ç”¨æˆ·åˆ†ç±» - å¤ç”¨user360_etlçš„å†…å®¹åˆ†æ"""
        try:
            # å¤ç”¨ç°æœ‰çš„å†…å®¹æ”¶é›†é€»è¾‘
            from .user360_etl import user360_etl
            content_text = await user360_etl._collect_user_text_content(user_id, limit=2000)
            
            if len(content_text) < 100:
                return "new_user", "general"
            
            # å¤ç”¨ç°æœ‰çš„å†…å®¹åˆ†æé€»è¾‘
            content_insights = await user360_etl._analyze_content_with_lang_extractor(content_text)
            
            # åŸºäºåˆ†æç»“æœæ¨æ–­åˆ†ç±»
            programming_languages = content_insights.get('programming_languages', {})
            primary_use_cases = content_insights.get('primary_use_cases', [])
            domain_expertise = content_insights.get('domain_expertise', {})
            
            # æ¨æ–­ç”¨æˆ·è§’è‰²
            user_persona = "general"
            if 'data_analysis' in primary_use_cases or 'machine_learning' in primary_use_cases:
                user_persona = "data_scientist"
            elif 'web_development' in primary_use_cases:
                user_persona = "developer"
            elif len(programming_languages) >= 3:
                user_persona = "developer"
            elif 'programming' in domain_expertise:
                if domain_expertise['programming'] in ['advanced', 'expert']:
                    user_persona = "developer"
                else:
                    user_persona = "learner"
            
            # æ¨æ–­ç”¨æˆ·æ´»è·ƒåº¦åˆ†æ®µ
            total_tech_items = len(programming_languages) + len(primary_use_cases)
            if total_tech_items >= 5:
                user_segment = "power_user"
            elif total_tech_items >= 2:
                user_segment = "regular_user"
            else:
                user_segment = "casual_user"
            
            return user_segment, user_persona
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to infer user classification for {user_id}: {e}")
            return "unknown", "general"
    
    async def _expire_user_dimension(self, user_id: str):
        """è®¾ç½®ç”¨æˆ·ç»´åº¦è®°å½•ä¸ºå†å²è®°å½•ï¼ˆSCD Type 2ï¼‰"""
        try:
            self.db_client.table('user_dimension')\
                .update({
                    "is_current": False,
                    "expiry_date": datetime.now().date()
                })\
                .eq('user_id', user_id)\
                .eq('is_current', True)\
                .execute()
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to expire user dimension for {user_id}: {e}")
    
    async def _refresh_session_dimension(self):
        """åˆ·æ–°ä¼šè¯ç»´åº¦è¡¨"""
        try:
            # è·å–æœ€è¿‘çš„ä¼šè¯æ•°æ®
            cutoff_date = datetime.now() - timedelta(days=7)
            sessions_response = self.db_client.table('sessions')\
                .select('*')\
                .gte('created_at', cutoff_date.isoformat())\
                .execute()
            
            if not sessions_response.data:
                return
            
            for session_data in sessions_response.data:
                session_id = session_data['id']
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing_response = self.db_client.table('session_dimension')\
                    .select('session_key')\
                    .eq('session_id', session_id)\
                    .execute()
                
                if not existing_response.data:
                    # å¤ç”¨user360_etlçš„é€»è¾‘æ¨æ–­ä¼šè¯ç±»å‹
                    session_type = await self._infer_session_type(session_id)
                    
                    session_dim_record = {
                        "session_id": session_id,
                        "session_start_time": session_data.get("created_at"),
                        "session_end_time": session_data.get("updated_at"),
                        "session_duration_minutes": session_data.get("duration", 0),
                        "session_type": session_type,
                        "device_type": "web",  # é»˜è®¤å€¼
                        "browser_type": "unknown",  # éœ€è¦ä»user_eventsè·å–
                        "interaction_count": 0,  # ç¨åè®¡ç®—
                        "etl_batch_id": self.batch_id
                    }
                    
                    self.db_client.table('session_dimension').insert(session_dim_record).execute()
                    
        except Exception as e:
            logger.error(f"âŒ Failed to refresh session dimension: {e}")
            raise
    
    async def _infer_session_type(self, session_id: str) -> str:
        """æ¨æ–­ä¼šè¯ç±»å‹ - åŸºäºæ¶ˆæ¯å†…å®¹"""
        try:
            # è·å–ä¼šè¯æ¶ˆæ¯ç¤ºä¾‹
            messages_response = self.db_client.table('session_messages')\
                .select('content')\
                .eq('session_id', session_id)\
                .limit(5)\
                .execute()
            
            if not messages_response.data:
                return "unknown"
            
            # ç®€å•çš„å…³é”®è¯åˆ†æ
            all_content = ' '.join([msg['content'] for msg in messages_response.data if msg.get('content')])
            content_lower = all_content.lower()
            
            if any(keyword in content_lower for keyword in ['code', 'function', 'debug', 'error']):
                return "coding"
            elif any(keyword in content_lower for keyword in ['learn', 'explain', 'how to', 'what is']):
                return "learning"
            elif any(keyword in content_lower for keyword in ['data', 'analysis', 'model', 'predict']):
                return "data_analysis"
            else:
                return "general"
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to infer session type for {session_id}: {e}")
            return "unknown"
    
    async def _process_interaction_facts(self, user_ids: List[str] = None, batch_size: int = 50) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·äº¤äº’äº‹å®è¡¨ - æ ¸å¿ƒè¡¨ï¼ŒåŸºäºsession_messageså’Œmemory"""
        try:
            logger.info("ğŸ”„ Processing interaction facts (core)...")
            
            processed_count = 0
            success_count = 0
            
            # è·å–å¾…å¤„ç†çš„ç”¨æˆ·
            if not user_ids:
                user_ids = await self._get_active_users()
            
            for user_id in user_ids[:batch_size]:
                try:
                    # å¤ç”¨user360_etlçš„æ•°æ®æ”¶é›†é€»è¾‘
                    from .user360_etl import user360_etl
                    
                    # 1. æ”¶é›†ç”¨æˆ·å†…å®¹
                    content_text = await user360_etl._collect_user_text_content(user_id)
                    
                    if len(content_text) < 50:
                        continue
                    
                    # 2. ä½¿ç”¨ç°æœ‰çš„å†…å®¹åˆ†æ
                    content_insights = await user360_etl._analyze_content_with_lang_extractor(content_text)
                    
                    # 3. è·å–ç›¸å…³ç»´åº¦é”®
                    user_key = await self._get_user_key(user_id)
                    time_key = int(datetime.now().strftime('%Y%m%d%H%M'))
                    
                    # 4. ä»session_messagesåˆ›å»ºäº¤äº’äº‹å®è®°å½•
                    await self._create_interaction_facts_from_messages(
                        user_id, user_key, time_key, content_insights
                    )
                    
                    # 5. ä»memoryåˆ›å»ºäº¤äº’äº‹å®è®°å½•
                    await self._create_interaction_facts_from_memory(
                        user_id, user_key, time_key, content_insights
                    )
                    
                    success_count += 1
                    logger.info(f"âœ… Processed interaction facts for user: {user_id}")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to process interaction facts for {user_id}: {e}")
                
                processed_count += 1
            
            return {
                "processed_users": processed_count,
                "successful_users": success_count,
                "table": "user_interaction_facts"
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed to process interaction facts: {e}")
            raise
    
    async def _create_interaction_facts_from_messages(self, user_id: str, user_key: int, time_key: int, content_insights: Dict):
        """ä»session_messagesåˆ›å»ºäº¤äº’äº‹å®è®°å½•"""
        try:
            # è·å–æœ€è¿‘çš„æ¶ˆæ¯
            cutoff_date = datetime.now() - timedelta(hours=24)  # åªå¤„ç†æœ€è¿‘24å°æ—¶çš„æ•°æ®
            messages_response = self.db_client.table('session_messages')\
                .select('id, session_id, content, created_at, message_type')\
                .eq('user_id', user_id)\
                .gte('created_at', cutoff_date.isoformat())\
                .limit(20)\
                .execute()
            
            if not messages_response.data:
                return
            
            for message in messages_response.data:
                if not message.get('content') or len(message['content']) < 10:
                    continue
                
                # è·å–session_key
                session_key = await self._get_session_key(message['session_id'])
                
                # æ¨æ–­éœ€æ±‚ç±»åˆ«
                need_category = self._infer_need_category(message['content'], content_insights)
                
                interaction_fact = {
                    "user_key": user_key,
                    "time_key": time_key,
                    "session_key": session_key,
                    "content_type_key": 1,  # é»˜è®¤å†…å®¹ç±»å‹
                    "source_id": message['id'],
                    "source_type": "session_message",
                    "content_length": len(message['content']),
                    "content_summary": message['content'][:500],
                    "need_category": need_category,
                    "primary_domain": content_insights.get('knowledge_domains', ['general'])[0] if content_insights.get('knowledge_domains') else 'general',
                    "interaction_quality_score": 0.8,  # åŸºäºæ¶ˆæ¯ç±»å‹å’Œé•¿åº¦çš„ç®€å•è¯„åˆ†
                    "etl_batch_id": self.batch_id
                }
                
                # æ’å…¥äº‹å®è¡¨
                self.db_client.table('user_interaction_facts').insert(interaction_fact).execute()
                
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to create interaction facts from messages for {user_id}: {e}")
    
    async def _create_interaction_facts_from_memory(self, user_id: str, user_key: int, time_key: int, content_insights: Dict):
        """ä»memoryè¡¨åˆ›å»ºäº¤äº’äº‹å®è®°å½•"""
        try:
            # è·å–å„ç±»memoryæ•°æ® - å¤ç”¨user360_etlçš„é€»è¾‘
            memory_tables = ['factual_memories', 'episodic_memories', 'semantic_memories']
            
            for table_name in memory_tables:
                try:
                    response = self.db_client.table(table_name)\
                        .select('*')\
                        .eq('user_id', user_id)\
                        .limit(10)\
                        .execute()
                    
                    if not response.data:
                        continue
                    
                    for memory in response.data:
                        # æ„é€ å†…å®¹æ‘˜è¦
                        content_summary = self._extract_memory_content(memory, table_name)
                        
                        if len(content_summary) < 10:
                            continue
                        
                        memory_fact = {
                            "user_key": user_key,
                            "time_key": time_key,
                            "session_key": -1,  # memoryæ²¡æœ‰session
                            "content_type_key": 2,  # memoryå†…å®¹ç±»å‹
                            "source_id": str(memory['id']),
                            "source_type": table_name,
                            "content_length": len(content_summary),
                            "content_summary": content_summary[:500],
                            "need_category": "knowledge_building",
                            "primary_domain": content_insights.get('knowledge_domains', ['general'])[0] if content_insights.get('knowledge_domains') else 'general',
                            "domain_confidence_score": memory.get('confidence_score', 0.5),
                            "etl_batch_id": self.batch_id
                        }
                        
                        self.db_client.table('user_interaction_facts').insert(memory_fact).execute()
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to process {table_name} for {user_id}: {e}")
                    continue
                    
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to create interaction facts from memory for {user_id}: {e}")
    
    def _extract_memory_content(self, memory: Dict, table_name: str) -> str:
        """ä»memoryè®°å½•ä¸­æå–å†…å®¹æ‘˜è¦"""
        if table_name == 'factual_memories':
            return f"{memory.get('subject', '')} {memory.get('predicate', '')} {memory.get('object_value', '')}"
        elif table_name == 'episodic_memories':
            return f"{memory.get('episode_title', '')}: {memory.get('summary', '')}"
        elif table_name == 'semantic_memories':
            return f"{memory.get('concept_name', '')} - {memory.get('definition', '')}"
        else:
            return str(memory)[:200]
    
    def _infer_need_category(self, content: str, content_insights: Dict) -> str:
        """æ¨æ–­ç”¨æˆ·éœ€æ±‚ç±»åˆ«"""
        content_lower = content.lower()
        
        if any(keyword in content_lower for keyword in ['help', 'how to', 'explain']):
            return 'learning'
        elif any(keyword in content_lower for keyword in ['error', 'bug', 'fix', 'debug']):
            return 'problem_solving'
        elif any(keyword in content_lower for keyword in ['create', 'build', 'develop']):
            return 'development'
        elif content_insights.get('primary_use_cases'):
            use_case = content_insights['primary_use_cases'][0]
            if use_case == 'data_analysis':
                return 'data_analysis'
            elif use_case == 'machine_learning':
                return 'ml_modeling'
        
        return 'general'
    
    async def _process_behavior_facts(self, user_ids: List[str] = None, batch_size: int = 50) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·è¡Œä¸ºäº‹å®è¡¨ - è¡¥å……è¡¨ï¼ŒåŸºäºuser_events"""
        try:
            logger.info("ğŸ”„ Processing behavior facts (supplemental)...")
            
            processed_count = 0
            success_count = 0
            
            if not user_ids:
                user_ids = await self._get_active_users()
            
            for user_id in user_ids[:batch_size]:
                try:
                    # è·å–æœ€è¿‘çš„ç”¨æˆ·äº‹ä»¶
                    cutoff_date = datetime.now() - timedelta(hours=24)
                    events_response = self.db_client.table('user_events')\
                        .select('*')\
                        .eq('user_id', user_id)\
                        .gte('timestamp', cutoff_date.isoformat())\
                        .limit(50)\
                        .execute()
                    
                    if not events_response.data:
                        continue
                    
                    user_key = await self._get_user_key(user_id)
                    
                    for event in events_response.data:
                        # åˆ›å»ºè¡Œä¸ºäº‹å®è®°å½•
                        time_key = int(datetime.fromisoformat(event['timestamp']).strftime('%Y%m%d%H%M'))
                        session_key = await self._get_session_key(event.get('session_id')) if event.get('session_id') else -1
                        
                        behavior_fact = {
                            "user_key": user_key,
                            "time_key": time_key,
                            "session_key": session_key,
                            "event_type_key": 1,  # é»˜è®¤äº‹ä»¶ç±»å‹
                            "event_id": str(event['id']),
                            "event_name": event['event_name'],
                            "page_path": event.get('properties', {}).get('page_path', ''),
                            "feature_used": event.get('properties', {}).get('feature_used', ''),
                            "event_timestamp": event['timestamp'],
                            "etl_batch_id": self.batch_id
                        }
                        
                        self.db_client.table('user_behavior_facts').insert(behavior_fact).execute()
                    
                    success_count += 1
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to process behavior facts for {user_id}: {e}")
                
                processed_count += 1
            
            return {
                "processed_users": processed_count,
                "successful_users": success_count,
                "table": "user_behavior_facts"
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed to process behavior facts: {e}")
            raise
    
    async def _process_time_behavior_facts(self, user_ids: List[str] = None) -> Dict[str, Any]:
        """å¤„ç†æ—¶é—´è¡Œä¸ºèšåˆäº‹å®è¡¨ - åˆ†æç”¨æˆ·ä»€ä¹ˆæ—¶é—´åšä»€ä¹ˆ"""
        try:
            logger.info("ğŸ”„ Processing time behavior aggregation...")
            
            processed_count = 0
            success_count = 0
            
            if not user_ids:
                user_ids = await self._get_active_users()
            
            for user_id in user_ids:
                try:
                    user_key = await self._get_user_key(user_id)
                    
                    # å¤ç”¨user360_etlçš„æ—¶é—´è¡Œä¸ºåˆ†æé€»è¾‘
                    from .user360_etl import user360_etl
                    time_stats = await user360_etl._extract_time_behavior_stats(user_id)
                    
                    # ä¸ºæ¯ä¸ªæ´»è·ƒå°æ—¶åˆ›å»ºæ—¶é—´è¡Œä¸ºè®°å½•
                    peak_hours = time_stats.get('peak_usage_hours', [])
                    
                    for hour in peak_hours:
                        time_period_key = hour + 1  # ç®€å•æ˜ å°„åˆ°time_period_dimension
                        
                        # åˆ†æè¯¥æ—¶é—´æ®µçš„ä¸»è¦æ´»åŠ¨
                        dominant_activity = await self._analyze_hourly_activity(user_id, hour)
                        
                        time_behavior_fact = {
                            "user_key": user_key,
                            "time_period_key": time_period_key,
                            "behavior_pattern_key": 1,  # é»˜è®¤æ¨¡å¼
                            "analysis_date": datetime.now().date(),
                            "hour_of_day": hour,
                            "dominant_need_category": "general",
                            "dominant_activity": dominant_activity,
                            "total_interactions": time_stats.get('sessions_last_7_days', 0),
                            "productivity_score": 0.7,  # åŸºäºpeak hourçš„å‡è®¾è¯„åˆ†
                            "etl_batch_id": self.batch_id
                        }
                        
                        self.db_client.table('user_time_behavior_facts').insert(time_behavior_fact).execute()
                    
                    success_count += 1
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to process time behavior facts for {user_id}: {e}")
                
                processed_count += 1
            
            return {
                "processed_users": processed_count,
                "successful_users": success_count,
                "table": "user_time_behavior_facts"
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed to process time behavior facts: {e}")
            raise
    
    async def _analyze_hourly_activity(self, user_id: str, hour: int) -> str:
        """åˆ†æç”¨æˆ·åœ¨ç‰¹å®šå°æ—¶çš„ä¸»è¦æ´»åŠ¨"""
        try:
            # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºè¯¥æ—¶é—´æ®µçš„äº‹ä»¶ç±»å‹åˆ†æ
            events_response = self.db_client.table('user_events')\
                .select('event_name, event_category')\
                .eq('user_id', user_id)\
                .execute()
            
            if not events_response.data:
                return "general"
            
            # ç»Ÿè®¡äº‹ä»¶ç±»å‹
            event_counts = {}
            for event in events_response.data:
                event_name = event.get('event_name', 'unknown')
                event_counts[event_name] = event_counts.get(event_name, 0) + 1
            
            # è¿”å›æœ€é¢‘ç¹çš„æ´»åŠ¨
            if event_counts:
                dominant = max(event_counts.items(), key=lambda x: x[1])[0]
                return dominant
            
            return "general"
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to analyze hourly activity for {user_id} at {hour}: {e}")
            return "general"
    
    async def _update_profile_snapshots(self, user_ids: List[str] = None) -> Dict[str, Any]:
        """æ›´æ–°ç”¨æˆ·ç”»åƒå¿«ç…§ - ä¸ºpersonaç”Ÿæˆå‡†å¤‡æ•°æ®"""
        try:
            logger.info("ğŸ”„ Updating user profile snapshots...")
            
            processed_count = 0
            success_count = 0
            
            if not user_ids:
                user_ids = await self._get_active_users()
            
            for user_id in user_ids:
                try:
                    user_key = await self._get_user_key(user_id)
                    
                    # å¤ç”¨user360_etlçš„å®Œæ•´åˆ†æèƒ½åŠ›
                    from .user360_etl import user360_etl
                    user_profile = await user360_etl.process_user(user_id, force_refresh=True)
                    
                    if user_profile.get('processed'):
                        record = user_profile['record']
                        
                        # åˆ›å»ºç”»åƒå¿«ç…§
                        snapshot_record = {
                            "user_key": user_key,
                            "snapshot_date_key": int(datetime.now().strftime('%Y%m%d')),
                            "effective_date": datetime.now().date(),
                            "is_current": True,
                            
                            # æŠ€èƒ½è¯„ä¼°
                            "technical_skill_level": self._assess_skill_level(record),
                            "domain_expertise_scores": record.get('domain_expertise', {}),
                            "problem_solving_maturity": "intermediate",  # åŸºäºäº¤äº’å¤æ‚åº¦
                            
                            # è¡Œä¸ºç‰¹å¾
                            "usage_intensity": self._assess_usage_intensity(record),
                            "preferred_interaction_style": record.get('communication_style', 'conversational'),
                            "tool_proficiency_scores": record.get('frameworks_and_tools', {}),
                            
                            # æ—¶é—´æ¨¡å¼
                            "peak_productivity_hours": record.get('peak_usage_hours', []),
                            "work_pattern_type": self._infer_work_pattern(record),
                            
                            # é¢„æµ‹ç‰¹å¾
                            "user_lifecycle_stage": "active",  # åŸºäºæœ€è¿‘æ´»åŠ¨
                            "engagement_trend": "stable",
                            "churn_risk_score": 0.1,  # ä½é£é™©ï¼ˆæ´»è·ƒç”¨æˆ·ï¼‰
                            "personalization_readiness": record.get('data_completeness_score', 0.5),
                            
                            # å…ƒæ•°æ®
                            "data_quality_score": record.get('data_completeness_score', 0.5),
                            "completeness_percentage": record.get('data_completeness_score', 0.5) * 100,
                            "confidence_level": 0.8,
                            "etl_batch_id": self.batch_id
                        }
                        
                        # å…ˆå¤±æ•ˆç°æœ‰å¿«ç…§
                        self.db_client.table('user_profile_snapshots')\
                            .update({"is_current": False, "expiry_date": datetime.now().date()})\
                            .eq('user_key', user_key)\
                            .eq('is_current', True)\
                            .execute()
                        
                        # æ’å…¥æ–°å¿«ç…§
                        self.db_client.table('user_profile_snapshots').insert(snapshot_record).execute()
                        
                        success_count += 1
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to update profile snapshot for {user_id}: {e}")
                
                processed_count += 1
            
            return {
                "processed_users": processed_count,
                "successful_users": success_count,
                "table": "user_profile_snapshots"
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed to update profile snapshots: {e}")
            raise
    
    def _assess_skill_level(self, record: Dict) -> str:
        """è¯„ä¼°æŠ€æœ¯æŠ€èƒ½æ°´å¹³"""
        programming_languages = record.get('programming_languages', {})
        frameworks_tools = record.get('frameworks_and_tools', {})
        
        total_items = len(programming_languages) + len(frameworks_tools)
        avg_confidence = 0
        
        if programming_languages:
            avg_confidence = sum(programming_languages.values()) / len(programming_languages)
        
        if total_items >= 5 and avg_confidence > 0.7:
            return "expert"
        elif total_items >= 3 and avg_confidence > 0.5:
            return "advanced"
        elif total_items >= 1:
            return "intermediate"
        else:
            return "beginner"
    
    def _assess_usage_intensity(self, record: Dict) -> str:
        """è¯„ä¼°ä½¿ç”¨å¼ºåº¦"""
        total_sessions = record.get('total_sessions', 0)
        sessions_7d = record.get('sessions_last_7_days', 0)
        
        if sessions_7d >= 5:
            return "heavy"
        elif sessions_7d >= 2:
            return "moderate"
        elif total_sessions > 10:
            return "light"
        else:
            return "minimal"
    
    def _infer_work_pattern(self, record: Dict) -> str:
        """æ¨æ–­å·¥ä½œæ¨¡å¼"""
        peak_hours = record.get('peak_usage_hours', [])
        
        if not peak_hours:
            return "flexible"
        
        morning_hours = [h for h in peak_hours if 6 <= h <= 10]
        evening_hours = [h for h in peak_hours if 18 <= h <= 23]
        
        if len(morning_hours) >= 2:
            return "morning_person"
        elif len(evening_hours) >= 2:
            return "night_owl"
        else:
            return "flexible"
    
    # è¾…åŠ©æ–¹æ³•
    async def _get_active_users(self, limit: int = 100) -> List[str]:
        """è·å–æ´»è·ƒç”¨æˆ·åˆ—è¡¨"""
        try:
            cutoff_date = datetime.now() - timedelta(days=30)
            response = self.db_client.table('sessions')\
                .select('user_id')\
                .gte('created_at', cutoff_date.isoformat())\
                .limit(limit)\
                .execute()
            
            if response.data:
                return list(set([session['user_id'] for session in response.data]))
            return []
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to get active users: {e}")
            return []
    
    async def _get_user_key(self, user_id: str) -> int:
        """è·å–ç”¨æˆ·ç»´åº¦é”®"""
        try:
            response = self.db_client.table('user_dimension')\
                .select('user_key')\
                .eq('user_id', user_id)\
                .eq('is_current', True)\
                .single()\
                .execute()
            
            if response.data:
                return response.data['user_key']
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›é»˜è®¤å€¼æˆ–åˆ›å»ºæ–°è®°å½•
                return -1
                
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to get user_key for {user_id}: {e}")
            return -1
    
    async def _get_session_key(self, session_id: str) -> int:
        """è·å–ä¼šè¯ç»´åº¦é”®"""
        try:
            response = self.db_client.table('session_dimension')\
                .select('session_key')\
                .eq('session_id', session_id)\
                .single()\
                .execute()
            
            if response.data:
                return response.data['session_key']
            else:
                return -1
                
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to get session_key for {session_id}: {e}")
            return -1

# å…¨å±€å®ä¾‹
etl_2_processor = ETL2Processor()

# ä¾¿æ·å‡½æ•°
async def run_etl_2_pipeline(user_ids: List[str] = None, batch_size: int = 50) -> Dict[str, Any]:
    """è¿è¡ŒETL 2.0å®Œæ•´ç®¡é“"""
    return await etl_2_processor.run_full_pipeline(user_ids, batch_size)