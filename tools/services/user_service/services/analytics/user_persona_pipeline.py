#!/usr/bin/env python3
"""
ç”¨æˆ·ç”»åƒç”Ÿæˆç®¡é“æœåŠ¡ - å®Œæ•´çš„Gold Data -> ML -> Personaç”Ÿæˆæµç¨‹
åè°ƒETL 2.0å¤„ç†å™¨ + MLåˆ†æå™¨ + Personaç”Ÿæˆå™¨
"""

import asyncio
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from core.logging import get_logger
from core.database import get_supabase_client

# å¯¼å…¥å­æ¨¡å—æœåŠ¡
from .etl.etl_2_0_processor import etl_2_processor, run_etl_2_pipeline
from .ml.user_behavior_ml_analyzer import user_behavior_ml_analyzer, analyze_user_ml_behavior
from .persona.persona_generator import persona_generator, generate_user_persona

logger = get_logger(__name__)

class UserPersonaPipeline:
    """
    ç”¨æˆ·ç”»åƒç”Ÿæˆç®¡é“æœåŠ¡
    
    å®Œæ•´æµç¨‹ï¼š
    1. ETL 2.0 æ•°æ®å¤„ç† (Gold Dataç”Ÿæˆ)
    2. MLè¡Œä¸ºåˆ†æ (æ—¶é—´è¡Œä¸ºæ¨¡å¼åˆ†æ)
    3. AI Personaç”Ÿæˆ (åŸºäºGold Data + MLç‰¹å¾)
    4. pgvectorå­˜å‚¨å’Œç´¢å¼•
    """
    
    def __init__(self):
        self.db_client = get_supabase_client()
        self.pipeline_id = f"persona_pipeline_{int(datetime.now().timestamp())}"
        
    async def run_complete_persona_pipeline(self, user_ids: List[str] = None, 
                                          batch_size: int = 10) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ç”¨æˆ·ç”»åƒç”Ÿæˆç®¡é“"""
        try:
            logger.info(f"ğŸš€ Starting Complete User Persona Pipeline - ID: {self.pipeline_id}")
            
            if not user_ids:
                user_ids = await self._get_active_users_for_persona_generation()
                
            if not user_ids:
                return {
                    "pipeline_id": self.pipeline_id,
                    "success": False,
                    "message": "No active users found for persona generation"
                }
            
            logger.info(f"ğŸ“Š Processing {len(user_ids)} users (batch size: {batch_size})")
            
            # Stage 1: ETL 2.0 æ•°æ®å¤„ç† (Gold Data)
            logger.info("ğŸ—ï¸ Stage 1: ETL 2.0 Data Processing (Gold Data Generation)")
            etl_results = await self._run_etl_stage(user_ids, batch_size)
            
            # Stage 2: MLè¡Œä¸ºåˆ†æ
            logger.info("ğŸ§  Stage 2: ML Behavior Analysis")
            ml_results = await self._run_ml_analysis_stage(user_ids[:batch_size])
            
            # Stage 3: AI Personaç”Ÿæˆ
            logger.info("ğŸ­ Stage 3: AI Persona Generation")
            persona_results = await self._run_persona_generation_stage(user_ids[:batch_size], ml_results)
            
            # æ±‡æ€»ç»“æœ
            pipeline_results = {
                "pipeline_id": self.pipeline_id,
                "completed_at": datetime.now().isoformat(),
                "total_users_processed": len(user_ids),
                "batch_size": batch_size,
                "success": True,
                
                # å„é˜¶æ®µç»“æœ
                "etl_stage": etl_results,
                "ml_analysis_stage": ml_results,
                "persona_generation_stage": persona_results,
                
                # æ±‡æ€»ç»Ÿè®¡
                "summary": {
                    "etl_successful_users": etl_results.get("successful_users", 0),
                    "ml_analyzed_users": len([r for r in ml_results.values() if r.get("success", False)]),
                    "personas_generated": len([r for r in persona_results.values() if r.get("success", False)]),
                    "total_processing_time_seconds": self._calculate_processing_time()
                }
            }
            
            logger.info(f"âœ… Complete User Persona Pipeline completed successfully")
            logger.info(f"ğŸ“ˆ Generated {pipeline_results['summary']['personas_generated']} personas")
            
            return pipeline_results
            
        except Exception as e:
            logger.error(f"âŒ User Persona Pipeline failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "pipeline_id": self.pipeline_id,
                "success": False,
                "error": str(e),
                "completed_at": datetime.now().isoformat()
            }
    
    async def _run_etl_stage(self, user_ids: List[str], batch_size: int) -> Dict[str, Any]:
        """è¿è¡ŒETL 2.0é˜¶æ®µ - ç”ŸæˆGold Data"""
        try:
            logger.info("ğŸ”„ Running ETL 2.0 Processor...")
            etl_results = await run_etl_2_pipeline(user_ids, batch_size)
            
            logger.info(f"âœ… ETL Stage completed - {etl_results.get('interaction_facts', {}).get('successful_users', 0)} users processed")
            return etl_results
            
        except Exception as e:
            logger.error(f"âŒ ETL Stage failed: {e}")
            return {"success": False, "error": str(e)}
    
    async def _run_ml_analysis_stage(self, user_ids: List[str]) -> Dict[str, Dict[str, Any]]:
        """è¿è¡ŒMLè¡Œä¸ºåˆ†æé˜¶æ®µ"""
        try:
            logger.info(f"ğŸ”„ Running ML Behavior Analysis for {len(user_ids)} users...")
            
            ml_results = {}
            
            # å¹¶è¡Œå¤„ç†å¤šä¸ªç”¨æˆ·çš„MLåˆ†æ
            analysis_tasks = []
            for user_id in user_ids:
                task = self._analyze_single_user_ml_behavior(user_id)
                analysis_tasks.append(task)
            
            # æ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
            results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                user_id = user_ids[i]
                if isinstance(result, Exception):
                    logger.error(f"âŒ ML Analysis failed for {user_id}: {result}")
                    ml_results[user_id] = {"success": False, "error": str(result)}
                else:
                    ml_results[user_id] = {"success": True, "analysis": result}
                    logger.info(f"âœ… ML Analysis completed for {user_id}")
            
            successful_analyses = len([r for r in ml_results.values() if r.get("success", False)])
            logger.info(f"âœ… ML Analysis Stage completed - {successful_analyses}/{len(user_ids)} users analyzed")
            
            return ml_results
            
        except Exception as e:
            logger.error(f"âŒ ML Analysis Stage failed: {e}")
            return {}
    
    async def _run_persona_generation_stage(self, user_ids: List[str], 
                                          ml_results: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """è¿è¡ŒAI Personaç”Ÿæˆé˜¶æ®µ"""
        try:
            logger.info(f"ğŸ”„ Running AI Persona Generation for {len(user_ids)} users...")
            
            persona_results = {}
            
            # å¹¶è¡Œå¤„ç†å¤šä¸ªç”¨æˆ·çš„personaç”Ÿæˆ
            generation_tasks = []
            for user_id in user_ids:
                # è·å–è¯¥ç”¨æˆ·çš„MLåˆ†æç»“æœ
                user_ml_result = ml_results.get(user_id, {})
                if user_ml_result.get("success"):
                    ml_analysis = user_ml_result["analysis"]
                    task = self._generate_single_user_persona(user_id, ml_analysis)
                    generation_tasks.append((user_id, task))
                else:
                    persona_results[user_id] = {
                        "success": False, 
                        "error": "ML analysis failed or unavailable"
                    }
            
            # æ‰§è¡Œpersonaç”Ÿæˆä»»åŠ¡
            if generation_tasks:
                tasks = [task for _, task in generation_tasks]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for i, result in enumerate(results):
                    user_id = generation_tasks[i][0]
                    if isinstance(result, Exception):
                        logger.error(f"âŒ Persona generation failed for {user_id}: {result}")
                        persona_results[user_id] = {"success": False, "error": str(result)}
                    else:
                        persona_results[user_id] = result
                        if result.get("success"):
                            logger.info(f"âœ… Persona generated for {user_id}")
            
            successful_personas = len([r for r in persona_results.values() if r.get("success", False)])
            logger.info(f"âœ… Persona Generation Stage completed - {successful_personas}/{len(user_ids)} personas generated")
            
            return persona_results
            
        except Exception as e:
            logger.error(f"âŒ Persona Generation Stage failed: {e}")
            return {}
    
    async def _analyze_single_user_ml_behavior(self, user_id: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªç”¨æˆ·çš„MLè¡Œä¸º"""
        try:
            return await analyze_user_ml_behavior(user_id)
        except Exception as e:
            logger.error(f"âŒ ML behavior analysis failed for {user_id}: {e}")
            raise
    
    async def _generate_single_user_persona(self, user_id: str, ml_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸ºå•ä¸ªç”¨æˆ·ç”Ÿæˆpersona"""
        try:
            # ä»MLåˆ†æç»“æœä¸­æå–ç‰¹å¾
            ml_features = ml_analysis.get("ml_user_features", {})
            
            # æ„å»ºGold Dataï¼ˆä»MLåˆ†æç»“æœä¸­æå–ï¼‰
            gold_data = {
                "technical_skills": self._extract_technical_skills_from_ml(ml_analysis),
                "knowledge_domains": self._extract_knowledge_domains_from_ml(ml_analysis),
                "key_insights": self._extract_key_insights_from_ml(ml_analysis),
                "content_length": ml_analysis.get("data_points", 0),
                "data_completeness_score": ml_analysis.get("analysis_quality", {}).get("data_completeness", 0.5)
            }
            
            # ç”Ÿæˆpersona
            return await persona_generator.generate_user_persona(user_id, ml_features, gold_data)
            
        except Exception as e:
            logger.error(f"âŒ Single user persona generation failed for {user_id}: {e}")
            raise
    
    def _extract_technical_skills_from_ml(self, ml_analysis: Dict[str, Any]) -> List[str]:
        """ä»MLåˆ†æç»“æœä¸­æå–æŠ€æœ¯æŠ€èƒ½"""
        try:
            skills = []
            
            # ä»activity patternsä¸­æ¨æ–­æŠ€èƒ½
            activity_patterns = ml_analysis.get("activity_patterns", {})
            content_patterns = activity_patterns.get("content_patterns", {})
            
            for hour_data in content_patterns.values():
                activity_scores = hour_data.get("activity_scores", {})
                if "coding" in activity_scores and activity_scores["coding"] > 2:
                    skills.extend(["python", "programming"])
                if "data_analysis" in activity_scores and activity_scores["data_analysis"] > 2:
                    skills.extend(["sql", "data_analysis"])
            
            return list(set(skills))  # å»é‡
            
        except Exception:
            return ["general_usage"]
    
    def _extract_knowledge_domains_from_ml(self, ml_analysis: Dict[str, Any]) -> List[str]:
        """ä»MLåˆ†æç»“æœä¸­æå–çŸ¥è¯†é¢†åŸŸ"""
        try:
            domains = []
            
            # åŸºäºdominant activityæ¨æ–­é¢†åŸŸ
            ml_features = ml_analysis.get("ml_user_features", {})
            dominant_activity = ml_features.get("dominant_activity_type")
            
            if dominant_activity == "coding":
                domains.append("Software Development")
            elif dominant_activity == "data_analysis":
                domains.append("Data Science")
            elif dominant_activity == "learning":
                domains.append("Education & Learning")
            else:
                domains.append("General Technology")
            
            return domains
            
        except Exception:
            return ["General Usage"]
    
    def _extract_key_insights_from_ml(self, ml_analysis: Dict[str, Any]) -> List[str]:
        """ä»MLåˆ†æç»“æœä¸­æå–å…³é”®æ´å¯Ÿ"""
        try:
            insights = []
            
            ml_features = ml_analysis.get("ml_user_features", {})
            
            # å·¥ä½œæ¨¡å¼æ´å¯Ÿ
            work_pattern = ml_features.get("primary_work_pattern", "")
            if work_pattern:
                insights.append(f"Primary work pattern: {work_pattern}")
            
            # ä½¿ç”¨å¼ºåº¦æ´å¯Ÿ
            usage_intensity = ml_features.get("usage_intensity", "")
            if usage_intensity:
                insights.append(f"Usage intensity: {usage_intensity}")
            
            # ä¸€è‡´æ€§æ´å¯Ÿ
            consistency = ml_features.get("behavior_consistency", "")
            if consistency:
                insights.append(f"Behavior consistency: {consistency}")
            
            return insights
            
        except Exception:
            return ["Basic usage patterns observed"]
    
    async def _get_active_users_for_persona_generation(self, limit: int = 50) -> List[str]:
        """è·å–éœ€è¦ç”Ÿæˆpersonaçš„æ´»è·ƒç”¨æˆ·"""
        try:
            # è·å–æœ€è¿‘æ´»è·ƒä½†è¿˜æ²¡æœ‰personaçš„ç”¨æˆ·
            response = self.db_client.table('sessions')\
                .select('user_id')\
                .gte('created_at', (datetime.now() - timedelta(days=7)).isoformat())\
                .limit(limit)\
                .execute()
            
            if not response.data:
                return []
            
            # è·å–å·²æœ‰personaçš„ç”¨æˆ·
            existing_personas_response = self.db_client.table('user_personas')\
                .select('user_id')\
                .execute()
            
            existing_user_ids = set()
            if existing_personas_response.data:
                existing_user_ids = {p['user_id'] for p in existing_personas_response.data}
            
            # è¿‡æ»¤å‡ºè¿˜æ²¡æœ‰personaçš„ç”¨æˆ·
            all_user_ids = list(set([session['user_id'] for session in response.data]))
            new_user_ids = [uid for uid in all_user_ids if uid not in existing_user_ids]
            
            logger.info(f"ğŸ“Š Found {len(new_user_ids)} users needing persona generation")
            return new_user_ids
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to get active users for persona generation: {e}")
            return []
    
    def _calculate_processing_time(self) -> float:
        """è®¡ç®—å¤„ç†æ—¶é—´"""
        # ç®€å•å®ç°ï¼Œå®é™…ä¸­å¯ä»¥è·Ÿè¸ªå¼€å§‹æ—¶é—´
        return 0.0
    
    async def run_single_user_persona_pipeline(self, user_id: str) -> Dict[str, Any]:
        """ä¸ºå•ä¸ªç”¨æˆ·è¿è¡Œå®Œæ•´çš„personaç”Ÿæˆç®¡é“"""
        try:
            logger.info(f"ğŸ¯ Running single user persona pipeline for: {user_id}")
            
            # 1. ETLå¤„ç†
            etl_result = await run_etl_2_pipeline([user_id], 1)
            
            # 2. MLåˆ†æ
            ml_result = await analyze_user_ml_behavior(user_id)
            
            # 3. Personaç”Ÿæˆ
            if not ml_result.get("error"):
                ml_features = ml_result.get("ml_user_features", {})
                gold_data = {
                    "technical_skills": self._extract_technical_skills_from_ml(ml_result),
                    "knowledge_domains": self._extract_knowledge_domains_from_ml(ml_result),
                    "key_insights": self._extract_key_insights_from_ml(ml_result),
                    "content_length": ml_result.get("data_points", 0),
                    "data_completeness_score": ml_result.get("analysis_quality", {}).get("data_completeness", 0.5)
                }
                
                persona_result = await persona_generator.generate_user_persona(user_id, ml_features, gold_data)
            else:
                persona_result = {"success": False, "error": "ML analysis failed"}
            
            return {
                "user_id": user_id,
                "pipeline_completed": True,
                "etl_success": etl_result.get("success", False),
                "ml_success": not bool(ml_result.get("error")),
                "persona_success": persona_result.get("success", False),
                "persona_result": persona_result
            }
            
        except Exception as e:
            logger.error(f"âŒ Single user persona pipeline failed for {user_id}: {e}")
            return {
                "user_id": user_id,
                "pipeline_completed": False,
                "error": str(e)
            }

# å…¨å±€å®ä¾‹
user_persona_pipeline = UserPersonaPipeline()

# ä¾¿æ·å‡½æ•°
async def run_complete_persona_pipeline(user_ids: List[str] = None, batch_size: int = 10) -> Dict[str, Any]:
    """è¿è¡Œå®Œæ•´çš„ç”¨æˆ·ç”»åƒç”Ÿæˆç®¡é“"""
    return await user_persona_pipeline.run_complete_persona_pipeline(user_ids, batch_size)

async def generate_single_user_persona(user_id: str) -> Dict[str, Any]:
    """ä¸ºå•ä¸ªç”¨æˆ·ç”Ÿæˆå®Œæ•´persona"""
    return await user_persona_pipeline.run_single_user_persona_pipeline(user_id)