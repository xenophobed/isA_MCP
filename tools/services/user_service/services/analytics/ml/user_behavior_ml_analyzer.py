#!/usr/bin/env python3
"""
ç”¨æˆ·è¡Œä¸ºMLåˆ†æå™¨ - é›†æˆç°æœ‰çš„MLæ¨¡å‹æœåŠ¡åˆ†æç”¨æˆ·æ—¶é—´è¡Œä¸ºæ¨¡å¼
Gold Data -> ML Analysis -> Structured Insights (ä¸ºPersonaç”Ÿæˆå‡†å¤‡)
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional, Tuple
import json

from core.logging import get_logger
from core.database import get_supabase_client
from tools.services.data_analytics_service.processors.data_processors.model.time_series_processor import TimeSeriesProcessor

logger = get_logger(__name__)

class UserBehaviorMLAnalyzer:
    """
    ç”¨æˆ·è¡Œä¸ºMLåˆ†æå™¨
    
    åŠŸèƒ½ï¼š
    1. ä»Gold Dataåˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
    2. ä½¿ç”¨ç°æœ‰çš„TimeSeriesProcessoråˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼
    3. ç”Ÿæˆç»“æ„åŒ–çš„è¡Œä¸ºæ´å¯Ÿ
    4. ä¸ºPersonaç”Ÿæˆæä¾›MLé©±åŠ¨çš„ç”¨æˆ·ç‰¹å¾
    """
    
    def __init__(self):
        self.db_client = get_supabase_client()
        self.ts_processor = None  # å»¶è¿Ÿåˆå§‹åŒ–
        
    async def analyze_user_behavior_patterns(self, user_id: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªç”¨æˆ·çš„è¡Œä¸ºæ¨¡å¼ - MLé©±åŠ¨çš„æ·±åº¦åˆ†æ"""
        try:
            logger.info(f"ğŸ§  Starting ML behavior analysis for user: {user_id}")
            
            # 1. ä»æ•°æ®åº“æ”¶é›†æ—¶é—´åºåˆ—æ•°æ®
            time_series_data = await self._collect_user_time_series_data(user_id)
            
            if not time_series_data or len(time_series_data) < 7:  # è‡³å°‘éœ€è¦7å¤©æ•°æ®
                logger.warning(f"âš ï¸ Insufficient time series data for user {user_id}")
                return self._generate_minimal_analysis()
            
            # 2. å‡†å¤‡MLåˆ†ææ•°æ®
            df = self._prepare_ml_dataframe(time_series_data)
            
            # 3. æ‰§è¡Œæ—¶é—´åºåˆ—MLåˆ†æ
            ml_results = await self._run_ml_time_series_analysis(df)
            
            # 4. åˆ†æç”¨æˆ·æ´»åŠ¨æ¨¡å¼ï¼ˆä»€ä¹ˆæ—¶é—´åšä»€ä¹ˆï¼‰
            activity_patterns = await self._analyze_time_activity_correlation(user_id, df)
            
            # 5. ç”ŸæˆMLé©±åŠ¨çš„ç”¨æˆ·ç‰¹å¾
            ml_features = self._generate_ml_user_features(ml_results, activity_patterns)
            
            # 6. ç»„è£…å®Œæ•´çš„MLåˆ†æç»“æœ
            analysis_result = {
                "user_id": user_id,
                "analysis_timestamp": datetime.now().isoformat(),
                "data_points": len(time_series_data),
                "analysis_period_days": self._calculate_analysis_period(time_series_data),
                
                # MLæ—¶é—´åºåˆ—åˆ†æç»“æœ
                "time_series_insights": ml_results,
                
                # æ—¶é—´-æ´»åŠ¨ç›¸å…³æ€§åˆ†æ
                "activity_patterns": activity_patterns,
                
                # MLç”Ÿæˆçš„ç”¨æˆ·ç‰¹å¾ï¼ˆç”¨äºPersonaï¼‰
                "ml_user_features": ml_features,
                
                # åˆ†æè´¨é‡æŒ‡æ ‡
                "analysis_quality": {
                    "data_completeness": min(1.0, len(time_series_data) / 30),
                    "pattern_confidence": ml_features.get("pattern_confidence", 0.5),
                    "prediction_accuracy": ml_results.get("forecast_accuracy", 0.5)
                }
            }
            
            logger.info(f"âœ… ML behavior analysis completed for user: {user_id}")
            logger.info(f"ğŸ“Š Generated {len(ml_features)} ML features and {len(activity_patterns)} activity patterns")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ ML behavior analysis failed for user {user_id}: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e), "user_id": user_id}
    
    async def _collect_user_time_series_data(self, user_id: str) -> List[Dict]:
        """æ”¶é›†ç”¨æˆ·çš„æ—¶é—´åºåˆ—æ•°æ®"""
        try:
            time_series_data = []
            
            # ä»sessionsè·å–æ´»åŠ¨æ—¶é—´ç‚¹
            sessions_response = self.db_client.table('sessions')\
                .select('created_at, duration, id')\
                .eq('user_id', user_id)\
                .order('created_at', desc=False)\
                .execute()
            
            if sessions_response.data:
                for session in sessions_response.data:
                    if session.get('created_at'):
                        time_series_data.append({
                            'timestamp': session['created_at'],
                            'activity_type': 'session_start',
                            'value': 1,  # ä¼šè¯å¼€å§‹äº‹ä»¶
                            'duration': session.get('duration', 0),
                            'source': 'sessions',
                            'source_id': session['id']
                        })
            
            # ä»session_messagesè·å–æ¶ˆæ¯æ—¶é—´ç‚¹
            messages_response = self.db_client.table('session_messages')\
                .select('created_at, content, session_id')\
                .eq('user_id', user_id)\
                .order('created_at', desc=False)\
                .limit(200)\
                .execute()
            
            if messages_response.data:
                for message in messages_response.data:
                    if message.get('created_at') and message.get('content'):
                        # æ ¹æ®å†…å®¹é•¿åº¦æ¨æ–­æ´»åŠ¨å¼ºåº¦
                        content_length = len(message['content'])
                        activity_intensity = min(1.0, content_length / 500)  # å½’ä¸€åŒ–åˆ°0-1
                        
                        time_series_data.append({
                            'timestamp': message['created_at'],
                            'activity_type': 'message',
                            'value': activity_intensity,
                            'content_length': content_length,
                            'source': 'session_messages',
                            'session_id': message.get('session_id')
                        })
            
            # ä»user_eventsè·å–è¡Œä¸ºäº‹ä»¶
            events_response = self.db_client.table('user_events')\
                .select('timestamp, event_name, properties')\
                .eq('user_id', user_id)\
                .order('timestamp', desc=False)\
                .execute()
            
            if events_response.data:
                for event in events_response.data:
                    if event.get('timestamp'):
                        time_series_data.append({
                            'timestamp': event['timestamp'],
                            'activity_type': event.get('event_name', 'unknown_event'),
                            'value': 0.5,  # äº‹ä»¶æƒé‡
                            'source': 'user_events',
                            'properties': event.get('properties', {})
                        })
            
            # æŒ‰æ—¶é—´æ’åº
            time_series_data.sort(key=lambda x: x['timestamp'])
            
            logger.info(f"ğŸ“Š Collected {len(time_series_data)} time series data points for user {user_id}")
            return time_series_data
            
        except Exception as e:
            logger.error(f"âŒ Failed to collect time series data for user {user_id}: {e}")
            return []
    
    def _prepare_ml_dataframe(self, time_series_data: List[Dict]) -> pd.DataFrame:
        """å‡†å¤‡MLåˆ†æç”¨çš„DataFrame"""
        try:
            if not time_series_data:
                return pd.DataFrame()
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(time_series_data)
            
            # å¤„ç†æ—¶é—´æˆ³
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.set_index('timestamp')
            
            # æŒ‰å°æ—¶èšåˆæ´»åŠ¨å¼ºåº¦
            df_hourly = df.groupby([
                df.index.date,
                df.index.hour
            ]).agg({
                'value': 'sum',  # æ€»æ´»åŠ¨å¼ºåº¦
                'activity_type': 'count'  # æ´»åŠ¨æ¬¡æ•°
            }).reset_index()
            
            # é‡æ–°æ„å»ºæ—¶é—´ç´¢å¼•
            df_hourly['datetime'] = pd.to_datetime(
                df_hourly['level_0'].astype(str) + ' ' + 
                df_hourly['level_1'].astype(str) + ':00:00'
            )
            df_hourly = df_hourly.set_index('datetime')
            df_hourly = df_hourly.drop(['level_0', 'level_1'], axis=1)
            
            # é‡å‘½ååˆ—
            df_hourly.columns = ['activity_intensity', 'activity_count']
            
            # å¡«å……ç¼ºå¤±æ—¶é—´ç‚¹ä¸º0
            full_range = pd.date_range(
                start=df_hourly.index.min(),
                end=df_hourly.index.max(),
                freq='H'
            )
            df_hourly = df_hourly.reindex(full_range, fill_value=0)
            
            logger.info(f"ğŸ“ˆ Prepared ML DataFrame with {len(df_hourly)} hourly data points")
            return df_hourly
            
        except Exception as e:
            logger.error(f"âŒ Failed to prepare ML dataframe: {e}")
            return pd.DataFrame()
    
    async def _run_ml_time_series_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è¿è¡ŒMLæ—¶é—´åºåˆ—åˆ†æ - ä½¿ç”¨ç°æœ‰çš„TimeSeriesProcessor"""
        try:
            if df.empty or len(df) < 7:
                return {"error": "insufficient_data"}
            
            # åˆå§‹åŒ–TimeSeriesProcessor (å¦‚æœè¿˜æ²¡åˆå§‹åŒ–)
            if self.ts_processor is None:
                # å°†DataFrameä¿å­˜ä¸ºä¸´æ—¶CSVç”¨äºTimeSeriesProcessor
                import tempfile
                import os
                
                temp_dir = tempfile.mkdtemp()
                csv_path = os.path.join(temp_dir, "user_behavior_timeseries.csv")
                
                # å‡†å¤‡TimeSeriesProcessoréœ€è¦çš„æ ¼å¼
                ts_df = df.reset_index()
                ts_df.columns = ['ds', 'y', 'activity_count']  # Prophetæ ¼å¼
                ts_df.to_csv(csv_path, index=False)
                
                # åˆå§‹åŒ–å¤„ç†å™¨
                self.ts_processor = TimeSeriesProcessor(file_path=csv_path)
            
            ml_results = {}
            
            # 1. æ£€æµ‹å­£èŠ‚æ€§æ¨¡å¼
            try:
                if len(df) >= 24:  # è‡³å°‘éœ€è¦24å°æ—¶æ•°æ®
                    seasonality_results = self.ts_processor.detect_seasonality('y')
                    ml_results['seasonality'] = seasonality_results
                    logger.info("âœ… Seasonality detection completed")
            except Exception as e:
                logger.warning(f"âš ï¸ Seasonality detection failed: {e}")
            
            # 2. å­£èŠ‚æ€§åˆ†è§£
            try:
                if len(df) >= 48:  # è‡³å°‘éœ€è¦48å°æ—¶æ•°æ®è¿›è¡Œåˆ†è§£
                    decomposition_results = self.ts_processor.seasonal_decomposition('y', period=24)
                    ml_results['decomposition'] = decomposition_results
                    logger.info("âœ… Seasonal decomposition completed")
            except Exception as e:
                logger.warning(f"âš ï¸ Seasonal decomposition failed: {e}")
            
            # 3. é¢„æµ‹ (å¦‚æœæ•°æ®è¶³å¤Ÿ)
            try:
                if len(df) >= 72:  # è‡³å°‘éœ€è¦3å¤©æ•°æ®è¿›è¡Œé¢„æµ‹
                    forecast_results = self.ts_processor.prophet_forecast('y', periods=24)  # é¢„æµ‹24å°æ—¶
                    ml_results['forecast'] = forecast_results
                    logger.info("âœ… Prophet forecasting completed")
            except Exception as e:
                logger.warning(f"âš ï¸ Prophet forecasting failed: {e}")
                # å¦‚æœProphetå¤±è´¥ï¼Œå°è¯•ç®€å•çš„æŒ‡æ•°å¹³æ»‘
                try:
                    forecast_results = self.ts_processor.exponential_smoothing_forecast('y', periods=24)
                    ml_results['forecast'] = forecast_results
                    logger.info("âœ… Exponential smoothing forecasting completed")
                except Exception as e2:
                    logger.warning(f"âš ï¸ All forecasting methods failed: {e2}")
            
            # 4. ç»¼åˆåˆ†æ
            if ml_results:
                try:
                    comprehensive_results = self.ts_processor.comprehensive_time_series_analysis(
                        'ds', 'y', periods=24
                    )
                    ml_results['comprehensive'] = comprehensive_results
                    logger.info("âœ… Comprehensive analysis completed")
                except Exception as e:
                    logger.warning(f"âš ï¸ Comprehensive analysis failed: {e}")
            
            return ml_results
            
        except Exception as e:
            logger.error(f"âŒ ML time series analysis failed: {e}")
            return {"error": str(e)}
    
    async def _analyze_time_activity_correlation(self, user_id: str, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´-æ´»åŠ¨ç›¸å…³æ€§ï¼šç”¨æˆ·åœ¨ä»€ä¹ˆæ—¶é—´ä¹ æƒ¯åšä»€ä¹ˆ"""
        try:
            if df.empty:
                return {}
            
            activity_patterns = {}
            
            # 1. æŒ‰å°æ—¶åˆ†ææ´»åŠ¨æ¨¡å¼
            hourly_patterns = df.groupby(df.index.hour).agg({
                'activity_intensity': ['mean', 'std', 'count'],
                'activity_count': 'mean'
            }).round(3)
            
            # æ‰å¹³åŒ–åˆ—å
            hourly_patterns.columns = [
                'avg_intensity', 'std_intensity', 'total_periods', 'avg_count'
            ]
            
            # è¯†åˆ«é«˜å³°æ—¶æ®µ
            mean_intensity = hourly_patterns['avg_intensity'].mean()
            peak_hours = hourly_patterns[
                hourly_patterns['avg_intensity'] > mean_intensity * 1.2
            ].index.tolist()
            
            activity_patterns['peak_hours'] = peak_hours
            activity_patterns['hourly_patterns'] = hourly_patterns.to_dict()
            
            # 2. æŒ‰æ˜ŸæœŸå‡ åˆ†æ
            daily_patterns = df.groupby(df.index.dayofweek).agg({
                'activity_intensity': 'mean',
                'activity_count': 'mean'
            }).round(3)
            
            day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
            daily_patterns.index = [day_names[i] for i in daily_patterns.index]
            
            activity_patterns['weekly_patterns'] = daily_patterns.to_dict()
            
            # 3. è·å–å…·ä½“çš„æ´»åŠ¨å†…å®¹åˆ†æ
            content_patterns = await self._analyze_time_specific_content(user_id, peak_hours)
            activity_patterns['content_patterns'] = content_patterns
            
            # 4. ç”Ÿæˆè¡Œä¸ºæ¨¡å¼æ‘˜è¦
            behavior_summary = self._generate_behavior_summary(hourly_patterns, daily_patterns, peak_hours)
            activity_patterns['behavior_summary'] = behavior_summary
            
            logger.info(f"ğŸ“Š Activity pattern analysis completed: {len(peak_hours)} peak hours identified")
            return activity_patterns
            
        except Exception as e:
            logger.error(f"âŒ Time-activity correlation analysis failed: {e}")
            return {}
    
    async def _analyze_time_specific_content(self, user_id: str, peak_hours: List[int]) -> Dict[str, Any]:
        """åˆ†æç‰¹å®šæ—¶é—´æ®µç”¨æˆ·åœ¨åšä»€ä¹ˆå…·ä½“å†…å®¹"""
        try:
            content_patterns = {}
            
            if not peak_hours:
                return content_patterns
            
            # è·å–é«˜å³°æ—¶æ®µçš„æ¶ˆæ¯å†…å®¹
            for hour in peak_hours:
                try:
                    # æ„å»ºSQLæŸ¥è¯¢æ¡ä»¶ï¼šè·å–è¯¥å°æ—¶çš„æ¶ˆæ¯
                    messages_response = self.db_client.table('session_messages')\
                        .select('content, created_at')\
                        .eq('user_id', user_id)\
                        .execute()
                    
                    if not messages_response.data:
                        continue
                    
                    # è¿‡æ»¤è¯¥å°æ—¶çš„æ¶ˆæ¯
                    hour_messages = []
                    for msg in messages_response.data:
                        if msg.get('created_at'):
                            try:
                                msg_time = datetime.fromisoformat(msg['created_at'].replace('Z', '+00:00'))
                                if msg_time.hour == hour and msg.get('content'):
                                    hour_messages.append(msg['content'])
                            except:
                                continue
                    
                    if hour_messages:
                        # ç®€å•çš„å…³é”®è¯åˆ†æ
                        all_content = ' '.join(hour_messages).lower()
                        
                        # è¯†åˆ«ä¸»è¦æ´»åŠ¨ç±»å‹
                        activity_keywords = {
                            'coding': ['code', 'function', 'python', 'javascript', 'debug', 'error', 'script'],
                            'learning': ['learn', 'understand', 'explain', 'how to', 'what is', 'tutorial'],
                            'data_analysis': ['data', 'analysis', 'pandas', 'dataframe', 'sql', 'chart'],
                            'problem_solving': ['help', 'issue', 'problem', 'fix', 'solve', 'error'],
                            'planning': ['plan', 'organize', 'schedule', 'todo', 'project', 'task']
                        }
                        
                        hour_activities = {}
                        for activity, keywords in activity_keywords.items():
                            count = sum(1 for keyword in keywords if keyword in all_content)
                            if count > 0:
                                hour_activities[activity] = count
                        
                        if hour_activities:
                            # æ‰¾å‡ºä¸»è¦æ´»åŠ¨
                            main_activity = max(hour_activities.items(), key=lambda x: x[1])[0]
                            content_patterns[f"hour_{hour}"] = {
                                'main_activity': main_activity,
                                'activity_scores': hour_activities,
                                'message_count': len(hour_messages),
                                'avg_message_length': sum(len(msg) for msg in hour_messages) / len(hour_messages)
                            }
                
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to analyze content for hour {hour}: {e}")
                    continue
            
            return content_patterns
            
        except Exception as e:
            logger.error(f"âŒ Time-specific content analysis failed: {e}")
            return {}
    
    def _generate_behavior_summary(self, hourly_patterns: pd.DataFrame, 
                                 daily_patterns: pd.DataFrame, peak_hours: List[int]) -> Dict[str, str]:
        """ç”Ÿæˆè¡Œä¸ºæ¨¡å¼æ‘˜è¦"""
        try:
            summary = {}
            
            # åˆ†æå·¥ä½œæ¨¡å¼
            if peak_hours:
                morning_hours = [h for h in peak_hours if 6 <= h <= 11]
                afternoon_hours = [h for h in peak_hours if 12 <= h <= 17]
                evening_hours = [h for h in peak_hours if 18 <= h <= 23]
                night_hours = [h for h in peak_hours if h >= 0 and h <= 5]
                
                if len(morning_hours) >= 2:
                    summary['work_pattern'] = 'morning_person'
                elif len(evening_hours) >= 2 or len(night_hours) >= 1:
                    summary['work_pattern'] = 'night_owl'
                elif len(afternoon_hours) >= 2:
                    summary['work_pattern'] = 'afternoon_focused'
                else:
                    summary['work_pattern'] = 'flexible'
            
            # åˆ†ææ´»åŠ¨å¼ºåº¦
            if not hourly_patterns.empty:
                max_intensity = hourly_patterns['avg_intensity'].max()
                if max_intensity > 2.0:
                    summary['intensity_level'] = 'high_intensity'
                elif max_intensity > 1.0:
                    summary['intensity_level'] = 'moderate_intensity'
                else:
                    summary['intensity_level'] = 'light_usage'
            
            # åˆ†æä¸€è‡´æ€§
            if not hourly_patterns.empty:
                intensity_std = hourly_patterns['avg_intensity'].std()
                if intensity_std < 0.5:
                    summary['consistency'] = 'highly_consistent'
                elif intensity_std < 1.0:
                    summary['consistency'] = 'moderately_consistent'
                else:
                    summary['consistency'] = 'variable_pattern'
            
            return summary
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to generate behavior summary: {e}")
            return {}
    
    def _generate_ml_user_features(self, ml_results: Dict[str, Any], 
                                 activity_patterns: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºMLåˆ†æç»“æœç”Ÿæˆç”¨æˆ·ç‰¹å¾ï¼ˆç”¨äºPersonaç”Ÿæˆï¼‰"""
        try:
            ml_features = {}
            
            # 1. æ—¶é—´è¡Œä¸ºç‰¹å¾
            if activity_patterns.get('peak_hours'):
                ml_features['peak_activity_hours'] = activity_patterns['peak_hours']
                ml_features['primary_work_pattern'] = activity_patterns.get('behavior_summary', {}).get('work_pattern', 'flexible')
            
            # 2. æ´»åŠ¨å¼ºåº¦ç‰¹å¾
            behavior_summary = activity_patterns.get('behavior_summary', {})
            ml_features['usage_intensity'] = behavior_summary.get('intensity_level', 'moderate_intensity')
            ml_features['behavior_consistency'] = behavior_summary.get('consistency', 'moderately_consistent')
            
            # 3. å­£èŠ‚æ€§ç‰¹å¾ï¼ˆæ¥è‡ªMLåˆ†æï¼‰
            if ml_results.get('seasonality'):
                seasonality = ml_results['seasonality']
                if seasonality.get('has_weekly_seasonality'):
                    ml_features['weekly_patterns_detected'] = True
                if seasonality.get('has_daily_seasonality'):
                    ml_features['daily_patterns_detected'] = True
            
            # 4. é¢„æµ‹ç‰¹å¾ï¼ˆæ¥è‡ªMLåˆ†æï¼‰
            if ml_results.get('forecast'):
                forecast = ml_results['forecast']
                if forecast.get('trend'):
                    ml_features['activity_trend'] = forecast['trend']
                if forecast.get('forecast_accuracy'):
                    ml_features['predictability_score'] = forecast['forecast_accuracy']
            
            # 5. å†…å®¹æ´»åŠ¨ç‰¹å¾
            content_patterns = activity_patterns.get('content_patterns', {})
            if content_patterns:
                # ç»Ÿè®¡å„æ—¶é—´æ®µçš„ä¸»è¦æ´»åŠ¨
                main_activities = [pattern.get('main_activity') for pattern in content_patterns.values()]
                if main_activities:
                    from collections import Counter
                    activity_counts = Counter(main_activities)
                    ml_features['dominant_activity_type'] = activity_counts.most_common(1)[0][0]
                    ml_features['activity_diversity'] = len(set(main_activities)) / len(main_activities)
            
            # 6. è®¡ç®—æ•´ä½“æ¨¡å¼ç½®ä¿¡åº¦
            pattern_indicators = [
                bool(ml_features.get('peak_activity_hours')),
                bool(ml_features.get('weekly_patterns_detected')),
                bool(ml_features.get('dominant_activity_type')),
                bool(ml_results.get('seasonality')),
                bool(ml_results.get('forecast'))
            ]
            
            ml_features['pattern_confidence'] = sum(pattern_indicators) / len(pattern_indicators)
            
            logger.info(f"ğŸ¯ Generated {len(ml_features)} ML user features")
            return ml_features
            
        except Exception as e:
            logger.error(f"âŒ Failed to generate ML user features: {e}")
            return {}
    
    def _generate_minimal_analysis(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€å°åŒ–åˆ†æç»“æœï¼ˆæ•°æ®ä¸è¶³æ—¶ï¼‰"""
        return {
            "analysis_timestamp": datetime.now().isoformat(),
            "data_points": 0,
            "status": "insufficient_data",
            "ml_user_features": {
                "usage_intensity": "unknown",
                "behavior_consistency": "unknown",
                "primary_work_pattern": "unknown",
                "pattern_confidence": 0.0
            },
            "activity_patterns": {},
            "time_series_insights": {}
        }
    
    def _calculate_analysis_period(self, time_series_data: List[Dict]) -> int:
        """è®¡ç®—åˆ†ææ—¶é—´æ®µçš„å¤©æ•°"""
        if not time_series_data:
            return 0
        
        try:
            timestamps = [datetime.fromisoformat(item['timestamp'].replace('Z', '+00:00')) 
                         for item in time_series_data]
            return (max(timestamps) - min(timestamps)).days + 1
        except:
            return 0

# å…¨å±€å®ä¾‹
user_behavior_ml_analyzer = UserBehaviorMLAnalyzer()

# ä¾¿æ·å‡½æ•°
async def analyze_user_ml_behavior(user_id: str) -> Dict[str, Any]:
    """åˆ†æç”¨æˆ·çš„MLé©±åŠ¨è¡Œä¸ºæ¨¡å¼"""
    return await user_behavior_ml_analyzer.analyze_user_behavior_patterns(user_id)