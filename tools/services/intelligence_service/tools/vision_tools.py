#!/usr/bin/env python3
"""
Vision Tools - MCP tools wrapper for image analysis
åŸºäº ImageAnalyzer çš„è§†è§‰åˆ†æå·¥å…·
"""

import json
from typing import List, Optional, Union
from mcp.server.fastmcp import FastMCP
from tools.base_tool import BaseTool
from tools.services.intelligence_service.vision.image_analyzer import image_analyzer
from core.logging import get_logger

logger = get_logger(__name__)

class VisionTools(BaseTool):
    """è§†è§‰åˆ†æå·¥å…·ï¼ŒåŸºäº ImageAnalyzer æä¾› MCP æ¥å£"""
    
    def __init__(self):
        super().__init__()
        self.analyzer = image_analyzer
    
    async def analyze_image(
        self,
        image: Union[str, bytes],
        prompt: str,
        model: Optional[str] = None,
        provider: str = "openai",
        response_format: str = "text"
    ) -> str:
        """
        åˆ†æå›¾åƒå†…å®¹
        
        Args:
            image: å›¾åƒæ–‡ä»¶è·¯å¾„æˆ–å­—èŠ‚æ•°æ®
            prompt: åˆ†ææç¤ºè¯
            model: å¯é€‰çš„æ¨¡å‹è§„æ ¼
            provider: æä¾›å•† (openai, isaç­‰)
            response_format: å“åº”æ ¼å¼æç¤º
        """
        logger.info(f"ğŸ” å¼€å§‹å›¾åƒåˆ†æ: {prompt[:50]}...")
        
        try:
            result = await self.analyzer.analyze(
                image=image,
                prompt=prompt,
                model=model,
                provider=provider,
                response_format=response_format
            )
            
            if result.success:
                logger.info(f"âœ… å›¾åƒåˆ†æå®Œæˆï¼Œç”¨æ—¶ {result.processing_time:.2f}s")
                return self.create_response(
                    "success",
                    "analyze_image",
                    {
                        "response": result.response,
                        "model_used": result.model_used,
                        "processing_time": result.processing_time,
                        "prompt": prompt,
                        "provider": provider
                    }
                )
            else:
                logger.error(f"âŒ å›¾åƒåˆ†æå¤±è´¥: {result.error}")
                return self.create_response(
                    "error",
                    "analyze_image",
                    {},
                    f"åˆ†æå¤±è´¥: {result.error}"
                )
                
        except Exception as e:
            logger.error(f"âŒ å›¾åƒåˆ†æå¼‚å¸¸: {e}")
            return self.create_response(
                "error",
                "analyze_image",
                {},
                f"åˆ†æå¼‚å¸¸: {str(e)}"
            )
    
    async def describe_image(
        self,
        image: Union[str, bytes],
        detail_level: str = "medium",
        language: str = "zh-cn"
    ) -> str:
        """
        æè¿°å›¾åƒå†…å®¹
        
        Args:
            image: å›¾åƒæ–‡ä»¶è·¯å¾„æˆ–å­—èŠ‚æ•°æ®
            detail_level: è¯¦ç»†çº§åˆ« (basic, medium, detailed)
            language: è¯­è¨€ (zh-cn, en)
        """
        
        prompts = {
            "basic": {
                "zh-cn": "è¯·ç®€è¦æè¿°è¿™å¼ å›¾ç‰‡çš„ä¸»è¦å†…å®¹ã€‚",
                "en": "Please briefly describe the main content of this image."
            },
            "medium": {
                "zh-cn": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€é¢œè‰²å’Œæ„å›¾ã€‚",
                "en": "Please describe this image in detail, including main objects, scene, colors and composition."
            },
            "detailed": {
                "zh-cn": "è¯·å…¨é¢åˆ†æè¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯è§å…ƒç´ ã€ç©ºé—´å…³ç³»ã€è§†è§‰é£æ ¼ã€æƒ…æ„Ÿè¡¨è¾¾å’Œå¯èƒ½çš„èƒŒæ™¯ä¿¡æ¯ã€‚",
                "en": "Please comprehensively analyze this image, including all visible elements, spatial relationships, visual style, emotional expression and possible background information."
            }
        }
        
        prompt = prompts.get(detail_level, prompts["medium"]).get(language, prompts["medium"]["zh-cn"])
        
        return await self.analyze_image(
            image=image,
            prompt=prompt,
            response_format="description"
        )
    
    async def extract_text(
        self,
        image: Union[str, bytes],
        language: str = "auto"
    ) -> str:
        """
        ä»å›¾åƒä¸­æå–æ–‡å­— (OCR)
        
        Args:
            image: å›¾åƒæ–‡ä»¶è·¯å¾„æˆ–å­—èŠ‚æ•°æ®
            language: è¯­è¨€ä»£ç  (auto, zh, enç­‰)
        """
        
        if language == "auto":
            prompt = "è¯·æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œä¿æŒåŸæœ‰æ ¼å¼å’Œå¸ƒå±€ã€‚å¦‚æœæœ‰å¤šç§è¯­è¨€ï¼Œè¯·å…¨éƒ¨æå–ã€‚"
        else:
            prompt = f"è¯·æå–å›¾ç‰‡ä¸­æ‰€æœ‰ {language} æ–‡å­—å†…å®¹ï¼Œä¿æŒåŸæœ‰æ ¼å¼å’Œå¸ƒå±€ã€‚"
        
        return await self.analyze_image(
            image=image,
            prompt=prompt,
            response_format="text"
        )
    
    async def identify_objects(
        self,
        image: Union[str, bytes],
        object_type: str = "all"
    ) -> str:
        """
        è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡
        
        Args:
            image: å›¾åƒæ–‡ä»¶è·¯å¾„æˆ–å­—èŠ‚æ•°æ®
            object_type: å¯¹è±¡ç±»å‹ (all, people, animals, vehicles, foodç­‰)
        """
        
        prompts = {
            "all": "è¯·è¯†åˆ«å¹¶åˆ—å‡ºå›¾ç‰‡ä¸­çš„æ‰€æœ‰ä¸»è¦å¯¹è±¡å’Œç‰©å“ã€‚",
            "people": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„äººç‰©ï¼ŒåŒ…æ‹¬äººæ•°ã€ä½ç½®ã€åŠ¨ä½œå’Œå¤–è§‚ç‰¹å¾ã€‚",
            "animals": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„åŠ¨ç‰©ï¼ŒåŒ…æ‹¬ç§ç±»ã€æ•°é‡å’Œè¡Œä¸ºã€‚",
            "vehicles": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„äº¤é€šå·¥å…·ï¼ŒåŒ…æ‹¬ç±»å‹ã€é¢œè‰²å’Œä½ç½®ã€‚",
            "food": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„é£Ÿç‰©ï¼ŒåŒ…æ‹¬ç§ç±»ã€çŠ¶æ€å’Œæ‘†ç›˜æ–¹å¼ã€‚",
            "text": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹å’Œæ–‡æœ¬å…ƒç´ ã€‚"
        }
        
        prompt = prompts.get(object_type, prompts["all"])
        
        return await self.analyze_image(
            image=image,
            prompt=prompt,
            response_format="json"
        )
    
    async def analyze_emotion(
        self,
        image: Union[str, bytes]
    ) -> str:
        """
        åˆ†æå›¾åƒä¸­çš„æƒ…æ„Ÿè¡¨è¾¾
        
        Args:
            image: å›¾åƒæ–‡ä»¶è·¯å¾„æˆ–å­—èŠ‚æ•°æ®
        """
        
        prompt = """è¯·åˆ†æå›¾ç‰‡ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾ï¼ŒåŒ…æ‹¬ï¼š
        1. äººç‰©çš„é¢éƒ¨è¡¨æƒ…å’Œæƒ…æ„ŸçŠ¶æ€
        2. æ•´ä½“ç”»é¢çš„æƒ…æ„Ÿæ°›å›´
        3. è‰²å½©å’Œæ„å›¾ä¼ è¾¾çš„æƒ…æ„Ÿä¿¡æ¯
        4. åœºæ™¯å’Œç¯å¢ƒçš„æƒ…æ„Ÿå½±å“
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«emotions, mood, atmosphereç­‰å­—æ®µã€‚"""
        
        return await self.analyze_image(
            image=image,
            prompt=prompt,
            response_format="json"
        )
    
    async def compare_images(
        self,
        image1: Union[str, bytes],
        image2: Union[str, bytes],
        comparison_aspects: Optional[List[str]] = None
    ) -> str:
        """
        æ¯”è¾ƒä¸¤å¼ å›¾ç‰‡
        
        Args:
            image1: ç¬¬ä¸€å¼ å›¾ç‰‡
            image2: ç¬¬äºŒå¼ å›¾ç‰‡  
            comparison_aspects: æ¯”è¾ƒæ–¹é¢åˆ—è¡¨
        """
        
        if comparison_aspects is None:
            comparison_aspects = ["å†…å®¹", "æ„å›¾", "è‰²å½©", "é£æ ¼"]
        
        aspects_str = "ã€".join(comparison_aspects)
        
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¤„ç†å¤šå›¾ç‰‡è¾“å…¥ï¼Œå½“å‰ image_analyzer å¯èƒ½éœ€è¦æ‰©å±•
        # æš‚æ—¶ä½¿ç”¨å•å›¾ç‰‡åˆ†ææ–¹å¼
        try:
            result1 = await self.analyze_image(image1, f"åˆ†æç¬¬ä¸€å¼ å›¾ç‰‡çš„{aspects_str}")
            result2 = await self.analyze_image(image2, f"åˆ†æç¬¬äºŒå¼ å›¾ç‰‡çš„{aspects_str}")
            
            comparison_prompt = f"""åŸºäºä»¥ä¸‹ä¸¤å¼ å›¾ç‰‡çš„åˆ†æç»“æœï¼Œè¯·è¿›è¡Œè¯¦ç»†æ¯”è¾ƒï¼š

ç¬¬ä¸€å¼ å›¾ç‰‡åˆ†æï¼š
{result1}

ç¬¬äºŒå¼ å›¾ç‰‡åˆ†æï¼š
{result2}

è¯·æ¯”è¾ƒè¿™ä¸¤å¼ å›¾ç‰‡åœ¨{aspects_str}æ–¹é¢çš„å¼‚åŒã€‚"""
            
            # è¿™é‡Œå¯ä»¥è°ƒç”¨æ–‡æœ¬ç”Ÿæˆå™¨è¿›è¡Œæ¯”è¾ƒåˆ†æ
            from tools.services.intelligence_service.language.text_generator import generate
            comparison_result = await generate(comparison_prompt)
            
            return self.create_response(
                "success",
                "compare_images",
                {
                    "comparison": comparison_result,
                    "aspects": comparison_aspects,
                    "image1_analysis": result1,
                    "image2_analysis": result2
                }
            )
            
        except Exception as e:
            return self.create_response(
                "error",
                "compare_images",
                {},
                f"å›¾ç‰‡æ¯”è¾ƒå¤±è´¥: {str(e)}"
            )


def register_vision_tools(mcp: FastMCP):
    """æ³¨å†Œè§†è§‰åˆ†æå·¥å…·"""
    vision_tools = VisionTools()
    
    @mcp.tool()
    async def analyze_image(
        image_path: str,
        prompt: str,
        model: str = None,
        provider: str = "openai"
    ) -> str:
        """
        é€šç”¨å›¾åƒåˆ†æå·¥å…·
        
        ä½¿ç”¨VLMæ¨¡å‹åˆ†æå›¾åƒå†…å®¹ï¼Œæ”¯æŒä»»æ„åˆ†ææç¤ºè¯ã€‚
        
        Keywords: vision, image, analyze, vlm, ai, recognition
        Category: vision
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            prompt: åˆ†ææç¤ºè¯ï¼Œæè¿°ä½ æƒ³è¦ä»å›¾åƒä¸­è·å–ä»€ä¹ˆä¿¡æ¯
            model: å¯é€‰çš„æ¨¡å‹åç§°
            provider: AIæä¾›å•† (openai, isaç­‰)
        """
        return await vision_tools.analyze_image(
            image=image_path,
            prompt=prompt,
            model=model,
            provider=provider
        )
    
    @mcp.tool()
    async def describe_image(
        image_path: str,
        detail_level: str = "medium",
        language: str = "zh-cn"
    ) -> str:
        """
        æè¿°å›¾åƒå†…å®¹
        
        è‡ªåŠ¨æè¿°å›¾åƒçš„ä¸»è¦å†…å®¹ï¼ŒåŒ…æ‹¬å¯¹è±¡ã€åœºæ™¯ã€æ„å›¾ç­‰ã€‚
        
        Keywords: describe, image, content, scene, objects
        Category: vision
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            detail_level: è¯¦ç»†ç¨‹åº¦ (basic/medium/detailed)
            language: è¯­è¨€ (zh-cn/en)
        """
        return await vision_tools.describe_image(
            image=image_path,
            detail_level=detail_level,
            language=language
        )
    
    @mcp.tool()
    async def extract_text_from_image(
        image_path: str,
        language: str = "auto"
    ) -> str:
        """
        ä»å›¾åƒä¸­æå–æ–‡å­— (OCR)
        
        ä½¿ç”¨å…‰å­¦å­—ç¬¦è¯†åˆ«æŠ€æœ¯æå–å›¾åƒä¸­çš„æ–‡å­—å†…å®¹ã€‚
        
        Keywords: ocr, text, extract, read, recognition
        Category: vision
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            language: æ–‡å­—è¯­è¨€ (auto/zh/enç­‰)
        """
        return await vision_tools.extract_text(
            image=image_path,
            language=language
        )
    
    @mcp.tool()
    async def identify_objects_in_image(
        image_path: str,
        object_type: str = "all"
    ) -> str:
        """
        è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡
        
        è¯†åˆ«å¹¶åˆ†ç±»å›¾åƒä¸­çš„å„ç§å¯¹è±¡å’Œå®ä½“ã€‚
        
        Keywords: identify, objects, detection, recognition, classification
        Category: vision
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            object_type: å¯¹è±¡ç±»å‹ (all/people/animals/vehicles/food/text)
        """
        return await vision_tools.identify_objects(
            image=image_path,
            object_type=object_type
        )
    
    @mcp.tool()
    async def analyze_image_emotion(
        image_path: str
    ) -> str:
        """
        åˆ†æå›¾åƒæƒ…æ„Ÿè¡¨è¾¾
        
        åˆ†æå›¾åƒä¸­çš„æƒ…æ„Ÿå†…å®¹ï¼ŒåŒ…æ‹¬äººç‰©è¡¨æƒ…ã€æ°›å›´ã€æƒ…ç»ªç­‰ã€‚
        
        Keywords: emotion, sentiment, mood, expression, feeling
        Category: vision
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
        """
        return await vision_tools.analyze_emotion(
            image=image_path
        )
    
    @mcp.tool()
    async def compare_two_images(
        image1_path: str,
        image2_path: str,
        comparison_aspects: str = "å†…å®¹,æ„å›¾,è‰²å½©,é£æ ¼"
    ) -> str:
        """
        æ¯”è¾ƒä¸¤å¼ å›¾ç‰‡
        
        å¯¹æ¯”åˆ†æä¸¤å¼ å›¾ç‰‡çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚ã€‚
        
        Keywords: compare, similarity, difference, analysis, contrast
        Category: vision
        
        Args:
            image1_path: ç¬¬ä¸€å¼ å›¾ç‰‡è·¯å¾„
            image2_path: ç¬¬äºŒå¼ å›¾ç‰‡è·¯å¾„
            comparison_aspects: æ¯”è¾ƒæ–¹é¢ï¼Œé€—å·åˆ†éš”
        """
        aspects_list = [aspect.strip() for aspect in comparison_aspects.split(',')]
        return await vision_tools.compare_images(
            image1=image1_path,
            image2=image2_path,
            comparison_aspects=aspects_list
        )
    
    @mcp.tool()
    def get_vision_capabilities() -> str:
        """
        è·å–è§†è§‰åˆ†æèƒ½åŠ›åˆ—è¡¨
        
        è¿”å›å½“å‰å¯ç”¨çš„è§†è§‰åˆ†æåŠŸèƒ½å’Œæ”¯æŒçš„æ“ä½œç±»å‹ã€‚
        
        Keywords: capabilities, features, vision, list, available
        Category: vision
        """
        capabilities = {
            "supported_formats": ["jpg", "jpeg", "png", "gif", "bmp", "webp"],
            "analysis_types": [
                "é€šç”¨å›¾åƒåˆ†æ",
                "å›¾åƒæè¿°ç”Ÿæˆ", 
                "OCRæ–‡å­—æå–",
                "å¯¹è±¡è¯†åˆ«æ£€æµ‹",
                "æƒ…æ„Ÿè¡¨è¾¾åˆ†æ",
                "å›¾åƒå¯¹æ¯”åˆ†æ"
            ],
            "supported_providers": ["openai", "isa"],
            "languages": ["zh-cn", "en"],
            "detail_levels": ["basic", "medium", "detailed"],
            "object_types": ["all", "people", "animals", "vehicles", "food", "text"]
        }
        
        return json.dumps(capabilities, indent=2, ensure_ascii=False)
    
    logger.info("ğŸ” è§†è§‰åˆ†æå·¥å…·æ³¨å†ŒæˆåŠŸ")