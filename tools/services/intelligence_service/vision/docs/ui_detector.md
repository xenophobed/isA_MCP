# UI Detector - UI Element Detection with Coordinates

## Overview

The `ui_detector` is an atomic function that detects UI elements in screenshots and maps them to specific requirements with precise coordinates. It uses a 3-step workflow: OmniParser detection â†’ Image annotation â†’ VLM requirement matching.

**Now includes intelligent caching** - if we've visited a page before, we reuse cached UI elements and skip the expensive OmniParser call, but still run VLM matching for new requirements.

## Key Features

- âœ… **Atomic 3-Step Workflow** - OmniParser â†’ Annotation â†’ VLM Matching
- âœ… **Intelligent Caching** - Reuses cached UI elements to avoid redundant OmniParser calls
- âœ… **Precise Coordinates** - Returns exact x,y coordinates for each element
- âœ… **Requirement Mapping** - Maps UI elements to specific interaction requirements
- âœ… **Visual Annotation** - Creates annotated images with numbered bounding boxes
- âœ… **Multiple Input Types** - Supports file paths and image bytes
- âœ… **Action Classification** - Determines appropriate action type (click, type, etc.)

## Basic Usage

### Simple UI Detection

```python
from tools.services.intelligence_service.vision.ui_detector import detect_ui_with_coordinates

# Define what UI elements you need
requirements = [
    {
        "element_name": "search_input",
        "element_purpose": "search query input field", 
        "visual_description": "white input field with placeholder text",
        "interaction_type": "click_and_type"
    },
    {
        "element_name": "search_button",
        "element_purpose": "submit search query",
        "visual_description": "blue button with Search text", 
        "interaction_type": "click"
    }
]

# Detect UI elements and get coordinates (with caching optimization)
result = await detect_ui_with_coordinates(
    screenshot="webpage.png",
    requirements=requirements,
    url="https://example.com"  # Optional: enables caching optimization
)

# Use the coordinate mappings
for element_name, mapping in result.element_mappings.items():
    x, y = mapping['x'], mapping['y']
    action = mapping['action']
    print(f"{element_name}: ({x}, {y}) - {action}")
```

### Advanced Usage with Caching

```python
from tools.services.intelligence_service.vision.ui_detector import UIDetector

detector = UIDetector()

# Full detection with caching control
result = await detector.detect_ui_with_coordinates(
    screenshot="complex_page.png",
    requirements=requirements,
    url="https://example.com/page",  # Enables caching
    force_refresh=False  # Set True to bypass cache
)

print(f"Found {len(result.ui_elements)} total UI elements")
print(f"Mapped {len(result.element_mappings)} requirements")
print(f"Processing time: {result.processing_time:.3f}s")
print(f"Annotated image: {result.annotated_image_path}")

# For repeated calls to same page, cached UI elements will be reused!
# Only VLM matching runs for new requirements
```

## API Reference

### detect_ui_with_coordinates() Function

Convenience function for UI detection with coordinates.

```python
async def detect_ui_with_coordinates(
    screenshot: Union[str, bytes],
    requirements: List[Dict[str, str]],
    url: Optional[str] = None,
    force_refresh: bool = False
) -> UIDetectionResult
```

**Parameters:**
- `screenshot` - Screenshot to analyze (file path or bytes)
- `requirements` - List of UI element requirements to match
- `url` - Optional URL for cache optimization (enables caching of UI elements)
- `force_refresh` - Force OmniParser detection even if cached results exist

### UIDetector Class

Main detector class with full functionality.

```python
class UIDetector:
    async def detect_ui_with_coordinates(
        self,
        screenshot: Union[str, bytes],
        requirements: List[Dict[str, str]],
        url: Optional[str] = None,
        force_refresh: bool = False
    ) -> UIDetectionResult
    
    async def annotate_ui_elements(
        self,
        screenshot: Union[str, bytes], 
        save_path: Optional[str] = None
    ) -> Optional[str]
```

### Requirement Format

Each requirement should be a dictionary with these fields:

```python
{
    "element_name": str,        # Unique identifier for the element
    "element_purpose": str,     # What this element is used for
    "visual_description": str,  # How the element looks
    "interaction_type": str     # How to interact (click, type, etc.)
}
```

**Example Requirements:**

```python
requirements = [
    {
        "element_name": "username_field",
        "element_purpose": "input field for username",
        "visual_description": "white text input with Username placeholder",
        "interaction_type": "click_and_type"
    },
    {
        "element_name": "login_button", 
        "element_purpose": "submit login form",
        "visual_description": "blue button with Login text",
        "interaction_type": "click"
    },
    {
        "element_name": "forgot_password_link",
        "element_purpose": "navigate to password reset",
        "visual_description": "blue underlined text link",
        "interaction_type": "click"
    }
]
```

### UIDetectionResult

Result object containing all detection information.

```python
@dataclass
class UIDetectionResult:
    ui_elements: List[Dict[str, Any]]           # Raw OmniParser elements
    annotated_image_path: Optional[str]         # Path to annotated image
    element_mappings: Dict[str, Dict[str, Any]] # Requirement mappings
    success: bool                               # Whether detection succeeded
    error: Optional[str]                        # Error message if failed
    processing_time: float                      # Time taken in seconds
```

**Element Mapping Format:**

```python
{
    "element_name": {
        "x": int,                    # X coordinate of element center
        "y": int,                    # Y coordinate of element center  
        "action": str,               # Recommended action (click, type)
        "confidence": float,         # VLM confidence in match
        "reasoning": str,            # Why this element was chosen
        "element": Dict[str, Any]    # Full OmniParser element data
    }
}
```

## Caching Optimization

### How Caching Works

The UI detector now includes intelligent caching to optimize performance:

1. **First Visit**: OmniParser detects UI elements â†’ Results cached by URL + image hash
2. **Subsequent Visits**: Cached UI elements reused â†’ Only VLM matching runs for new requirements
3. **Cache Validation**: Image hash comparison ensures page hasn't changed significantly

### Benefits

- âš¡ **2-3x Faster**: Skip expensive OmniParser calls for visited pages
- ðŸ’° **Cost Reduction**: Fewer API calls to OmniParser service
- ðŸŽ¯ **Smart Invalidation**: Cache automatically invalidates when page content changes
- ðŸ”„ **Always Fresh VLM**: Requirements matching always runs to handle new requirements

### Cache Control

```python
# Enable caching (recommended)
result = await detect_ui_with_coordinates(
    screenshot="page.png",
    requirements=requirements,
    url="https://example.com"  # Enables caching
)

# Force refresh (bypass cache)
result = await detect_ui_with_coordinates(
    screenshot="page.png", 
    requirements=requirements,
    url="https://example.com",
    force_refresh=True  # Always run OmniParser
)

# No caching (legacy mode)
result = await detect_ui_with_coordinates(
    screenshot="page.png",
    requirements=requirements
    # No URL = no caching
)
```

## 3-Step Workflow

### Step 1: OmniParser Detection (with Caching)

Uses Microsoft OmniParser to detect all UI elements, with intelligent caching:

```python
# Cache-aware detection - happens internally
if cached_elements_available:
    ui_elements = cached_ui_elements  # Reuse cached results
    logger.info("âœ… Using cached UI elements")
else:
    ui_elements = await self._get_ui_elements_with_omniparser(image_path)
    # Cache for future use
    await cache_manager.save_detection_result(...)

# Returns elements like:
[
    {
        "id": "omni_0",
        "type": "button", 
        "content": "Search",
        "center": [400, 200],
        "bbox": [350, 180, 450, 220],
        "confidence": 0.95,
        "interactable": True
    }
]
```

### Step 2: Image Annotation

Creates annotated image with numbered red bounding boxes:

```python
# Automatic - happens internally  
annotated_path = await self._create_annotated_image(image_path, ui_elements)

# Also available as convenience function
annotated_path = await detector.annotate_ui_elements("screenshot.png")
```

### Step 3: VLM Requirement Matching (Always Fresh)

Uses VLM to match requirements with numbered elements in annotated image.
**This step always runs** to ensure new requirements are properly matched:

```python
# Always runs - even with cached UI elements
element_mappings = await self._match_requirements_with_elements(
    annotated_image_path, ui_elements, requirements
)
```

## Use Cases

### 1. Form Interaction

```python
form_requirements = [
    {
        "element_name": "name_field",
        "element_purpose": "input field for full name",
        "visual_description": "white input field labeled Name",
        "interaction_type": "click_and_type"
    },
    {
        "element_name": "submit_button",
        "element_purpose": "submit the form",
        "visual_description": "green Submit button",
        "interaction_type": "click"
    }
]

result = await detect_ui_with_coordinates("form.png", form_requirements)
```

### 2. Search Functionality

```python
search_requirements = [
    {
        "element_name": "search_box",
        "element_purpose": "enter search terms",
        "visual_description": "search input field",
        "interaction_type": "click_and_type"
    },
    {
        "element_name": "search_submit",
        "element_purpose": "execute search",
        "visual_description": "search button or magnifying glass icon",
        "interaction_type": "click"
    }
]
```

### 3. E-commerce Interactions

```python
ecommerce_requirements = [
    {
        "element_name": "add_to_cart",
        "element_purpose": "add product to shopping cart",
        "visual_description": "Add to Cart button",
        "interaction_type": "click"
    },
    {
        "element_name": "quantity_selector",
        "element_purpose": "select product quantity",
        "visual_description": "quantity input or dropdown",
        "interaction_type": "click_and_type"
    }
]
```

### 4. Navigation Elements

```python
nav_requirements = [
    {
        "element_name": "menu_button",
        "element_purpose": "open navigation menu",
        "visual_description": "hamburger menu icon or Menu button",
        "interaction_type": "click"
    },
    {
        "element_name": "home_link",
        "element_purpose": "navigate to home page",
        "visual_description": "Home link in navigation",
        "interaction_type": "click"
    }
]
```

## Error Handling

```python
result = await detect_ui_with_coordinates(screenshot, requirements)

if not result.success:
    print(f"Detection failed: {result.error}")
    # Handle error appropriately
elif not result.element_mappings:
    print("No element mappings found - requirements may not match UI")
    # Try different requirements or check image
else:
    # Use the mappings
    for name, mapping in result.element_mappings.items():
        print(f"Found {name} at ({mapping['x']}, {mapping['y']})")
```

**Common Error Types:**
- `No UI elements detected by OmniParser` - Image may not contain UI elements
- `Unsupported image type` - Invalid image input
- `VLM matching failed` - Requirements don't match detected elements

## Performance

### Without Caching (First Visit)
- **Typical Response Time**: 2-5 seconds (depends on UI complexity)
- **OmniParser Time**: 1-3 seconds for element detection
- **VLM Matching Time**: 1-2 seconds for requirement mapping

### With Caching (Subsequent Visits)
- **Typical Response Time**: 1-2 seconds (60-70% faster)
- **OmniParser Time**: 0 seconds (cached)
- **VLM Matching Time**: 1-2 seconds (always runs)
- **Cache Lookup**: ~10ms

### Limits
- **Image Size Limit**: Up to 20MB screenshots
- **Element Limit**: Can handle 50+ UI elements per screenshot
- **Cache Duration**: 24 hours (configurable)
- **Cache Size**: ~1000 entries (auto-cleanup)

## Testing

Run comprehensive tests with real ISA client calls:

```bash
python tools/services/intelligence_service/vision/test_ui_detector.py
```

**Test Coverage:**
- âœ… Simple UI detection (buttons, inputs)
- âœ… Search page detection  
- âœ… Form page detection
- âœ… E-commerce page detection
- âœ… Bytes input handling
- âœ… UI annotation function
- âœ… Convenience functions
- âœ… Invalid input handling
- âœ… VLM matching debug

## Integration with Web Automation

### Step 3 of 5-Step Workflow

```python
# After Step 2 (image_analyzer screen understanding)
ui_result = await detect_ui_with_coordinates(
    screenshot=screenshot_path,
    requirements=extracted_requirements  # From Step 2
)

# Use coordinates for Step 5 (Playwright actions)
for element_name, mapping in ui_result.element_mappings.items():
    if mapping['action'] == 'click':
        await page.click(x=mapping['x'], y=mapping['y'])
    elif mapping['action'] == 'type':
        await page.click(x=mapping['x'], y=mapping['y'])
        await page.type(text_to_input)
```

### Integration with Playwright

```python
from playwright.async_api import Page

async def perform_ui_actions(page: Page, ui_result: UIDetectionResult, actions: Dict):
    """Execute Playwright actions using UI detection results"""
    
    for element_name, action_data in actions.items():
        if element_name in ui_result.element_mappings:
            mapping = ui_result.element_mappings[element_name]
            x, y = mapping['x'], mapping['y']
            
            if action_data['type'] == 'click':
                await page.mouse.click(x, y)
            elif action_data['type'] == 'type':
                await page.mouse.click(x, y)
                await page.keyboard.type(action_data['text'])
```

## Best Practices

1. **Specific Descriptions** - Use detailed visual descriptions for better matching
2. **Reasonable Requirements** - Don't over-specify requirements that may not exist
3. **Error Handling** - Always check success status and handle missing mappings
4. **Image Quality** - Use high-quality screenshots for better detection
5. **Element Uniqueness** - Ensure element names are unique within requirements
6. **Action Types** - Choose appropriate interaction types (click, type, etc.)

## Atomic Function Design

The ui_detector follows atomic function principles:

- **Self-Contained 3-Step Workflow** - Complete UI detection pipeline
- **No Business Logic** - Only UI detection and coordinate mapping
- **Pure Functionality** - Given same input, produces consistent output structure
- **Generic Interface** - Works with any screenshot and requirements
- **Independent Operation** - No dependencies on other services or state

This makes it highly reusable for any UI automation task requiring element coordinates.